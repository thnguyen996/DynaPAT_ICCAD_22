2021-11-20 13:18:47,122 - Log file for this run: /home/th.nguyen/drift-encode/logs/googlenet-imagenet-helmet___2021.11.20-131847/googlenet-imagenet-helmet___2021.11.20-131847.log
2021-11-20 13:18:47,123 - Number of CPUs: 56
2021-11-20 13:18:47,353 - Number of GPUs: 6
2021-11-20 13:18:47,354 - CUDA version: 10.0.130
2021-11-20 13:18:47,357 - CUDNN version: 7603
2021-11-20 13:18:47,357 - Kernel: 5.4.0-90-generic
2021-11-20 13:18:47,357 - Python: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
[GCC 7.5.0]
2021-11-20 13:18:47,358 - pip freeze: {'absl-py': '0.12.0', 'aiohttp': '3.7.4.post0', 'argon2-cffi': '21.1.0', 'astor': '0.8.1', 'astunparse': '1.6.3', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'backcall': '0.2.0', 'black': '21.9b0', 'bleach': '4.1.0', 'blessed': '1.19.0', 'blessings': '1.7', 'blinker': '1.4', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2020.12.5', 'cffi': '1.14.6', 'chardet': '4.0.0', 'charset-normalizer': '2.0.4', 'click': '8.0.1', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'dataclasses': '0.8', 'decorator': '5.1.0', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'gast': '0.2.2', 'gitdb': '4.0.9', 'gitdb2': '3.0.3.post1', 'gitpython': '3.1.0', 'google-auth': '1.30.1', 'google-auth-oauthlib': '0.4.4', 'google-pasta': '0.2.0', 'gpustat': '1.0.0.dev1', 'graphviz': '0.10.1', 'grpcio': '1.38.0', 'gym': '0.12.5', 'h5py': '2.10.0', 'idna': '2.10', 'idna-ssl': '1.1.0', 'importlib-metadata': '4.8.1', 'ipykernel': '5.5.6', 'ipython': '7.16.1', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.17.2', 'jinja2': '3.0.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '7.0.6', 'jupyter-console': '6.4.0', 'jupyter-core': '4.8.1', 'jupyterlab-pygments': '0.1.2', 'keras-applications': '1.0.8', 'keras-preprocessing': '1.1.2', 'kiwisolver': '1.3.1', 'markdown': '3.3.4', 'markupsafe': '2.0.1', 'matplotlib': '3.3.4', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.10.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'mypy-extensions': '0.4.3', 'nbclient': '0.5.4', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'notebook': '6.4.4', 'numpy': '1.18.5', 'nvidia-ml-py3': '7.352.0', 'oauthlib': '3.1.0', 'olefile': '0.46', 'opt-einsum': '3.3.0', 'packaging': '21.0', 'pandas': '1.1.5', 'pandocfilters': '1.5.0', 'parse': '1.19.0', 'parso': '0.7.1', 'pathspec': '0.9.0', 'pexpect': '4.8.0', 'pickle5': '0.0.11', 'pickleshare': '0.7.5', 'pillow': '6.2.2', 'pip': '21.2.2', 'platformdirs': '2.4.0', 'pluggy': '0.13.1', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.11.0', 'prompt-toolkit': '3.0.20', 'protobuf': '3.17.1', 'psutil': '5.8.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pyglet': '1.5.21', 'pygments': '2.10.0', 'pyhocon': '0.3.58', 'pyjwt': '2.1.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyrsistent': '0.18.0', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'python-jsonrpc-server': '0.4.0', 'python-language-server': '0.36.2', 'pytz': '2021.3', 'pyyaml': '6.0', 'pyzmq': '22.3.0', 'qgrid': '1.1.1', 'qtconsole': '5.1.1', 'qtpy': '1.11.2', 'ranger-fm': '1.9.3', 'regex': '2021.10.21', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'rsa': '4.7.2', 'scikit-learn': '0.21.2', 'scipy': '1.5.4', 'seaborn': '0.11.2', 'send2trash': '1.8.0', 'setuptools': '57.0.0', 'six': '1.16.0', 'smmap': '4.0.0', 'smmap2': '2.0.5', 'tabulate': '0.8.3', 'tensorboard': '1.15.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.0', 'tensorflow': '1.15.0', 'tensorflow-estimator': '1.15.1', 'termcolor': '1.1.0', 'terminado': '0.12.1', 'testpath': '0.5.0', 'timm': '0.4.9', 'tomli': '1.2.1', 'torch': '1.3.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchsummary': '1.5.1', 'torchvision': '0.4.2', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '4.3.3', 'traittypes': '0.2.1', 'typed-ast': '1.4.3', 'typing-extensions': '3.10.0.1', 'ujson': '4.1.0', 'urllib3': '1.26.4', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.2.1', 'werkzeug': '2.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '3.0.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2021-11-20 13:18:47,463 - Git is dirty
2021-11-20 13:18:47,464 - Active Git branch: master
2021-11-20 13:18:47,512 - Git commit: f401db9403c1608e10258733e013b6c087e3ef34
2021-11-20 13:18:47,513 - Command line: compress_classifier.py --arch googlenet -p 10 -j 22 /home/imagenet/ --pretrained --run --qe-config-file ./conf/lenet_conf.yaml --gpus 5 --name googlenet-imagenet-helmet --method helmet --mlc 8 --num_bits 8 --encode
2021-11-20 13:18:47,514 - Distiller: 0.4.0rc0
2021-11-20 13:18:47,516 - Random seed: 53305
2021-11-20 13:18:47,753 - => created a pretrained googlenet model with the imagenet dataset
2021-11-20 13:18:49,802 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2021-11-20 13:18:49,804 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2021-11-20 13:18:49,985 - Dataset sizes:
	test=50000
2021-11-20 13:18:49,986 - Reading configuration from: ./conf/lenet_conf.yaml
2021-11-20 13:18:49,990 - Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers
2021-11-20 13:18:49,992 - Loading activation stats from: ./quant_stats/lenet-imagenet.yaml
2021-11-20 13:18:50,310 - Preparing model for quantization using PostTrainLinearQuantizer
2021-11-20 13:18:53,252 - Applying batch-norm folding ahead of post-training quantization
2021-11-20 13:18:53,254 - Fusing sequence ['module.conv1.conv', 'module.conv1.bn']
2021-11-20 13:18:53,255 - Fusing sequence ['module.conv2.conv', 'module.conv2.bn']
2021-11-20 13:18:53,255 - Fusing sequence ['module.conv3.conv', 'module.conv3.bn']
2021-11-20 13:18:53,255 - Fusing sequence ['module.inception3a.branch1.conv', 'module.inception3a.branch1.bn']
2021-11-20 13:18:53,256 - Fusing sequence ['module.inception3a.branch2.0.conv', 'module.inception3a.branch2.0.bn']
2021-11-20 13:18:53,256 - Fusing sequence ['module.inception3a.branch2.1.conv', 'module.inception3a.branch2.1.bn']
2021-11-20 13:18:53,256 - Fusing sequence ['module.inception3a.branch3.0.conv', 'module.inception3a.branch3.0.bn']
2021-11-20 13:18:53,257 - Fusing sequence ['module.inception3a.branch3.1.conv', 'module.inception3a.branch3.1.bn']
2021-11-20 13:18:53,257 - Fusing sequence ['module.inception3a.branch4.1.conv', 'module.inception3a.branch4.1.bn']
2021-11-20 13:18:53,257 - Fusing sequence ['module.inception3b.branch1.conv', 'module.inception3b.branch1.bn']
2021-11-20 13:18:53,258 - Fusing sequence ['module.inception3b.branch2.0.conv', 'module.inception3b.branch2.0.bn']
2021-11-20 13:18:53,258 - Fusing sequence ['module.inception3b.branch2.1.conv', 'module.inception3b.branch2.1.bn']
2021-11-20 13:18:53,258 - Fusing sequence ['module.inception3b.branch3.0.conv', 'module.inception3b.branch3.0.bn']
2021-11-20 13:18:53,258 - Fusing sequence ['module.inception3b.branch3.1.conv', 'module.inception3b.branch3.1.bn']
2021-11-20 13:18:53,259 - Fusing sequence ['module.inception3b.branch4.1.conv', 'module.inception3b.branch4.1.bn']
2021-11-20 13:18:53,259 - Fusing sequence ['module.inception4a.branch1.conv', 'module.inception4a.branch1.bn']
2021-11-20 13:18:53,259 - Fusing sequence ['module.inception4a.branch2.0.conv', 'module.inception4a.branch2.0.bn']
2021-11-20 13:18:53,260 - Fusing sequence ['module.inception4a.branch2.1.conv', 'module.inception4a.branch2.1.bn']
2021-11-20 13:18:53,260 - Fusing sequence ['module.inception4a.branch3.0.conv', 'module.inception4a.branch3.0.bn']
2021-11-20 13:18:53,260 - Fusing sequence ['module.inception4a.branch3.1.conv', 'module.inception4a.branch3.1.bn']
2021-11-20 13:18:53,261 - Fusing sequence ['module.inception4a.branch4.1.conv', 'module.inception4a.branch4.1.bn']
2021-11-20 13:18:53,261 - Fusing sequence ['module.inception4b.branch1.conv', 'module.inception4b.branch1.bn']
2021-11-20 13:18:53,261 - Fusing sequence ['module.inception4b.branch2.0.conv', 'module.inception4b.branch2.0.bn']
2021-11-20 13:18:53,262 - Fusing sequence ['module.inception4b.branch2.1.conv', 'module.inception4b.branch2.1.bn']
2021-11-20 13:18:53,262 - Fusing sequence ['module.inception4b.branch3.0.conv', 'module.inception4b.branch3.0.bn']
2021-11-20 13:18:53,262 - Fusing sequence ['module.inception4b.branch3.1.conv', 'module.inception4b.branch3.1.bn']
2021-11-20 13:18:53,263 - Fusing sequence ['module.inception4b.branch4.1.conv', 'module.inception4b.branch4.1.bn']
2021-11-20 13:18:53,263 - Fusing sequence ['module.inception4c.branch1.conv', 'module.inception4c.branch1.bn']
2021-11-20 13:18:53,263 - Fusing sequence ['module.inception4c.branch2.0.conv', 'module.inception4c.branch2.0.bn']
2021-11-20 13:18:53,263 - Fusing sequence ['module.inception4c.branch2.1.conv', 'module.inception4c.branch2.1.bn']
2021-11-20 13:18:53,264 - Fusing sequence ['module.inception4c.branch3.0.conv', 'module.inception4c.branch3.0.bn']
2021-11-20 13:18:53,264 - Fusing sequence ['module.inception4c.branch3.1.conv', 'module.inception4c.branch3.1.bn']
2021-11-20 13:18:53,265 - Fusing sequence ['module.inception4c.branch4.1.conv', 'module.inception4c.branch4.1.bn']
2021-11-20 13:18:53,265 - Fusing sequence ['module.inception4d.branch1.conv', 'module.inception4d.branch1.bn']
2021-11-20 13:18:53,265 - Fusing sequence ['module.inception4d.branch2.0.conv', 'module.inception4d.branch2.0.bn']
2021-11-20 13:18:53,265 - Fusing sequence ['module.inception4d.branch2.1.conv', 'module.inception4d.branch2.1.bn']
2021-11-20 13:18:53,266 - Fusing sequence ['module.inception4d.branch3.0.conv', 'module.inception4d.branch3.0.bn']
2021-11-20 13:18:53,266 - Fusing sequence ['module.inception4d.branch3.1.conv', 'module.inception4d.branch3.1.bn']
2021-11-20 13:18:53,266 - Fusing sequence ['module.inception4d.branch4.1.conv', 'module.inception4d.branch4.1.bn']
2021-11-20 13:18:53,267 - Fusing sequence ['module.inception4e.branch1.conv', 'module.inception4e.branch1.bn']
2021-11-20 13:18:53,267 - Fusing sequence ['module.inception4e.branch2.0.conv', 'module.inception4e.branch2.0.bn']
2021-11-20 13:18:53,267 - Fusing sequence ['module.inception4e.branch2.1.conv', 'module.inception4e.branch2.1.bn']
2021-11-20 13:18:53,268 - Fusing sequence ['module.inception4e.branch3.0.conv', 'module.inception4e.branch3.0.bn']
2021-11-20 13:18:53,268 - Fusing sequence ['module.inception4e.branch3.1.conv', 'module.inception4e.branch3.1.bn']
2021-11-20 13:18:53,268 - Fusing sequence ['module.inception4e.branch4.1.conv', 'module.inception4e.branch4.1.bn']
2021-11-20 13:18:53,269 - Fusing sequence ['module.inception5a.branch1.conv', 'module.inception5a.branch1.bn']
2021-11-20 13:18:53,269 - Fusing sequence ['module.inception5a.branch2.0.conv', 'module.inception5a.branch2.0.bn']
2021-11-20 13:18:53,269 - Fusing sequence ['module.inception5a.branch2.1.conv', 'module.inception5a.branch2.1.bn']
2021-11-20 13:18:53,270 - Fusing sequence ['module.inception5a.branch3.0.conv', 'module.inception5a.branch3.0.bn']
2021-11-20 13:18:53,270 - Fusing sequence ['module.inception5a.branch3.1.conv', 'module.inception5a.branch3.1.bn']
2021-11-20 13:18:53,270 - Fusing sequence ['module.inception5a.branch4.1.conv', 'module.inception5a.branch4.1.bn']
2021-11-20 13:18:53,271 - Fusing sequence ['module.inception5b.branch1.conv', 'module.inception5b.branch1.bn']
2021-11-20 13:18:53,271 - Fusing sequence ['module.inception5b.branch2.0.conv', 'module.inception5b.branch2.0.bn']
2021-11-20 13:18:53,271 - Fusing sequence ['module.inception5b.branch2.1.conv', 'module.inception5b.branch2.1.bn']
2021-11-20 13:18:53,271 - Fusing sequence ['module.inception5b.branch3.0.conv', 'module.inception5b.branch3.0.bn']
2021-11-20 13:18:53,272 - Fusing sequence ['module.inception5b.branch3.1.conv', 'module.inception5b.branch3.1.bn']
2021-11-20 13:18:53,272 - Fusing sequence ['module.inception5b.branch4.1.conv', 'module.inception5b.branch4.1.bn']
2021-11-20 13:18:54,348 - Propagating output statistics from BN modules to folded modules
2021-11-20 13:18:54,348 -   conv1.bn --> module.conv1.conv
2021-11-20 13:18:54,348 -   conv2.bn --> module.conv2.conv
2021-11-20 13:18:54,348 -   conv3.bn --> module.conv3.conv
2021-11-20 13:18:54,348 -   inception3a.branch1.bn --> module.inception3a.branch1.conv
2021-11-20 13:18:54,349 -   inception3a.branch2.0.bn --> module.inception3a.branch2.0.conv
2021-11-20 13:18:54,349 -   inception3a.branch2.1.bn --> module.inception3a.branch2.1.conv
2021-11-20 13:18:54,349 -   inception3a.branch3.0.bn --> module.inception3a.branch3.0.conv
2021-11-20 13:18:54,349 -   inception3a.branch3.1.bn --> module.inception3a.branch3.1.conv
2021-11-20 13:18:54,349 -   inception3a.branch4.1.bn --> module.inception3a.branch4.1.conv
2021-11-20 13:18:54,349 -   inception3b.branch1.bn --> module.inception3b.branch1.conv
2021-11-20 13:18:54,349 -   inception3b.branch2.0.bn --> module.inception3b.branch2.0.conv
2021-11-20 13:18:54,349 -   inception3b.branch2.1.bn --> module.inception3b.branch2.1.conv
2021-11-20 13:18:54,349 -   inception3b.branch3.0.bn --> module.inception3b.branch3.0.conv
2021-11-20 13:18:54,349 -   inception3b.branch3.1.bn --> module.inception3b.branch3.1.conv
2021-11-20 13:18:54,349 -   inception3b.branch4.1.bn --> module.inception3b.branch4.1.conv
2021-11-20 13:18:54,349 -   inception4a.branch1.bn --> module.inception4a.branch1.conv
2021-11-20 13:18:54,349 -   inception4a.branch2.0.bn --> module.inception4a.branch2.0.conv
2021-11-20 13:18:54,349 -   inception4a.branch2.1.bn --> module.inception4a.branch2.1.conv
2021-11-20 13:18:54,349 -   inception4a.branch3.0.bn --> module.inception4a.branch3.0.conv
2021-11-20 13:18:54,349 -   inception4a.branch3.1.bn --> module.inception4a.branch3.1.conv
2021-11-20 13:18:54,349 -   inception4a.branch4.1.bn --> module.inception4a.branch4.1.conv
2021-11-20 13:18:54,349 -   inception4b.branch1.bn --> module.inception4b.branch1.conv
2021-11-20 13:18:54,349 -   inception4b.branch2.0.bn --> module.inception4b.branch2.0.conv
2021-11-20 13:18:54,349 -   inception4b.branch2.1.bn --> module.inception4b.branch2.1.conv
2021-11-20 13:18:54,349 -   inception4b.branch3.0.bn --> module.inception4b.branch3.0.conv
2021-11-20 13:18:54,349 -   inception4b.branch3.1.bn --> module.inception4b.branch3.1.conv
2021-11-20 13:18:54,349 -   inception4b.branch4.1.bn --> module.inception4b.branch4.1.conv
2021-11-20 13:18:54,349 -   inception4c.branch1.bn --> module.inception4c.branch1.conv
2021-11-20 13:18:54,349 -   inception4c.branch2.0.bn --> module.inception4c.branch2.0.conv
2021-11-20 13:18:54,350 -   inception4c.branch2.1.bn --> module.inception4c.branch2.1.conv
2021-11-20 13:18:54,350 -   inception4c.branch3.0.bn --> module.inception4c.branch3.0.conv
2021-11-20 13:18:54,350 -   inception4c.branch3.1.bn --> module.inception4c.branch3.1.conv
2021-11-20 13:18:54,350 -   inception4c.branch4.1.bn --> module.inception4c.branch4.1.conv
2021-11-20 13:18:54,350 -   inception4d.branch1.bn --> module.inception4d.branch1.conv
2021-11-20 13:18:54,350 -   inception4d.branch2.0.bn --> module.inception4d.branch2.0.conv
2021-11-20 13:18:54,350 -   inception4d.branch2.1.bn --> module.inception4d.branch2.1.conv
2021-11-20 13:18:54,350 -   inception4d.branch3.0.bn --> module.inception4d.branch3.0.conv
2021-11-20 13:18:54,350 -   inception4d.branch3.1.bn --> module.inception4d.branch3.1.conv
2021-11-20 13:18:54,350 -   inception4d.branch4.1.bn --> module.inception4d.branch4.1.conv
2021-11-20 13:18:54,350 -   inception4e.branch1.bn --> module.inception4e.branch1.conv
2021-11-20 13:18:54,350 -   inception4e.branch2.0.bn --> module.inception4e.branch2.0.conv
2021-11-20 13:18:54,350 -   inception4e.branch2.1.bn --> module.inception4e.branch2.1.conv
2021-11-20 13:18:54,350 -   inception4e.branch3.0.bn --> module.inception4e.branch3.0.conv
2021-11-20 13:18:54,350 -   inception4e.branch3.1.bn --> module.inception4e.branch3.1.conv
2021-11-20 13:18:54,350 -   inception4e.branch4.1.bn --> module.inception4e.branch4.1.conv
2021-11-20 13:18:54,350 -   inception5a.branch1.bn --> module.inception5a.branch1.conv
2021-11-20 13:18:54,350 -   inception5a.branch2.0.bn --> module.inception5a.branch2.0.conv
2021-11-20 13:18:54,350 -   inception5a.branch2.1.bn --> module.inception5a.branch2.1.conv
2021-11-20 13:18:54,350 -   inception5a.branch3.0.bn --> module.inception5a.branch3.0.conv
2021-11-20 13:18:54,350 -   inception5a.branch3.1.bn --> module.inception5a.branch3.1.conv
2021-11-20 13:18:54,350 -   inception5a.branch4.1.bn --> module.inception5a.branch4.1.conv
2021-11-20 13:18:54,350 -   inception5b.branch1.bn --> module.inception5b.branch1.conv
2021-11-20 13:18:54,350 -   inception5b.branch2.0.bn --> module.inception5b.branch2.0.conv
2021-11-20 13:18:54,350 -   inception5b.branch2.1.bn --> module.inception5b.branch2.1.conv
2021-11-20 13:18:54,351 -   inception5b.branch3.0.bn --> module.inception5b.branch3.0.conv
2021-11-20 13:18:54,351 -   inception5b.branch3.1.bn --> module.inception5b.branch3.1.conv
2021-11-20 13:18:54,351 -   inception5b.branch4.1.bn --> module.inception5b.branch4.1.conv
2021-11-20 13:18:54,351 - Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid
2021-11-20 13:18:54,351 -   Module conv1.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module conv2.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module conv3.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module inception3a.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module inception3a.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module inception3a.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,351 -   Module inception3a.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3a.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3a.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception3b.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4a.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4b.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4c.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,352 -   Module inception4c.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4c.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4c.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4c.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4c.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4d.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception4e.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,353 -   Module inception5a.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch1.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch2.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch2.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch3.0.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch3.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,354 -   Module inception5b.branch4.1.conv followed by Relu, updating stats
2021-11-20 13:18:54,638 - Updated stats saved to logs/googlenet-imagenet-helmet___2021.11.20-131847/quant_stats_after_prepare_model.yaml
2021-11-20 13:18:54,638 - Module module
2021-11-20 13:18:54,638 - 	Skipping
2021-11-20 13:18:54,638 - Module module.conv1
2021-11-20 13:18:54,638 - 	Skipping
2021-11-20 13:18:54,641 - Module module.conv1.conv
2021-11-20 13:18:54,641 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,641 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,643 - Module module.maxpool1
2021-11-20 13:18:54,643 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,643 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,645 - Module module.conv2
2021-11-20 13:18:54,645 - 	Skipping
2021-11-20 13:18:54,646 - Module module.conv2.conv
2021-11-20 13:18:54,646 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,646 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,648 - Module module.conv3
2021-11-20 13:18:54,648 - 	Skipping
2021-11-20 13:18:54,649 - Module module.conv3.conv
2021-11-20 13:18:54,649 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,649 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,651 - Module module.maxpool2
2021-11-20 13:18:54,651 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,651 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,653 - Module module.inception3a
2021-11-20 13:18:54,653 - 	Skipping
2021-11-20 13:18:54,653 - Module module.inception3a.branch1
2021-11-20 13:18:54,653 - 	Skipping
2021-11-20 13:18:54,654 - Module module.inception3a.branch1.conv
2021-11-20 13:18:54,654 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,654 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,656 - Module module.inception3a.branch2
2021-11-20 13:18:54,656 - 	Skipping
2021-11-20 13:18:54,656 - Module module.inception3a.branch2.0
2021-11-20 13:18:54,656 - 	Skipping
2021-11-20 13:18:54,657 - Module module.inception3a.branch2.0.conv
2021-11-20 13:18:54,657 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,657 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,659 - Module module.inception3a.branch2.1
2021-11-20 13:18:54,659 - 	Skipping
2021-11-20 13:18:54,660 - Module module.inception3a.branch2.1.conv
2021-11-20 13:18:54,660 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,660 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,662 - Module module.inception3a.branch3
2021-11-20 13:18:54,662 - 	Skipping
2021-11-20 13:18:54,662 - Module module.inception3a.branch3.0
2021-11-20 13:18:54,662 - 	Skipping
2021-11-20 13:18:54,663 - Module module.inception3a.branch3.0.conv
2021-11-20 13:18:54,663 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,663 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,665 - Module module.inception3a.branch3.1
2021-11-20 13:18:54,665 - 	Skipping
2021-11-20 13:18:54,666 - Module module.inception3a.branch3.1.conv
2021-11-20 13:18:54,666 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,666 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,668 - Module module.inception3a.branch4
2021-11-20 13:18:54,668 - 	Skipping
2021-11-20 13:18:54,668 - Module module.inception3a.branch4.0
2021-11-20 13:18:54,668 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,668 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,670 - Module module.inception3a.branch4.1
2021-11-20 13:18:54,670 - 	Skipping
2021-11-20 13:18:54,671 - Module module.inception3a.branch4.1.conv
2021-11-20 13:18:54,671 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,671 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,673 - Module module.inception3b
2021-11-20 13:18:54,673 - 	Skipping
2021-11-20 13:18:54,673 - Module module.inception3b.branch1
2021-11-20 13:18:54,673 - 	Skipping
2021-11-20 13:18:54,674 - Module module.inception3b.branch1.conv
2021-11-20 13:18:54,674 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,674 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,676 - Module module.inception3b.branch2
2021-11-20 13:18:54,676 - 	Skipping
2021-11-20 13:18:54,676 - Module module.inception3b.branch2.0
2021-11-20 13:18:54,676 - 	Skipping
2021-11-20 13:18:54,677 - Module module.inception3b.branch2.0.conv
2021-11-20 13:18:54,677 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,678 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,679 - Module module.inception3b.branch2.1
2021-11-20 13:18:54,679 - 	Skipping
2021-11-20 13:18:54,680 - Module module.inception3b.branch2.1.conv
2021-11-20 13:18:54,680 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,680 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,682 - Module module.inception3b.branch3
2021-11-20 13:18:54,682 - 	Skipping
2021-11-20 13:18:54,682 - Module module.inception3b.branch3.0
2021-11-20 13:18:54,682 - 	Skipping
2021-11-20 13:18:54,683 - Module module.inception3b.branch3.0.conv
2021-11-20 13:18:54,683 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,684 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,685 - Module module.inception3b.branch3.1
2021-11-20 13:18:54,685 - 	Skipping
2021-11-20 13:18:54,686 - Module module.inception3b.branch3.1.conv
2021-11-20 13:18:54,687 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,687 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,688 - Module module.inception3b.branch4
2021-11-20 13:18:54,688 - 	Skipping
2021-11-20 13:18:54,689 - Module module.inception3b.branch4.0
2021-11-20 13:18:54,689 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,689 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,691 - Module module.inception3b.branch4.1
2021-11-20 13:18:54,691 - 	Skipping
2021-11-20 13:18:54,692 - Module module.inception3b.branch4.1.conv
2021-11-20 13:18:54,692 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,692 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,694 - Module module.maxpool3
2021-11-20 13:18:54,694 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,694 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,696 - Module module.inception4a
2021-11-20 13:18:54,696 - 	Skipping
2021-11-20 13:18:54,696 - Module module.inception4a.branch1
2021-11-20 13:18:54,696 - 	Skipping
2021-11-20 13:18:54,697 - Module module.inception4a.branch1.conv
2021-11-20 13:18:54,697 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,697 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,699 - Module module.inception4a.branch2
2021-11-20 13:18:54,699 - 	Skipping
2021-11-20 13:18:54,699 - Module module.inception4a.branch2.0
2021-11-20 13:18:54,699 - 	Skipping
2021-11-20 13:18:54,700 - Module module.inception4a.branch2.0.conv
2021-11-20 13:18:54,701 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,701 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,702 - Module module.inception4a.branch2.1
2021-11-20 13:18:54,702 - 	Skipping
2021-11-20 13:18:54,704 - Module module.inception4a.branch2.1.conv
2021-11-20 13:18:54,704 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,704 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,706 - Module module.inception4a.branch3
2021-11-20 13:18:54,706 - 	Skipping
2021-11-20 13:18:54,706 - Module module.inception4a.branch3.0
2021-11-20 13:18:54,706 - 	Skipping
2021-11-20 13:18:54,707 - Module module.inception4a.branch3.0.conv
2021-11-20 13:18:54,707 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,707 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,709 - Module module.inception4a.branch3.1
2021-11-20 13:18:54,709 - 	Skipping
2021-11-20 13:18:54,710 - Module module.inception4a.branch3.1.conv
2021-11-20 13:18:54,710 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,710 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,712 - Module module.inception4a.branch4
2021-11-20 13:18:54,712 - 	Skipping
2021-11-20 13:18:54,712 - Module module.inception4a.branch4.0
2021-11-20 13:18:54,713 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,713 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,714 - Module module.inception4a.branch4.1
2021-11-20 13:18:54,714 - 	Skipping
2021-11-20 13:18:54,716 - Module module.inception4a.branch4.1.conv
2021-11-20 13:18:54,716 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,716 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,717 - Module module.inception4b
2021-11-20 13:18:54,718 - 	Skipping
2021-11-20 13:18:54,718 - Module module.inception4b.branch1
2021-11-20 13:18:54,718 - 	Skipping
2021-11-20 13:18:54,719 - Module module.inception4b.branch1.conv
2021-11-20 13:18:54,719 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,719 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,721 - Module module.inception4b.branch2
2021-11-20 13:18:54,721 - 	Skipping
2021-11-20 13:18:54,721 - Module module.inception4b.branch2.0
2021-11-20 13:18:54,721 - 	Skipping
2021-11-20 13:18:54,722 - Module module.inception4b.branch2.0.conv
2021-11-20 13:18:54,722 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,722 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,724 - Module module.inception4b.branch2.1
2021-11-20 13:18:54,724 - 	Skipping
2021-11-20 13:18:54,726 - Module module.inception4b.branch2.1.conv
2021-11-20 13:18:54,726 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,726 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,727 - Module module.inception4b.branch3
2021-11-20 13:18:54,728 - 	Skipping
2021-11-20 13:18:54,728 - Module module.inception4b.branch3.0
2021-11-20 13:18:54,728 - 	Skipping
2021-11-20 13:18:54,729 - Module module.inception4b.branch3.0.conv
2021-11-20 13:18:54,729 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,729 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,731 - Module module.inception4b.branch3.1
2021-11-20 13:18:54,731 - 	Skipping
2021-11-20 13:18:54,732 - Module module.inception4b.branch3.1.conv
2021-11-20 13:18:54,732 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,732 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,734 - Module module.inception4b.branch4
2021-11-20 13:18:54,734 - 	Skipping
2021-11-20 13:18:54,734 - Module module.inception4b.branch4.0
2021-11-20 13:18:54,735 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,735 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,737 - Module module.inception4b.branch4.1
2021-11-20 13:18:54,737 - 	Skipping
2021-11-20 13:18:54,738 - Module module.inception4b.branch4.1.conv
2021-11-20 13:18:54,738 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,738 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,740 - Module module.inception4c
2021-11-20 13:18:54,740 - 	Skipping
2021-11-20 13:18:54,740 - Module module.inception4c.branch1
2021-11-20 13:18:54,741 - 	Skipping
2021-11-20 13:18:54,742 - Module module.inception4c.branch1.conv
2021-11-20 13:18:54,742 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,742 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,744 - Module module.inception4c.branch2
2021-11-20 13:18:54,744 - 	Skipping
2021-11-20 13:18:54,744 - Module module.inception4c.branch2.0
2021-11-20 13:18:54,744 - 	Skipping
2021-11-20 13:18:54,745 - Module module.inception4c.branch2.0.conv
2021-11-20 13:18:54,745 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,745 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,747 - Module module.inception4c.branch2.1
2021-11-20 13:18:54,747 - 	Skipping
2021-11-20 13:18:54,748 - Module module.inception4c.branch2.1.conv
2021-11-20 13:18:54,748 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,748 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,750 - Module module.inception4c.branch3
2021-11-20 13:18:54,750 - 	Skipping
2021-11-20 13:18:54,751 - Module module.inception4c.branch3.0
2021-11-20 13:18:54,751 - 	Skipping
2021-11-20 13:18:54,752 - Module module.inception4c.branch3.0.conv
2021-11-20 13:18:54,752 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,752 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,754 - Module module.inception4c.branch3.1
2021-11-20 13:18:54,754 - 	Skipping
2021-11-20 13:18:54,755 - Module module.inception4c.branch3.1.conv
2021-11-20 13:18:54,755 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,755 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,757 - Module module.inception4c.branch4
2021-11-20 13:18:54,757 - 	Skipping
2021-11-20 13:18:54,758 - Module module.inception4c.branch4.0
2021-11-20 13:18:54,758 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,758 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,760 - Module module.inception4c.branch4.1
2021-11-20 13:18:54,760 - 	Skipping
2021-11-20 13:18:54,761 - Module module.inception4c.branch4.1.conv
2021-11-20 13:18:54,761 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,761 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,763 - Module module.inception4d
2021-11-20 13:18:54,763 - 	Skipping
2021-11-20 13:18:54,763 - Module module.inception4d.branch1
2021-11-20 13:18:54,763 - 	Skipping
2021-11-20 13:18:54,764 - Module module.inception4d.branch1.conv
2021-11-20 13:18:54,764 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,764 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,766 - Module module.inception4d.branch2
2021-11-20 13:18:54,766 - 	Skipping
2021-11-20 13:18:54,767 - Module module.inception4d.branch2.0
2021-11-20 13:18:54,767 - 	Skipping
2021-11-20 13:18:54,768 - Module module.inception4d.branch2.0.conv
2021-11-20 13:18:54,768 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,768 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,770 - Module module.inception4d.branch2.1
2021-11-20 13:18:54,770 - 	Skipping
2021-11-20 13:18:54,771 - Module module.inception4d.branch2.1.conv
2021-11-20 13:18:54,771 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,771 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,773 - Module module.inception4d.branch3
2021-11-20 13:18:54,773 - 	Skipping
2021-11-20 13:18:54,773 - Module module.inception4d.branch3.0
2021-11-20 13:18:54,773 - 	Skipping
2021-11-20 13:18:54,775 - Module module.inception4d.branch3.0.conv
2021-11-20 13:18:54,775 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,775 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,777 - Module module.inception4d.branch3.1
2021-11-20 13:18:54,777 - 	Skipping
2021-11-20 13:18:54,778 - Module module.inception4d.branch3.1.conv
2021-11-20 13:18:54,778 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,778 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,780 - Module module.inception4d.branch4
2021-11-20 13:18:54,780 - 	Skipping
2021-11-20 13:18:54,781 - Module module.inception4d.branch4.0
2021-11-20 13:18:54,781 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,781 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,783 - Module module.inception4d.branch4.1
2021-11-20 13:18:54,783 - 	Skipping
2021-11-20 13:18:54,784 - Module module.inception4d.branch4.1.conv
2021-11-20 13:18:54,784 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,784 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,786 - Module module.inception4e
2021-11-20 13:18:54,786 - 	Skipping
2021-11-20 13:18:54,787 - Module module.inception4e.branch1
2021-11-20 13:18:54,787 - 	Skipping
2021-11-20 13:18:54,788 - Module module.inception4e.branch1.conv
2021-11-20 13:18:54,788 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,788 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,790 - Module module.inception4e.branch2
2021-11-20 13:18:54,790 - 	Skipping
2021-11-20 13:18:54,790 - Module module.inception4e.branch2.0
2021-11-20 13:18:54,790 - 	Skipping
2021-11-20 13:18:54,791 - Module module.inception4e.branch2.0.conv
2021-11-20 13:18:54,791 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,791 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,793 - Module module.inception4e.branch2.1
2021-11-20 13:18:54,794 - 	Skipping
2021-11-20 13:18:54,795 - Module module.inception4e.branch2.1.conv
2021-11-20 13:18:54,795 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,795 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,797 - Module module.inception4e.branch3
2021-11-20 13:18:54,797 - 	Skipping
2021-11-20 13:18:54,797 - Module module.inception4e.branch3.0
2021-11-20 13:18:54,797 - 	Skipping
2021-11-20 13:18:54,798 - Module module.inception4e.branch3.0.conv
2021-11-20 13:18:54,798 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,798 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,801 - Module module.inception4e.branch3.1
2021-11-20 13:18:54,801 - 	Skipping
2021-11-20 13:18:54,802 - Module module.inception4e.branch3.1.conv
2021-11-20 13:18:54,802 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,802 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,804 - Module module.inception4e.branch4
2021-11-20 13:18:54,804 - 	Skipping
2021-11-20 13:18:54,805 - Module module.inception4e.branch4.0
2021-11-20 13:18:54,805 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,805 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,807 - Module module.inception4e.branch4.1
2021-11-20 13:18:54,807 - 	Skipping
2021-11-20 13:18:54,808 - Module module.inception4e.branch4.1.conv
2021-11-20 13:18:54,808 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,808 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,811 - Module module.maxpool4
2021-11-20 13:18:54,811 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,811 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,813 - Module module.inception5a
2021-11-20 13:18:54,813 - 	Skipping
2021-11-20 13:18:54,813 - Module module.inception5a.branch1
2021-11-20 13:18:54,813 - 	Skipping
2021-11-20 13:18:54,814 - Module module.inception5a.branch1.conv
2021-11-20 13:18:54,815 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,815 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,817 - Module module.inception5a.branch2
2021-11-20 13:18:54,817 - 	Skipping
2021-11-20 13:18:54,817 - Module module.inception5a.branch2.0
2021-11-20 13:18:54,817 - 	Skipping
2021-11-20 13:18:54,818 - Module module.inception5a.branch2.0.conv
2021-11-20 13:18:54,818 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,818 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,821 - Module module.inception5a.branch2.1
2021-11-20 13:18:54,821 - 	Skipping
2021-11-20 13:18:54,822 - Module module.inception5a.branch2.1.conv
2021-11-20 13:18:54,822 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,822 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,824 - Module module.inception5a.branch3
2021-11-20 13:18:54,824 - 	Skipping
2021-11-20 13:18:54,824 - Module module.inception5a.branch3.0
2021-11-20 13:18:54,824 - 	Skipping
2021-11-20 13:18:54,826 - Module module.inception5a.branch3.0.conv
2021-11-20 13:18:54,826 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,826 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,828 - Module module.inception5a.branch3.1
2021-11-20 13:18:54,828 - 	Skipping
2021-11-20 13:18:54,829 - Module module.inception5a.branch3.1.conv
2021-11-20 13:18:54,829 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,829 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,832 - Module module.inception5a.branch4
2021-11-20 13:18:54,832 - 	Skipping
2021-11-20 13:18:54,832 - Module module.inception5a.branch4.0
2021-11-20 13:18:54,832 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,832 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,835 - Module module.inception5a.branch4.1
2021-11-20 13:18:54,835 - 	Skipping
2021-11-20 13:18:54,837 - Module module.inception5a.branch4.1.conv
2021-11-20 13:18:54,837 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,837 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,839 - Module module.inception5b
2021-11-20 13:18:54,839 - 	Skipping
2021-11-20 13:18:54,839 - Module module.inception5b.branch1
2021-11-20 13:18:54,839 - 	Skipping
2021-11-20 13:18:54,841 - Module module.inception5b.branch1.conv
2021-11-20 13:18:54,841 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,841 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,843 - Module module.inception5b.branch2
2021-11-20 13:18:54,843 - 	Skipping
2021-11-20 13:18:54,843 - Module module.inception5b.branch2.0
2021-11-20 13:18:54,843 - 	Skipping
2021-11-20 13:18:54,844 - Module module.inception5b.branch2.0.conv
2021-11-20 13:18:54,845 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,845 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,847 - Module module.inception5b.branch2.1
2021-11-20 13:18:54,847 - 	Skipping
2021-11-20 13:18:54,848 - Module module.inception5b.branch2.1.conv
2021-11-20 13:18:54,848 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,848 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,851 - Module module.inception5b.branch3
2021-11-20 13:18:54,851 - 	Skipping
2021-11-20 13:18:54,851 - Module module.inception5b.branch3.0
2021-11-20 13:18:54,851 - 	Skipping
2021-11-20 13:18:54,852 - Module module.inception5b.branch3.0.conv
2021-11-20 13:18:54,852 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,852 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,855 - Module module.inception5b.branch3.1
2021-11-20 13:18:54,855 - 	Skipping
2021-11-20 13:18:54,856 - Module module.inception5b.branch3.1.conv
2021-11-20 13:18:54,856 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,856 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,858 - Module module.inception5b.branch4
2021-11-20 13:18:54,858 - 	Skipping
2021-11-20 13:18:54,859 - Module module.inception5b.branch4.0
2021-11-20 13:18:54,859 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-20 13:18:54,859 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,861 - Module module.inception5b.branch4.1
2021-11-20 13:18:54,862 - 	Skipping
2021-11-20 13:18:54,863 - Module module.inception5b.branch4.1.conv
2021-11-20 13:18:54,863 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:18:54,863 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,866 - Module module.avgpool
2021-11-20 13:18:54,866 - 	Replacing: torch.nn.modules.pooling.AdaptiveAvgPool2d
2021-11-20 13:18:54,866 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:18:54,868 - Module module.dropout
2021-11-20 13:18:54,868 - 	Skipping
2021-11-20 13:18:54,870 - Module module.fc
2021-11-20 13:18:54,870 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-20 13:18:54,870 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:18:54,874 - Parameter 'module.conv1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.conv1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.conv2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.conv2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.conv3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.conv3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,874 - Parameter 'module.inception3a.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3a.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3a.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3a.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3a.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3a.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,875 - Parameter 'module.inception3b.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4a.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,876 - Parameter 'module.inception4b.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4b.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,877 - Parameter 'module.inception4c.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4d.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,878 - Parameter 'module.inception4e.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception4e.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,879 - Parameter 'module.inception5a.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch2.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch2.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch2.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch2.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch3.0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch3.0.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch3.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch3.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch4.1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.inception5b.branch4.1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,880 - Parameter 'module.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:18:54,880 - Parameter 'module.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:18:54,898 - Quantized model:

DataParallel(
  (module): GoogLeNet(
    (conv1): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=19.794741, output_zero_point=0.000000
        weights_scale=52.628613, weights_zero_point=-141.000000
        (wrapped_module): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
      )
      (bn): Identity()
    )
    (maxpool1): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=19.794739, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    )
    (conv2): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=32.234566, output_zero_point=0.000000
        weights_scale=48.206913, weights_zero_point=-140.000000
        (wrapped_module): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (bn): Identity()
    )
    (conv3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=32.474514, output_zero_point=0.000000
        weights_scale=146.348602, weights_zero_point=-133.000000
        (wrapped_module): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (bn): Identity()
    )
    (maxpool2): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=32.474518, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    )
    (inception3a): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.901630, output_zero_point=0.000000
          weights_scale=156.408112, weights_zero_point=-102.000000
          (wrapped_module): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=39.071026, output_zero_point=0.000000
            weights_scale=129.148117, weights_zero_point=-175.000000
            (wrapped_module): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.038952, output_zero_point=0.000000
            weights_scale=221.991409, weights_zero_point=-95.000000
            (wrapped_module): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=48.043613, output_zero_point=0.000000
            weights_scale=238.067215, weights_zero_point=-94.000000
            (wrapped_module): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=33.217621, output_zero_point=0.000000
            weights_scale=96.854530, weights_zero_point=-84.000000
            (wrapped_module): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.474518, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=57.398308, output_zero_point=0.000000
            weights_scale=127.162895, weights_zero_point=-156.000000
            (wrapped_module): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception3b): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.590420, output_zero_point=0.000000
          weights_scale=120.407204, weights_zero_point=-139.000000
          (wrapped_module): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=45.170135, output_zero_point=0.000000
            weights_scale=185.198792, weights_zero_point=-132.000000
            (wrapped_module): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=45.305084, output_zero_point=0.000000
            weights_scale=281.997742, weights_zero_point=-93.000000
            (wrapped_module): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.011024, output_zero_point=0.000000
            weights_scale=190.592331, weights_zero_point=-128.000000
            (wrapped_module): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=53.749088, output_zero_point=0.000000
            weights_scale=241.876205, weights_zero_point=-121.000000
            (wrapped_module): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.356359, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=51.435131, output_zero_point=0.000000
            weights_scale=188.447815, weights_zero_point=-129.000000
            (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (maxpool3): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=37.082455, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    )
    (inception4a): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.760616, output_zero_point=0.000000
          weights_scale=195.341400, weights_zero_point=-108.000000
          (wrapped_module): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=39.571960, output_zero_point=0.000000
            weights_scale=192.789398, weights_zero_point=-67.000000
            (wrapped_module): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=44.047611, output_zero_point=0.000000
            weights_scale=253.467880, weights_zero_point=-82.000000
            (wrapped_module): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=46.956959, output_zero_point=0.000000
            weights_scale=327.034180, weights_zero_point=-153.000000
            (wrapped_module): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=50.120174, output_zero_point=0.000000
            weights_scale=107.290710, weights_zero_point=-90.000000
            (wrapped_module): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.082455, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=61.342426, output_zero_point=0.000000
            weights_scale=157.131897, weights_zero_point=-115.000000
            (wrapped_module): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception4b): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.263729, output_zero_point=0.000000
          weights_scale=167.274246, weights_zero_point=-129.000000
          (wrapped_module): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=40.690357, output_zero_point=0.000000
            weights_scale=168.635117, weights_zero_point=-112.000000
            (wrapped_module): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=43.851208, output_zero_point=0.000000
            weights_scale=209.578522, weights_zero_point=-76.000000
            (wrapped_module): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=47.288277, output_zero_point=0.000000
            weights_scale=210.757004, weights_zero_point=-128.000000
            (wrapped_module): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=54.682735, output_zero_point=0.000000
            weights_scale=215.837646, weights_zero_point=-104.000000
            (wrapped_module): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.036388, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=61.867256, output_zero_point=0.000000
            weights_scale=342.059937, weights_zero_point=-128.000000
            (wrapped_module): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception4c): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.743141, output_zero_point=0.000000
          weights_scale=188.639038, weights_zero_point=-140.000000
          (wrapped_module): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.857525, output_zero_point=0.000000
            weights_scale=173.395172, weights_zero_point=-121.000000
            (wrapped_module): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=44.979568, output_zero_point=0.000000
            weights_scale=336.736816, weights_zero_point=-104.000000
            (wrapped_module): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=50.212582, output_zero_point=0.000000
            weights_scale=158.206268, weights_zero_point=-94.000000
            (wrapped_module): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=44.512226, output_zero_point=0.000000
            weights_scale=116.648499, weights_zero_point=-91.000000
            (wrapped_module): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.518429, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=70.943390, output_zero_point=0.000000
            weights_scale=290.133728, weights_zero_point=-130.000000
            (wrapped_module): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception4d): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.898758, output_zero_point=0.000000
          weights_scale=191.444626, weights_zero_point=-133.000000
          (wrapped_module): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=40.380928, output_zero_point=0.000000
            weights_scale=191.357819, weights_zero_point=-100.000000
            (wrapped_module): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=41.548214, output_zero_point=0.000000
            weights_scale=346.617828, weights_zero_point=-118.000000
            (wrapped_module): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=45.990223, output_zero_point=0.000000
            weights_scale=218.448792, weights_zero_point=-129.000000
            (wrapped_module): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=55.102955, output_zero_point=0.000000
            weights_scale=204.724808, weights_zero_point=-93.000000
            (wrapped_module): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.756554, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=71.712570, output_zero_point=0.000000
            weights_scale=311.107941, weights_zero_point=-125.000000
            (wrapped_module): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception4e): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=50.943546, output_zero_point=0.000000
          weights_scale=175.343048, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.655457, output_zero_point=0.000000
            weights_scale=185.093491, weights_zero_point=-105.000000
            (wrapped_module): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=46.863026, output_zero_point=0.000000
            weights_scale=336.996033, weights_zero_point=-84.000000
            (wrapped_module): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.824467, output_zero_point=0.000000
            weights_scale=200.927994, weights_zero_point=-126.000000
            (wrapped_module): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=53.356922, output_zero_point=0.000000
            weights_scale=278.850677, weights_zero_point=-79.000000
            (wrapped_module): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.133553, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=68.312004, output_zero_point=0.000000
            weights_scale=313.331390, weights_zero_point=-91.000000
            (wrapped_module): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (maxpool4): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=38.848484, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)
    )
    (inception5a): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=51.007530, output_zero_point=0.000000
          weights_scale=94.782570, weights_zero_point=-181.000000
          (wrapped_module): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=47.428646, output_zero_point=0.000000
            weights_scale=169.627869, weights_zero_point=-116.000000
            (wrapped_module): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=56.432934, output_zero_point=0.000000
            weights_scale=331.158295, weights_zero_point=-123.000000
            (wrapped_module): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=51.647415, output_zero_point=0.000000
            weights_scale=263.635590, weights_zero_point=-122.000000
            (wrapped_module): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=73.297241, output_zero_point=0.000000
            weights_scale=344.201660, weights_zero_point=-103.000000
            (wrapped_module): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.848484, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=84.105614, output_zero_point=0.000000
            weights_scale=395.841156, weights_zero_point=-122.000000
            (wrapped_module): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (inception5b): Inception(
      (branch1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.793171, output_zero_point=0.000000
          weights_scale=67.361938, weights_zero_point=-102.000000
          (wrapped_module): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch2): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=60.919296, output_zero_point=0.000000
            weights_scale=173.085449, weights_zero_point=-79.000000
            (wrapped_module): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=35.929512, output_zero_point=0.000000
            weights_scale=174.770859, weights_zero_point=-80.000000
            (wrapped_module): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch3): Sequential(
        (0): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=67.603775, output_zero_point=0.000000
            weights_scale=174.569138, weights_zero_point=-86.000000
            (wrapped_module): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=37.991692, output_zero_point=0.000000
            weights_scale=133.212997, weights_zero_point=-71.000000
            (wrapped_module): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (bn): Identity()
        )
      )
      (branch4): Sequential(
        (0): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=45.912437, output_zero_point=0.000000
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
        )
        (1): BasicConv2d(
          (conv): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=54.134121, output_zero_point=0.000000
            weights_scale=175.746857, weights_zero_point=-57.000000
            (wrapped_module): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (bn): Identity()
        )
      )
    )
    (avgpool): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=127.127838, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (fc): RangeLinearQuantParamLayerWrapper(
      weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=24.665533, output_zero_point=-63.000000
      weights_scale=425.467896, weights_zero_point=-105.000000
      (wrapped_module): Linear(in_features=1024, out_features=1000, bias=True)
    )
  )
)

2021-11-20 13:18:55,180 - Per-layer quantization parameters saved to logs/googlenet-imagenet-helmet___2021.11.20-131847/layer_quant_params.yaml
2021-11-20 13:18:57,135 - --- test ---------------------
2021-11-20 13:18:57,136 - 50000 samples (256 per mini-batch)
2021-11-20 13:19:17,936 - Test: [   10/  195]    Loss 6.907756    Top1 0.156250    Top5 0.664062    
2021-11-20 13:19:26,454 - Test: [   20/  195]    Loss 6.907756    Top1 0.097656    Top5 0.625000    
2021-11-20 13:19:34,986 - Test: [   30/  195]    Loss 6.907756    Top1 0.104167    Top5 0.664062    
2021-11-20 13:19:43,613 - Test: [   40/  195]    Loss 6.907756    Top1 0.097656    Top5 0.634766    
2021-11-20 13:19:52,244 - Test: [   50/  195]    Loss 6.907756    Top1 0.093750    Top5 0.570313    
2021-11-20 13:20:00,956 - Test: [   60/  195]    Loss 6.907756    Top1 0.110677    Top5 0.572917    
2021-11-20 13:20:09,719 - Test: [   70/  195]    Loss 6.907756    Top1 0.100446    Top5 0.541295    
2021-11-20 13:20:18,544 - Test: [   80/  195]    Loss 6.907756    Top1 0.097656    Top5 0.522461    
2021-11-20 13:20:27,307 - Test: [   90/  195]    Loss 6.907756    Top1 0.086806    Top5 0.516493    
2021-11-20 13:20:36,071 - Test: [  100/  195]    Loss 6.907756    Top1 0.093750    Top5 0.511719    
2021-11-20 13:20:44,758 - Test: [  110/  195]    Loss 6.907756    Top1 0.102983    Top5 0.518466    
2021-11-20 13:20:53,710 - Test: [  120/  195]    Loss 6.907756    Top1 0.100911    Top5 0.517578    
2021-11-20 13:21:02,558 - Test: [  130/  195]    Loss 6.907756    Top1 0.108173    Top5 0.513822    
2021-11-20 13:21:11,427 - Test: [  140/  195]    Loss 6.907756    Top1 0.106027    Top5 0.507813    
2021-11-20 13:21:20,191 - Test: [  150/  195]    Loss 6.907756    Top1 0.101562    Top5 0.507813    
2021-11-20 13:21:28,765 - Test: [  160/  195]    Loss 6.907756    Top1 0.102539    Top5 0.505371    
2021-11-20 13:21:37,216 - Test: [  170/  195]    Loss 6.907756    Top1 0.103401    Top5 0.500919    
2021-11-20 13:21:45,788 - Test: [  180/  195]    Loss 6.907756    Top1 0.099826    Top5 0.505642    
2021-11-20 13:21:54,237 - Test: [  190/  195]    Loss 6.907756    Top1 0.102796    Top5 0.503701    
2021-11-20 13:22:00,557 - ==> Top1: 0.100    Top5: 0.500    Loss: 6.908

2021-11-20 13:22:00,619 - 
2021-11-20 13:22:00,619 - Log file for this run: /home/th.nguyen/drift-encode/logs/googlenet-imagenet-helmet___2021.11.20-131847/googlenet-imagenet-helmet___2021.11.20-131847.log
