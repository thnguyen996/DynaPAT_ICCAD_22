2021-11-13 17:14:04,091 - Log file for this run: /home/th.nguyen/drift-encode/logs/resnet50-imagenet-grayencode___2021.11.13-171404/resnet50-imagenet-grayencode___2021.11.13-171404.log
2021-11-13 17:14:04,091 - Number of CPUs: 56
2021-11-13 17:14:04,289 - Number of GPUs: 6
2021-11-13 17:14:04,289 - CUDA version: 10.0.130
2021-11-13 17:14:04,291 - CUDNN version: 7603
2021-11-13 17:14:04,291 - Kernel: 5.4.0-86-generic
2021-11-13 17:14:04,291 - Python: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
[GCC 7.5.0]
2021-11-13 17:14:04,291 - pip freeze: {'absl-py': '0.12.0', 'aiohttp': '3.7.4.post0', 'argon2-cffi': '21.1.0', 'astor': '0.8.1', 'astunparse': '1.6.3', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'backcall': '0.2.0', 'black': '21.9b0', 'bleach': '4.1.0', 'blessed': '1.19.0', 'blessings': '1.7', 'blinker': '1.4', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2020.12.5', 'cffi': '1.14.6', 'chardet': '4.0.0', 'charset-normalizer': '2.0.4', 'click': '8.0.1', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'dataclasses': '0.8', 'decorator': '5.1.0', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'gast': '0.2.2', 'gitdb': '4.0.9', 'gitdb2': '3.0.3.post1', 'gitpython': '3.1.0', 'google-auth': '1.30.1', 'google-auth-oauthlib': '0.4.4', 'google-pasta': '0.2.0', 'gpustat': '1.0.0.dev1', 'graphviz': '0.10.1', 'grpcio': '1.38.0', 'gym': '0.12.5', 'h5py': '2.10.0', 'idna': '2.10', 'idna-ssl': '1.1.0', 'importlib-metadata': '4.8.1', 'ipykernel': '5.5.6', 'ipython': '7.16.1', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.17.2', 'jinja2': '3.0.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '7.0.6', 'jupyter-console': '6.4.0', 'jupyter-core': '4.8.1', 'jupyterlab-pygments': '0.1.2', 'keras-applications': '1.0.8', 'keras-preprocessing': '1.1.2', 'kiwisolver': '1.3.1', 'markdown': '3.3.4', 'markupsafe': '2.0.1', 'matplotlib': '3.3.4', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.10.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'mypy-extensions': '0.4.3', 'nbclient': '0.5.4', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'notebook': '6.4.4', 'numpy': '1.18.5', 'nvidia-ml-py3': '7.352.0', 'oauthlib': '3.1.0', 'olefile': '0.46', 'opt-einsum': '3.3.0', 'packaging': '21.0', 'pandas': '1.1.5', 'pandocfilters': '1.5.0', 'parse': '1.19.0', 'parso': '0.7.1', 'pathspec': '0.9.0', 'pexpect': '4.8.0', 'pickle5': '0.0.11', 'pickleshare': '0.7.5', 'pillow': '6.2.2', 'pip': '21.2.2', 'platformdirs': '2.4.0', 'pluggy': '0.13.1', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.11.0', 'prompt-toolkit': '3.0.20', 'protobuf': '3.17.1', 'psutil': '5.8.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pyglet': '1.5.21', 'pygments': '2.10.0', 'pyjwt': '2.1.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyrsistent': '0.18.0', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'python-jsonrpc-server': '0.4.0', 'python-language-server': '0.36.2', 'pytz': '2021.3', 'pyyaml': '6.0', 'pyzmq': '22.3.0', 'qgrid': '1.1.1', 'qtconsole': '5.1.1', 'qtpy': '1.11.2', 'ranger-fm': '1.9.3', 'regex': '2021.10.21', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'rsa': '4.7.2', 'scikit-learn': '0.21.2', 'scipy': '1.5.4', 'seaborn': '0.11.2', 'send2trash': '1.8.0', 'setuptools': '57.0.0', 'six': '1.16.0', 'smmap': '4.0.0', 'smmap2': '2.0.5', 'tabulate': '0.8.3', 'tensorboard': '1.15.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.0', 'tensorflow': '1.15.0', 'tensorflow-estimator': '1.15.1', 'termcolor': '1.1.0', 'terminado': '0.12.1', 'testpath': '0.5.0', 'timm': '0.4.9', 'tomli': '1.2.1', 'torch': '1.3.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchsummary': '1.5.1', 'torchvision': '0.4.2', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '4.3.3', 'traittypes': '0.2.1', 'typed-ast': '1.4.3', 'typing-extensions': '3.10.0.1', 'ujson': '4.1.0', 'urllib3': '1.26.4', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.2.1', 'werkzeug': '2.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '3.0.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2021-11-13 17:14:04,379 - Git is dirty
2021-11-13 17:14:04,380 - Active Git branch: master
2021-11-13 17:14:04,422 - Git commit: f401db9403c1608e10258733e013b6c087e3ef34
2021-11-13 17:14:04,423 - Command line: compress_classifier.py --arch inception_v3 -p 10 -j 22 /home/imagenet/ --pretrained --run --qe-config-file ./conf/inception_conf.yaml --gpus 2 --name resnet50-imagenet-grayencode --method proposed_method --mlc 8 --num_bits 8
2021-11-13 17:14:04,423 - Distiller: 0.4.0rc0
2021-11-13 17:14:04,426 - Random seed: 13259
2021-11-13 17:14:07,320 - => created a pretrained inception_v3 model with the imagenet dataset
2021-11-13 17:14:09,992 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2021-11-13 17:14:09,996 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2021-11-13 17:14:10,195 - Dataset sizes:
	test=50000
2021-11-13 17:14:10,197 - Reading configuration from: ./conf/inception_conf.yaml
2021-11-13 17:14:10,204 - Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers
2021-11-13 17:14:10,207 - Loading activation stats from: ./quant_stats/inceptionv3-imagenet.yaml
2021-11-13 17:14:11,045 - Preparing model for quantization using PostTrainLinearQuantizer
2021-11-13 17:14:17,573 - Applying batch-norm folding ahead of post-training quantization
2021-11-13 17:14:17,575 - Fusing sequence ['module.Conv2d_1a_3x3.conv', 'module.Conv2d_1a_3x3.bn']
2021-11-13 17:14:17,577 - Fusing sequence ['module.Conv2d_2a_3x3.conv', 'module.Conv2d_2a_3x3.bn']
2021-11-13 17:14:17,578 - Fusing sequence ['module.Conv2d_2b_3x3.conv', 'module.Conv2d_2b_3x3.bn']
2021-11-13 17:14:17,578 - Fusing sequence ['module.Conv2d_3b_1x1.conv', 'module.Conv2d_3b_1x1.bn']
2021-11-13 17:14:17,578 - Fusing sequence ['module.Conv2d_4a_3x3.conv', 'module.Conv2d_4a_3x3.bn']
2021-11-13 17:14:17,579 - Fusing sequence ['module.Mixed_5b.branch1x1.conv', 'module.Mixed_5b.branch1x1.bn']
2021-11-13 17:14:17,579 - Fusing sequence ['module.Mixed_5b.branch5x5_1.conv', 'module.Mixed_5b.branch5x5_1.bn']
2021-11-13 17:14:17,579 - Fusing sequence ['module.Mixed_5b.branch5x5_2.conv', 'module.Mixed_5b.branch5x5_2.bn']
2021-11-13 17:14:17,580 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_1.conv', 'module.Mixed_5b.branch3x3dbl_1.bn']
2021-11-13 17:14:17,580 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_2.conv', 'module.Mixed_5b.branch3x3dbl_2.bn']
2021-11-13 17:14:17,580 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_3.conv', 'module.Mixed_5b.branch3x3dbl_3.bn']
2021-11-13 17:14:17,581 - Fusing sequence ['module.Mixed_5b.branch_pool.conv', 'module.Mixed_5b.branch_pool.bn']
2021-11-13 17:14:17,581 - Fusing sequence ['module.Mixed_5c.branch1x1.conv', 'module.Mixed_5c.branch1x1.bn']
2021-11-13 17:14:17,581 - Fusing sequence ['module.Mixed_5c.branch5x5_1.conv', 'module.Mixed_5c.branch5x5_1.bn']
2021-11-13 17:14:17,582 - Fusing sequence ['module.Mixed_5c.branch5x5_2.conv', 'module.Mixed_5c.branch5x5_2.bn']
2021-11-13 17:14:17,582 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_1.conv', 'module.Mixed_5c.branch3x3dbl_1.bn']
2021-11-13 17:14:17,582 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_2.conv', 'module.Mixed_5c.branch3x3dbl_2.bn']
2021-11-13 17:14:17,583 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_3.conv', 'module.Mixed_5c.branch3x3dbl_3.bn']
2021-11-13 17:14:17,583 - Fusing sequence ['module.Mixed_5c.branch_pool.conv', 'module.Mixed_5c.branch_pool.bn']
2021-11-13 17:14:17,583 - Fusing sequence ['module.Mixed_5d.branch1x1.conv', 'module.Mixed_5d.branch1x1.bn']
2021-11-13 17:14:17,584 - Fusing sequence ['module.Mixed_5d.branch5x5_1.conv', 'module.Mixed_5d.branch5x5_1.bn']
2021-11-13 17:14:17,584 - Fusing sequence ['module.Mixed_5d.branch5x5_2.conv', 'module.Mixed_5d.branch5x5_2.bn']
2021-11-13 17:14:17,584 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_1.conv', 'module.Mixed_5d.branch3x3dbl_1.bn']
2021-11-13 17:14:17,585 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_2.conv', 'module.Mixed_5d.branch3x3dbl_2.bn']
2021-11-13 17:14:17,585 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_3.conv', 'module.Mixed_5d.branch3x3dbl_3.bn']
2021-11-13 17:14:17,585 - Fusing sequence ['module.Mixed_5d.branch_pool.conv', 'module.Mixed_5d.branch_pool.bn']
2021-11-13 17:14:17,585 - Fusing sequence ['module.Mixed_6a.branch3x3.conv', 'module.Mixed_6a.branch3x3.bn']
2021-11-13 17:14:17,586 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_1.conv', 'module.Mixed_6a.branch3x3dbl_1.bn']
2021-11-13 17:14:17,586 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_2.conv', 'module.Mixed_6a.branch3x3dbl_2.bn']
2021-11-13 17:14:17,586 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_3.conv', 'module.Mixed_6a.branch3x3dbl_3.bn']
2021-11-13 17:14:17,587 - Fusing sequence ['module.Mixed_6b.branch1x1.conv', 'module.Mixed_6b.branch1x1.bn']
2021-11-13 17:14:17,587 - Fusing sequence ['module.Mixed_6b.branch7x7_1.conv', 'module.Mixed_6b.branch7x7_1.bn']
2021-11-13 17:14:17,588 - Fusing sequence ['module.Mixed_6b.branch7x7_2.conv', 'module.Mixed_6b.branch7x7_2.bn']
2021-11-13 17:14:17,588 - Fusing sequence ['module.Mixed_6b.branch7x7_3.conv', 'module.Mixed_6b.branch7x7_3.bn']
2021-11-13 17:14:17,588 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_1.conv', 'module.Mixed_6b.branch7x7dbl_1.bn']
2021-11-13 17:14:17,589 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_2.conv', 'module.Mixed_6b.branch7x7dbl_2.bn']
2021-11-13 17:14:17,589 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_3.conv', 'module.Mixed_6b.branch7x7dbl_3.bn']
2021-11-13 17:14:17,589 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_4.conv', 'module.Mixed_6b.branch7x7dbl_4.bn']
2021-11-13 17:14:17,590 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_5.conv', 'module.Mixed_6b.branch7x7dbl_5.bn']
2021-11-13 17:14:17,590 - Fusing sequence ['module.Mixed_6b.branch_pool.conv', 'module.Mixed_6b.branch_pool.bn']
2021-11-13 17:14:17,590 - Fusing sequence ['module.Mixed_6c.branch1x1.conv', 'module.Mixed_6c.branch1x1.bn']
2021-11-13 17:14:17,590 - Fusing sequence ['module.Mixed_6c.branch7x7_1.conv', 'module.Mixed_6c.branch7x7_1.bn']
2021-11-13 17:14:17,591 - Fusing sequence ['module.Mixed_6c.branch7x7_2.conv', 'module.Mixed_6c.branch7x7_2.bn']
2021-11-13 17:14:17,591 - Fusing sequence ['module.Mixed_6c.branch7x7_3.conv', 'module.Mixed_6c.branch7x7_3.bn']
2021-11-13 17:14:17,591 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_1.conv', 'module.Mixed_6c.branch7x7dbl_1.bn']
2021-11-13 17:14:17,592 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_2.conv', 'module.Mixed_6c.branch7x7dbl_2.bn']
2021-11-13 17:14:17,592 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_3.conv', 'module.Mixed_6c.branch7x7dbl_3.bn']
2021-11-13 17:14:17,592 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_4.conv', 'module.Mixed_6c.branch7x7dbl_4.bn']
2021-11-13 17:14:17,593 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_5.conv', 'module.Mixed_6c.branch7x7dbl_5.bn']
2021-11-13 17:14:17,593 - Fusing sequence ['module.Mixed_6c.branch_pool.conv', 'module.Mixed_6c.branch_pool.bn']
2021-11-13 17:14:17,593 - Fusing sequence ['module.Mixed_6d.branch1x1.conv', 'module.Mixed_6d.branch1x1.bn']
2021-11-13 17:14:17,594 - Fusing sequence ['module.Mixed_6d.branch7x7_1.conv', 'module.Mixed_6d.branch7x7_1.bn']
2021-11-13 17:14:17,594 - Fusing sequence ['module.Mixed_6d.branch7x7_2.conv', 'module.Mixed_6d.branch7x7_2.bn']
2021-11-13 17:14:17,594 - Fusing sequence ['module.Mixed_6d.branch7x7_3.conv', 'module.Mixed_6d.branch7x7_3.bn']
2021-11-13 17:14:17,595 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_1.conv', 'module.Mixed_6d.branch7x7dbl_1.bn']
2021-11-13 17:14:17,595 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_2.conv', 'module.Mixed_6d.branch7x7dbl_2.bn']
2021-11-13 17:14:17,595 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_3.conv', 'module.Mixed_6d.branch7x7dbl_3.bn']
2021-11-13 17:14:17,596 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_4.conv', 'module.Mixed_6d.branch7x7dbl_4.bn']
2021-11-13 17:14:17,596 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_5.conv', 'module.Mixed_6d.branch7x7dbl_5.bn']
2021-11-13 17:14:17,596 - Fusing sequence ['module.Mixed_6d.branch_pool.conv', 'module.Mixed_6d.branch_pool.bn']
2021-11-13 17:14:17,597 - Fusing sequence ['module.Mixed_6e.branch1x1.conv', 'module.Mixed_6e.branch1x1.bn']
2021-11-13 17:14:17,597 - Fusing sequence ['module.Mixed_6e.branch7x7_1.conv', 'module.Mixed_6e.branch7x7_1.bn']
2021-11-13 17:14:17,597 - Fusing sequence ['module.Mixed_6e.branch7x7_2.conv', 'module.Mixed_6e.branch7x7_2.bn']
2021-11-13 17:14:17,598 - Fusing sequence ['module.Mixed_6e.branch7x7_3.conv', 'module.Mixed_6e.branch7x7_3.bn']
2021-11-13 17:14:17,598 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_1.conv', 'module.Mixed_6e.branch7x7dbl_1.bn']
2021-11-13 17:14:17,598 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_2.conv', 'module.Mixed_6e.branch7x7dbl_2.bn']
2021-11-13 17:14:17,599 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_3.conv', 'module.Mixed_6e.branch7x7dbl_3.bn']
2021-11-13 17:14:17,599 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_4.conv', 'module.Mixed_6e.branch7x7dbl_4.bn']
2021-11-13 17:14:17,599 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_5.conv', 'module.Mixed_6e.branch7x7dbl_5.bn']
2021-11-13 17:14:17,600 - Fusing sequence ['module.Mixed_6e.branch_pool.conv', 'module.Mixed_6e.branch_pool.bn']
2021-11-13 17:14:17,600 - Fusing sequence ['module.Mixed_7a.branch3x3_1.conv', 'module.Mixed_7a.branch3x3_1.bn']
2021-11-13 17:14:17,600 - Fusing sequence ['module.Mixed_7a.branch3x3_2.conv', 'module.Mixed_7a.branch3x3_2.bn']
2021-11-13 17:14:17,600 - Fusing sequence ['module.Mixed_7a.branch7x7x3_1.conv', 'module.Mixed_7a.branch7x7x3_1.bn']
2021-11-13 17:14:17,601 - Fusing sequence ['module.Mixed_7a.branch7x7x3_2.conv', 'module.Mixed_7a.branch7x7x3_2.bn']
2021-11-13 17:14:17,601 - Fusing sequence ['module.Mixed_7a.branch7x7x3_3.conv', 'module.Mixed_7a.branch7x7x3_3.bn']
2021-11-13 17:14:17,601 - Fusing sequence ['module.Mixed_7a.branch7x7x3_4.conv', 'module.Mixed_7a.branch7x7x3_4.bn']
2021-11-13 17:14:17,602 - Fusing sequence ['module.Mixed_7b.branch1x1.conv', 'module.Mixed_7b.branch1x1.bn']
2021-11-13 17:14:17,602 - Fusing sequence ['module.Mixed_7b.branch3x3_1.conv', 'module.Mixed_7b.branch3x3_1.bn']
2021-11-13 17:14:17,602 - Fusing sequence ['module.Mixed_7b.branch3x3_2a.conv', 'module.Mixed_7b.branch3x3_2a.bn']
2021-11-13 17:14:17,603 - Fusing sequence ['module.Mixed_7b.branch3x3_2b.conv', 'module.Mixed_7b.branch3x3_2b.bn']
2021-11-13 17:14:17,603 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_1.conv', 'module.Mixed_7b.branch3x3dbl_1.bn']
2021-11-13 17:14:17,603 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_2.conv', 'module.Mixed_7b.branch3x3dbl_2.bn']
2021-11-13 17:14:17,604 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_3a.conv', 'module.Mixed_7b.branch3x3dbl_3a.bn']
2021-11-13 17:14:17,604 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_3b.conv', 'module.Mixed_7b.branch3x3dbl_3b.bn']
2021-11-13 17:14:17,604 - Fusing sequence ['module.Mixed_7b.branch_pool.conv', 'module.Mixed_7b.branch_pool.bn']
2021-11-13 17:14:17,605 - Fusing sequence ['module.Mixed_7c.branch1x1.conv', 'module.Mixed_7c.branch1x1.bn']
2021-11-13 17:14:17,605 - Fusing sequence ['module.Mixed_7c.branch3x3_1.conv', 'module.Mixed_7c.branch3x3_1.bn']
2021-11-13 17:14:17,605 - Fusing sequence ['module.Mixed_7c.branch3x3_2a.conv', 'module.Mixed_7c.branch3x3_2a.bn']
2021-11-13 17:14:17,606 - Fusing sequence ['module.Mixed_7c.branch3x3_2b.conv', 'module.Mixed_7c.branch3x3_2b.bn']
2021-11-13 17:14:17,606 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_1.conv', 'module.Mixed_7c.branch3x3dbl_1.bn']
2021-11-13 17:14:17,606 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_2.conv', 'module.Mixed_7c.branch3x3dbl_2.bn']
2021-11-13 17:14:17,607 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_3a.conv', 'module.Mixed_7c.branch3x3dbl_3a.bn']
2021-11-13 17:14:17,607 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_3b.conv', 'module.Mixed_7c.branch3x3dbl_3b.bn']
2021-11-13 17:14:17,607 - Fusing sequence ['module.Mixed_7c.branch_pool.conv', 'module.Mixed_7c.branch_pool.bn']
2021-11-13 17:14:19,956 - Propagating output statistics from BN modules to folded modules
2021-11-13 17:14:19,957 -   Conv2d_1a_3x3.bn --> module.Conv2d_1a_3x3.conv
2021-11-13 17:14:19,957 -   Conv2d_2a_3x3.bn --> module.Conv2d_2a_3x3.conv
2021-11-13 17:14:19,957 -   Conv2d_2b_3x3.bn --> module.Conv2d_2b_3x3.conv
2021-11-13 17:14:19,957 -   Conv2d_3b_1x1.bn --> module.Conv2d_3b_1x1.conv
2021-11-13 17:14:19,957 -   Conv2d_4a_3x3.bn --> module.Conv2d_4a_3x3.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch1x1.bn --> module.Mixed_5b.branch1x1.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch5x5_1.bn --> module.Mixed_5b.branch5x5_1.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch5x5_2.bn --> module.Mixed_5b.branch5x5_2.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch3x3dbl_1.bn --> module.Mixed_5b.branch3x3dbl_1.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch3x3dbl_2.bn --> module.Mixed_5b.branch3x3dbl_2.conv
2021-11-13 17:14:19,957 -   Mixed_5b.branch3x3dbl_3.bn --> module.Mixed_5b.branch3x3dbl_3.conv
2021-11-13 17:14:19,958 -   Mixed_5b.branch_pool.bn --> module.Mixed_5b.branch_pool.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch1x1.bn --> module.Mixed_5c.branch1x1.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch5x5_1.bn --> module.Mixed_5c.branch5x5_1.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch5x5_2.bn --> module.Mixed_5c.branch5x5_2.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch3x3dbl_1.bn --> module.Mixed_5c.branch3x3dbl_1.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch3x3dbl_2.bn --> module.Mixed_5c.branch3x3dbl_2.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch3x3dbl_3.bn --> module.Mixed_5c.branch3x3dbl_3.conv
2021-11-13 17:14:19,958 -   Mixed_5c.branch_pool.bn --> module.Mixed_5c.branch_pool.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch1x1.bn --> module.Mixed_5d.branch1x1.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch5x5_1.bn --> module.Mixed_5d.branch5x5_1.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch5x5_2.bn --> module.Mixed_5d.branch5x5_2.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch3x3dbl_1.bn --> module.Mixed_5d.branch3x3dbl_1.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch3x3dbl_2.bn --> module.Mixed_5d.branch3x3dbl_2.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch3x3dbl_3.bn --> module.Mixed_5d.branch3x3dbl_3.conv
2021-11-13 17:14:19,958 -   Mixed_5d.branch_pool.bn --> module.Mixed_5d.branch_pool.conv
2021-11-13 17:14:19,958 -   Mixed_6a.branch3x3.bn --> module.Mixed_6a.branch3x3.conv
2021-11-13 17:14:19,958 -   Mixed_6a.branch3x3dbl_1.bn --> module.Mixed_6a.branch3x3dbl_1.conv
2021-11-13 17:14:19,958 -   Mixed_6a.branch3x3dbl_2.bn --> module.Mixed_6a.branch3x3dbl_2.conv
2021-11-13 17:14:19,958 -   Mixed_6a.branch3x3dbl_3.bn --> module.Mixed_6a.branch3x3dbl_3.conv
2021-11-13 17:14:19,958 -   Mixed_6b.branch1x1.bn --> module.Mixed_6b.branch1x1.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7_1.bn --> module.Mixed_6b.branch7x7_1.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7_2.bn --> module.Mixed_6b.branch7x7_2.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7_3.bn --> module.Mixed_6b.branch7x7_3.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7dbl_1.bn --> module.Mixed_6b.branch7x7dbl_1.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7dbl_2.bn --> module.Mixed_6b.branch7x7dbl_2.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7dbl_3.bn --> module.Mixed_6b.branch7x7dbl_3.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7dbl_4.bn --> module.Mixed_6b.branch7x7dbl_4.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch7x7dbl_5.bn --> module.Mixed_6b.branch7x7dbl_5.conv
2021-11-13 17:14:19,959 -   Mixed_6b.branch_pool.bn --> module.Mixed_6b.branch_pool.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch1x1.bn --> module.Mixed_6c.branch1x1.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7_1.bn --> module.Mixed_6c.branch7x7_1.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7_2.bn --> module.Mixed_6c.branch7x7_2.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7_3.bn --> module.Mixed_6c.branch7x7_3.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7dbl_1.bn --> module.Mixed_6c.branch7x7dbl_1.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7dbl_2.bn --> module.Mixed_6c.branch7x7dbl_2.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7dbl_3.bn --> module.Mixed_6c.branch7x7dbl_3.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7dbl_4.bn --> module.Mixed_6c.branch7x7dbl_4.conv
2021-11-13 17:14:19,959 -   Mixed_6c.branch7x7dbl_5.bn --> module.Mixed_6c.branch7x7dbl_5.conv
2021-11-13 17:14:19,960 -   Mixed_6c.branch_pool.bn --> module.Mixed_6c.branch_pool.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch1x1.bn --> module.Mixed_6d.branch1x1.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7_1.bn --> module.Mixed_6d.branch7x7_1.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7_2.bn --> module.Mixed_6d.branch7x7_2.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7_3.bn --> module.Mixed_6d.branch7x7_3.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7dbl_1.bn --> module.Mixed_6d.branch7x7dbl_1.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7dbl_2.bn --> module.Mixed_6d.branch7x7dbl_2.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7dbl_3.bn --> module.Mixed_6d.branch7x7dbl_3.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7dbl_4.bn --> module.Mixed_6d.branch7x7dbl_4.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch7x7dbl_5.bn --> module.Mixed_6d.branch7x7dbl_5.conv
2021-11-13 17:14:19,960 -   Mixed_6d.branch_pool.bn --> module.Mixed_6d.branch_pool.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch1x1.bn --> module.Mixed_6e.branch1x1.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch7x7_1.bn --> module.Mixed_6e.branch7x7_1.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch7x7_2.bn --> module.Mixed_6e.branch7x7_2.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch7x7_3.bn --> module.Mixed_6e.branch7x7_3.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch7x7dbl_1.bn --> module.Mixed_6e.branch7x7dbl_1.conv
2021-11-13 17:14:19,960 -   Mixed_6e.branch7x7dbl_2.bn --> module.Mixed_6e.branch7x7dbl_2.conv
2021-11-13 17:14:19,961 -   Mixed_6e.branch7x7dbl_3.bn --> module.Mixed_6e.branch7x7dbl_3.conv
2021-11-13 17:14:19,961 -   Mixed_6e.branch7x7dbl_4.bn --> module.Mixed_6e.branch7x7dbl_4.conv
2021-11-13 17:14:19,961 -   Mixed_6e.branch7x7dbl_5.bn --> module.Mixed_6e.branch7x7dbl_5.conv
2021-11-13 17:14:19,961 -   Mixed_6e.branch_pool.bn --> module.Mixed_6e.branch_pool.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch3x3_1.bn --> module.Mixed_7a.branch3x3_1.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch3x3_2.bn --> module.Mixed_7a.branch3x3_2.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch7x7x3_1.bn --> module.Mixed_7a.branch7x7x3_1.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch7x7x3_2.bn --> module.Mixed_7a.branch7x7x3_2.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch7x7x3_3.bn --> module.Mixed_7a.branch7x7x3_3.conv
2021-11-13 17:14:19,961 -   Mixed_7a.branch7x7x3_4.bn --> module.Mixed_7a.branch7x7x3_4.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch1x1.bn --> module.Mixed_7b.branch1x1.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3_1.bn --> module.Mixed_7b.branch3x3_1.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3_2a.bn --> module.Mixed_7b.branch3x3_2a.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3_2b.bn --> module.Mixed_7b.branch3x3_2b.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3dbl_1.bn --> module.Mixed_7b.branch3x3dbl_1.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3dbl_2.bn --> module.Mixed_7b.branch3x3dbl_2.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3dbl_3a.bn --> module.Mixed_7b.branch3x3dbl_3a.conv
2021-11-13 17:14:19,961 -   Mixed_7b.branch3x3dbl_3b.bn --> module.Mixed_7b.branch3x3dbl_3b.conv
2021-11-13 17:14:19,962 -   Mixed_7b.branch_pool.bn --> module.Mixed_7b.branch_pool.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch1x1.bn --> module.Mixed_7c.branch1x1.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3_1.bn --> module.Mixed_7c.branch3x3_1.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3_2a.bn --> module.Mixed_7c.branch3x3_2a.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3_2b.bn --> module.Mixed_7c.branch3x3_2b.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3dbl_1.bn --> module.Mixed_7c.branch3x3dbl_1.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3dbl_2.bn --> module.Mixed_7c.branch3x3dbl_2.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3dbl_3a.bn --> module.Mixed_7c.branch3x3dbl_3a.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch3x3dbl_3b.bn --> module.Mixed_7c.branch3x3dbl_3b.conv
2021-11-13 17:14:19,962 -   Mixed_7c.branch_pool.bn --> module.Mixed_7c.branch_pool.conv
2021-11-13 17:14:19,962 - Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid
2021-11-13 17:14:19,963 -   Module Conv2d_1a_3x3.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Conv2d_2a_3x3.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Conv2d_2b_3x3.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Conv2d_3b_1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Conv2d_4a_3x3.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch5x5_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch5x5_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5b.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5c.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5c.branch5x5_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5c.branch5x5_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,963 -   Module Mixed_5c.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5c.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5c.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5c.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch5x5_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch5x5_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_5d.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6a.branch3x3.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6a.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6a.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6a.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,964 -   Module Mixed_6b.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6b.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6b.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6b.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6c.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,965 -   Module Mixed_6d.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6d.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6d.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_6e.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch3x3_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch3x3_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch7x7x3_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch7x7x3_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch7x7x3_3.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7a.branch7x7x3_4.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7b.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7b.branch3x3_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7b.branch3x3_2a.conv followed by Relu, updating stats
2021-11-13 17:14:19,966 -   Module Mixed_7b.branch3x3_2b.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7b.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7b.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7b.branch3x3dbl_3a.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7b.branch3x3dbl_3b.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7b.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch1x1.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3_2a.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3_2b.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3dbl_3a.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch3x3dbl_3b.conv followed by Relu, updating stats
2021-11-13 17:14:19,967 -   Module Mixed_7c.branch_pool.conv followed by Relu, updating stats
2021-11-13 17:14:20,101 - Updated stats saved to logs/resnet50-imagenet-grayencode___2021.11.13-171404/quant_stats_after_prepare_model.yaml
2021-11-13 17:14:20,101 - Module module
2021-11-13 17:14:20,101 - 	Skipping
2021-11-13 17:14:20,102 - Module module.Conv2d_1a_3x3
2021-11-13 17:14:20,102 - 	Skipping
2021-11-13 17:14:20,106 - Module module.Conv2d_1a_3x3.conv
2021-11-13 17:14:20,106 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,106 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,108 - Module module.Conv2d_2a_3x3
2021-11-13 17:14:20,108 - 	Skipping
2021-11-13 17:14:20,109 - Module module.Conv2d_2a_3x3.conv
2021-11-13 17:14:20,109 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,109 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,111 - Module module.Conv2d_2b_3x3
2021-11-13 17:14:20,111 - 	Skipping
2021-11-13 17:14:20,113 - Module module.Conv2d_2b_3x3.conv
2021-11-13 17:14:20,113 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,113 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,115 - Module module.Conv2d_3b_1x1
2021-11-13 17:14:20,115 - 	Skipping
2021-11-13 17:14:20,116 - Module module.Conv2d_3b_1x1.conv
2021-11-13 17:14:20,116 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,116 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,118 - Module module.Conv2d_4a_3x3
2021-11-13 17:14:20,118 - 	Skipping
2021-11-13 17:14:20,119 - Module module.Conv2d_4a_3x3.conv
2021-11-13 17:14:20,119 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,119 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,121 - Module module.Mixed_5b
2021-11-13 17:14:20,121 - 	Skipping
2021-11-13 17:14:20,121 - Module module.Mixed_5b.branch1x1
2021-11-13 17:14:20,121 - 	Skipping
2021-11-13 17:14:20,122 - Module module.Mixed_5b.branch1x1.conv
2021-11-13 17:14:20,122 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,122 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,124 - Module module.Mixed_5b.branch5x5_1
2021-11-13 17:14:20,124 - 	Skipping
2021-11-13 17:14:20,126 - Module module.Mixed_5b.branch5x5_1.conv
2021-11-13 17:14:20,126 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,126 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,128 - Module module.Mixed_5b.branch5x5_2
2021-11-13 17:14:20,128 - 	Skipping
2021-11-13 17:14:20,129 - Module module.Mixed_5b.branch5x5_2.conv
2021-11-13 17:14:20,129 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,129 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,131 - Module module.Mixed_5b.branch3x3dbl_1
2021-11-13 17:14:20,131 - 	Skipping
2021-11-13 17:14:20,132 - Module module.Mixed_5b.branch3x3dbl_1.conv
2021-11-13 17:14:20,132 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,132 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,134 - Module module.Mixed_5b.branch3x3dbl_2
2021-11-13 17:14:20,134 - 	Skipping
2021-11-13 17:14:20,135 - Module module.Mixed_5b.branch3x3dbl_2.conv
2021-11-13 17:14:20,136 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,136 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,138 - Module module.Mixed_5b.branch3x3dbl_3
2021-11-13 17:14:20,138 - 	Skipping
2021-11-13 17:14:20,139 - Module module.Mixed_5b.branch3x3dbl_3.conv
2021-11-13 17:14:20,139 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,139 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,141 - Module module.Mixed_5b.branch_pool
2021-11-13 17:14:20,141 - 	Skipping
2021-11-13 17:14:20,142 - Module module.Mixed_5b.branch_pool.conv
2021-11-13 17:14:20,142 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,142 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,144 - Module module.Mixed_5c
2021-11-13 17:14:20,144 - 	Skipping
2021-11-13 17:14:20,144 - Module module.Mixed_5c.branch1x1
2021-11-13 17:14:20,144 - 	Skipping
2021-11-13 17:14:20,146 - Module module.Mixed_5c.branch1x1.conv
2021-11-13 17:14:20,146 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,146 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,148 - Module module.Mixed_5c.branch5x5_1
2021-11-13 17:14:20,148 - 	Skipping
2021-11-13 17:14:20,149 - Module module.Mixed_5c.branch5x5_1.conv
2021-11-13 17:14:20,149 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,149 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,151 - Module module.Mixed_5c.branch5x5_2
2021-11-13 17:14:20,151 - 	Skipping
2021-11-13 17:14:20,152 - Module module.Mixed_5c.branch5x5_2.conv
2021-11-13 17:14:20,152 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,152 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,154 - Module module.Mixed_5c.branch3x3dbl_1
2021-11-13 17:14:20,154 - 	Skipping
2021-11-13 17:14:20,156 - Module module.Mixed_5c.branch3x3dbl_1.conv
2021-11-13 17:14:20,156 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,156 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,158 - Module module.Mixed_5c.branch3x3dbl_2
2021-11-13 17:14:20,158 - 	Skipping
2021-11-13 17:14:20,159 - Module module.Mixed_5c.branch3x3dbl_2.conv
2021-11-13 17:14:20,159 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,159 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,161 - Module module.Mixed_5c.branch3x3dbl_3
2021-11-13 17:14:20,161 - 	Skipping
2021-11-13 17:14:20,162 - Module module.Mixed_5c.branch3x3dbl_3.conv
2021-11-13 17:14:20,162 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,162 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,164 - Module module.Mixed_5c.branch_pool
2021-11-13 17:14:20,164 - 	Skipping
2021-11-13 17:14:20,166 - Module module.Mixed_5c.branch_pool.conv
2021-11-13 17:14:20,166 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,166 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,168 - Module module.Mixed_5d
2021-11-13 17:14:20,168 - 	Skipping
2021-11-13 17:14:20,172 - Module module.Mixed_5d.branch1x1
2021-11-13 17:14:20,172 - 	Skipping
2021-11-13 17:14:20,174 - Module module.Mixed_5d.branch1x1.conv
2021-11-13 17:14:20,174 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,174 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,176 - Module module.Mixed_5d.branch5x5_1
2021-11-13 17:14:20,176 - 	Skipping
2021-11-13 17:14:20,177 - Module module.Mixed_5d.branch5x5_1.conv
2021-11-13 17:14:20,177 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,177 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,179 - Module module.Mixed_5d.branch5x5_2
2021-11-13 17:14:20,179 - 	Skipping
2021-11-13 17:14:20,180 - Module module.Mixed_5d.branch5x5_2.conv
2021-11-13 17:14:20,180 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,180 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,182 - Module module.Mixed_5d.branch3x3dbl_1
2021-11-13 17:14:20,183 - 	Skipping
2021-11-13 17:14:20,184 - Module module.Mixed_5d.branch3x3dbl_1.conv
2021-11-13 17:14:20,184 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,184 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,186 - Module module.Mixed_5d.branch3x3dbl_2
2021-11-13 17:14:20,186 - 	Skipping
2021-11-13 17:14:20,187 - Module module.Mixed_5d.branch3x3dbl_2.conv
2021-11-13 17:14:20,187 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,187 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,190 - Module module.Mixed_5d.branch3x3dbl_3
2021-11-13 17:14:20,190 - 	Skipping
2021-11-13 17:14:20,191 - Module module.Mixed_5d.branch3x3dbl_3.conv
2021-11-13 17:14:20,191 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,191 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,194 - Module module.Mixed_5d.branch_pool
2021-11-13 17:14:20,194 - 	Skipping
2021-11-13 17:14:20,195 - Module module.Mixed_5d.branch_pool.conv
2021-11-13 17:14:20,195 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,195 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,197 - Module module.Mixed_6a
2021-11-13 17:14:20,197 - 	Skipping
2021-11-13 17:14:20,198 - Module module.Mixed_6a.branch3x3
2021-11-13 17:14:20,198 - 	Skipping
2021-11-13 17:14:20,199 - Module module.Mixed_6a.branch3x3.conv
2021-11-13 17:14:20,199 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,199 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,201 - Module module.Mixed_6a.branch3x3dbl_1
2021-11-13 17:14:20,201 - 	Skipping
2021-11-13 17:14:20,202 - Module module.Mixed_6a.branch3x3dbl_1.conv
2021-11-13 17:14:20,202 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,202 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,205 - Module module.Mixed_6a.branch3x3dbl_2
2021-11-13 17:14:20,205 - 	Skipping
2021-11-13 17:14:20,206 - Module module.Mixed_6a.branch3x3dbl_2.conv
2021-11-13 17:14:20,206 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,206 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,208 - Module module.Mixed_6a.branch3x3dbl_3
2021-11-13 17:14:20,208 - 	Skipping
2021-11-13 17:14:20,209 - Module module.Mixed_6a.branch3x3dbl_3.conv
2021-11-13 17:14:20,210 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,210 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,212 - Module module.Mixed_6b
2021-11-13 17:14:20,212 - 	Skipping
2021-11-13 17:14:20,212 - Module module.Mixed_6b.branch1x1
2021-11-13 17:14:20,212 - 	Skipping
2021-11-13 17:14:20,213 - Module module.Mixed_6b.branch1x1.conv
2021-11-13 17:14:20,213 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,213 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,216 - Module module.Mixed_6b.branch7x7_1
2021-11-13 17:14:20,216 - 	Skipping
2021-11-13 17:14:20,217 - Module module.Mixed_6b.branch7x7_1.conv
2021-11-13 17:14:20,217 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,217 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,219 - Module module.Mixed_6b.branch7x7_2
2021-11-13 17:14:20,219 - 	Skipping
2021-11-13 17:14:20,220 - Module module.Mixed_6b.branch7x7_2.conv
2021-11-13 17:14:20,220 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,220 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,223 - Module module.Mixed_6b.branch7x7_3
2021-11-13 17:14:20,223 - 	Skipping
2021-11-13 17:14:20,224 - Module module.Mixed_6b.branch7x7_3.conv
2021-11-13 17:14:20,224 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,224 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,227 - Module module.Mixed_6b.branch7x7dbl_1
2021-11-13 17:14:20,227 - 	Skipping
2021-11-13 17:14:20,228 - Module module.Mixed_6b.branch7x7dbl_1.conv
2021-11-13 17:14:20,228 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,228 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,230 - Module module.Mixed_6b.branch7x7dbl_2
2021-11-13 17:14:20,230 - 	Skipping
2021-11-13 17:14:20,231 - Module module.Mixed_6b.branch7x7dbl_2.conv
2021-11-13 17:14:20,231 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,232 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,234 - Module module.Mixed_6b.branch7x7dbl_3
2021-11-13 17:14:20,234 - 	Skipping
2021-11-13 17:14:20,235 - Module module.Mixed_6b.branch7x7dbl_3.conv
2021-11-13 17:14:20,235 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,235 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,238 - Module module.Mixed_6b.branch7x7dbl_4
2021-11-13 17:14:20,238 - 	Skipping
2021-11-13 17:14:20,239 - Module module.Mixed_6b.branch7x7dbl_4.conv
2021-11-13 17:14:20,239 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,239 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,241 - Module module.Mixed_6b.branch7x7dbl_5
2021-11-13 17:14:20,241 - 	Skipping
2021-11-13 17:14:20,242 - Module module.Mixed_6b.branch7x7dbl_5.conv
2021-11-13 17:14:20,242 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,242 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,245 - Module module.Mixed_6b.branch_pool
2021-11-13 17:14:20,245 - 	Skipping
2021-11-13 17:14:20,246 - Module module.Mixed_6b.branch_pool.conv
2021-11-13 17:14:20,246 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,246 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,249 - Module module.Mixed_6c
2021-11-13 17:14:20,249 - 	Skipping
2021-11-13 17:14:20,249 - Module module.Mixed_6c.branch1x1
2021-11-13 17:14:20,249 - 	Skipping
2021-11-13 17:14:20,250 - Module module.Mixed_6c.branch1x1.conv
2021-11-13 17:14:20,250 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,250 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,252 - Module module.Mixed_6c.branch7x7_1
2021-11-13 17:14:20,252 - 	Skipping
2021-11-13 17:14:20,254 - Module module.Mixed_6c.branch7x7_1.conv
2021-11-13 17:14:20,254 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,254 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,256 - Module module.Mixed_6c.branch7x7_2
2021-11-13 17:14:20,256 - 	Skipping
2021-11-13 17:14:20,257 - Module module.Mixed_6c.branch7x7_2.conv
2021-11-13 17:14:20,257 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,257 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,260 - Module module.Mixed_6c.branch7x7_3
2021-11-13 17:14:20,260 - 	Skipping
2021-11-13 17:14:20,261 - Module module.Mixed_6c.branch7x7_3.conv
2021-11-13 17:14:20,261 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,261 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,264 - Module module.Mixed_6c.branch7x7dbl_1
2021-11-13 17:14:20,264 - 	Skipping
2021-11-13 17:14:20,265 - Module module.Mixed_6c.branch7x7dbl_1.conv
2021-11-13 17:14:20,265 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,265 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,268 - Module module.Mixed_6c.branch7x7dbl_2
2021-11-13 17:14:20,268 - 	Skipping
2021-11-13 17:14:20,269 - Module module.Mixed_6c.branch7x7dbl_2.conv
2021-11-13 17:14:20,269 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,269 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,271 - Module module.Mixed_6c.branch7x7dbl_3
2021-11-13 17:14:20,271 - 	Skipping
2021-11-13 17:14:20,273 - Module module.Mixed_6c.branch7x7dbl_3.conv
2021-11-13 17:14:20,273 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,273 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,275 - Module module.Mixed_6c.branch7x7dbl_4
2021-11-13 17:14:20,275 - 	Skipping
2021-11-13 17:14:20,276 - Module module.Mixed_6c.branch7x7dbl_4.conv
2021-11-13 17:14:20,276 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,276 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,279 - Module module.Mixed_6c.branch7x7dbl_5
2021-11-13 17:14:20,279 - 	Skipping
2021-11-13 17:14:20,280 - Module module.Mixed_6c.branch7x7dbl_5.conv
2021-11-13 17:14:20,280 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,280 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,283 - Module module.Mixed_6c.branch_pool
2021-11-13 17:14:20,283 - 	Skipping
2021-11-13 17:14:20,284 - Module module.Mixed_6c.branch_pool.conv
2021-11-13 17:14:20,284 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,284 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,287 - Module module.Mixed_6d
2021-11-13 17:14:20,287 - 	Skipping
2021-11-13 17:14:20,287 - Module module.Mixed_6d.branch1x1
2021-11-13 17:14:20,287 - 	Skipping
2021-11-13 17:14:20,288 - Module module.Mixed_6d.branch1x1.conv
2021-11-13 17:14:20,288 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,288 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,291 - Module module.Mixed_6d.branch7x7_1
2021-11-13 17:14:20,291 - 	Skipping
2021-11-13 17:14:20,292 - Module module.Mixed_6d.branch7x7_1.conv
2021-11-13 17:14:20,292 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,292 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,295 - Module module.Mixed_6d.branch7x7_2
2021-11-13 17:14:20,295 - 	Skipping
2021-11-13 17:14:20,296 - Module module.Mixed_6d.branch7x7_2.conv
2021-11-13 17:14:20,296 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,296 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,299 - Module module.Mixed_6d.branch7x7_3
2021-11-13 17:14:20,299 - 	Skipping
2021-11-13 17:14:20,300 - Module module.Mixed_6d.branch7x7_3.conv
2021-11-13 17:14:20,300 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,300 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,303 - Module module.Mixed_6d.branch7x7dbl_1
2021-11-13 17:14:20,303 - 	Skipping
2021-11-13 17:14:20,304 - Module module.Mixed_6d.branch7x7dbl_1.conv
2021-11-13 17:14:20,304 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,304 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,307 - Module module.Mixed_6d.branch7x7dbl_2
2021-11-13 17:14:20,307 - 	Skipping
2021-11-13 17:14:20,308 - Module module.Mixed_6d.branch7x7dbl_2.conv
2021-11-13 17:14:20,308 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,308 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,311 - Module module.Mixed_6d.branch7x7dbl_3
2021-11-13 17:14:20,311 - 	Skipping
2021-11-13 17:14:20,312 - Module module.Mixed_6d.branch7x7dbl_3.conv
2021-11-13 17:14:20,312 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,312 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,315 - Module module.Mixed_6d.branch7x7dbl_4
2021-11-13 17:14:20,315 - 	Skipping
2021-11-13 17:14:20,316 - Module module.Mixed_6d.branch7x7dbl_4.conv
2021-11-13 17:14:20,316 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,316 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,319 - Module module.Mixed_6d.branch7x7dbl_5
2021-11-13 17:14:20,319 - 	Skipping
2021-11-13 17:14:20,320 - Module module.Mixed_6d.branch7x7dbl_5.conv
2021-11-13 17:14:20,320 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,320 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,322 - Module module.Mixed_6d.branch_pool
2021-11-13 17:14:20,323 - 	Skipping
2021-11-13 17:14:20,324 - Module module.Mixed_6d.branch_pool.conv
2021-11-13 17:14:20,324 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,324 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,326 - Module module.Mixed_6e
2021-11-13 17:14:20,326 - 	Skipping
2021-11-13 17:14:20,327 - Module module.Mixed_6e.branch1x1
2021-11-13 17:14:20,327 - 	Skipping
2021-11-13 17:14:20,328 - Module module.Mixed_6e.branch1x1.conv
2021-11-13 17:14:20,328 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,328 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,331 - Module module.Mixed_6e.branch7x7_1
2021-11-13 17:14:20,331 - 	Skipping
2021-11-13 17:14:20,332 - Module module.Mixed_6e.branch7x7_1.conv
2021-11-13 17:14:20,332 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,332 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,335 - Module module.Mixed_6e.branch7x7_2
2021-11-13 17:14:20,335 - 	Skipping
2021-11-13 17:14:20,336 - Module module.Mixed_6e.branch7x7_2.conv
2021-11-13 17:14:20,336 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,336 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,339 - Module module.Mixed_6e.branch7x7_3
2021-11-13 17:14:20,339 - 	Skipping
2021-11-13 17:14:20,340 - Module module.Mixed_6e.branch7x7_3.conv
2021-11-13 17:14:20,340 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,340 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,343 - Module module.Mixed_6e.branch7x7dbl_1
2021-11-13 17:14:20,343 - 	Skipping
2021-11-13 17:14:20,344 - Module module.Mixed_6e.branch7x7dbl_1.conv
2021-11-13 17:14:20,344 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,344 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,347 - Module module.Mixed_6e.branch7x7dbl_2
2021-11-13 17:14:20,347 - 	Skipping
2021-11-13 17:14:20,348 - Module module.Mixed_6e.branch7x7dbl_2.conv
2021-11-13 17:14:20,348 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,348 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,351 - Module module.Mixed_6e.branch7x7dbl_3
2021-11-13 17:14:20,351 - 	Skipping
2021-11-13 17:14:20,352 - Module module.Mixed_6e.branch7x7dbl_3.conv
2021-11-13 17:14:20,352 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,352 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,355 - Module module.Mixed_6e.branch7x7dbl_4
2021-11-13 17:14:20,355 - 	Skipping
2021-11-13 17:14:20,356 - Module module.Mixed_6e.branch7x7dbl_4.conv
2021-11-13 17:14:20,356 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,356 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,359 - Module module.Mixed_6e.branch7x7dbl_5
2021-11-13 17:14:20,359 - 	Skipping
2021-11-13 17:14:20,360 - Module module.Mixed_6e.branch7x7dbl_5.conv
2021-11-13 17:14:20,360 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,360 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,363 - Module module.Mixed_6e.branch_pool
2021-11-13 17:14:20,363 - 	Skipping
2021-11-13 17:14:20,364 - Module module.Mixed_6e.branch_pool.conv
2021-11-13 17:14:20,364 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,365 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,367 - Module module.AuxLogits
2021-11-13 17:14:20,367 - 	Skipping
2021-11-13 17:14:20,367 - Module module.AuxLogits.conv0
2021-11-13 17:14:20,368 - 	Skipping
2021-11-13 17:14:20,368 - Module module.AuxLogits.conv0.conv
2021-11-13 17:14:20,368 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,368 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,371 - Module module.AuxLogits.conv0.bn
2021-11-13 17:14:20,371 - 	Replacing: torch.nn.modules.batchnorm.BatchNorm2d
2021-11-13 17:14:20,371 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-13 17:14:20,374 - Module module.AuxLogits.conv1
2021-11-13 17:14:20,374 - 	Skipping
2021-11-13 17:14:20,375 - Module module.AuxLogits.conv1.conv
2021-11-13 17:14:20,376 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,376 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,378 - Module module.AuxLogits.conv1.bn
2021-11-13 17:14:20,378 - 	Replacing: torch.nn.modules.batchnorm.BatchNorm2d
2021-11-13 17:14:20,378 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-13 17:14:20,445 - Module module.AuxLogits.fc
2021-11-13 17:14:20,445 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-13 17:14:20,445 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,452 - Module module.Mixed_7a
2021-11-13 17:14:20,452 - 	Skipping
2021-11-13 17:14:20,453 - Module module.Mixed_7a.branch3x3_1
2021-11-13 17:14:20,453 - 	Skipping
2021-11-13 17:14:20,456 - Module module.Mixed_7a.branch3x3_1.conv
2021-11-13 17:14:20,456 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,456 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,463 - Module module.Mixed_7a.branch3x3_2
2021-11-13 17:14:20,463 - 	Skipping
2021-11-13 17:14:20,465 - Module module.Mixed_7a.branch3x3_2.conv
2021-11-13 17:14:20,465 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,465 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,470 - Module module.Mixed_7a.branch7x7x3_1
2021-11-13 17:14:20,470 - 	Skipping
2021-11-13 17:14:20,471 - Module module.Mixed_7a.branch7x7x3_1.conv
2021-11-13 17:14:20,471 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,471 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,475 - Module module.Mixed_7a.branch7x7x3_2
2021-11-13 17:14:20,475 - 	Skipping
2021-11-13 17:14:20,476 - Module module.Mixed_7a.branch7x7x3_2.conv
2021-11-13 17:14:20,476 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,476 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,480 - Module module.Mixed_7a.branch7x7x3_3
2021-11-13 17:14:20,480 - 	Skipping
2021-11-13 17:14:20,481 - Module module.Mixed_7a.branch7x7x3_3.conv
2021-11-13 17:14:20,481 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,481 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,484 - Module module.Mixed_7a.branch7x7x3_4
2021-11-13 17:14:20,484 - 	Skipping
2021-11-13 17:14:20,485 - Module module.Mixed_7a.branch7x7x3_4.conv
2021-11-13 17:14:20,485 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,485 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,488 - Module module.Mixed_7b
2021-11-13 17:14:20,488 - 	Skipping
2021-11-13 17:14:20,489 - Module module.Mixed_7b.branch1x1
2021-11-13 17:14:20,489 - 	Skipping
2021-11-13 17:14:20,490 - Module module.Mixed_7b.branch1x1.conv
2021-11-13 17:14:20,490 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,490 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,493 - Module module.Mixed_7b.branch3x3_1
2021-11-13 17:14:20,493 - 	Skipping
2021-11-13 17:14:20,494 - Module module.Mixed_7b.branch3x3_1.conv
2021-11-13 17:14:20,494 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,494 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,497 - Module module.Mixed_7b.branch3x3_2a
2021-11-13 17:14:20,497 - 	Skipping
2021-11-13 17:14:20,498 - Module module.Mixed_7b.branch3x3_2a.conv
2021-11-13 17:14:20,498 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,498 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,502 - Module module.Mixed_7b.branch3x3_2b
2021-11-13 17:14:20,502 - 	Skipping
2021-11-13 17:14:20,503 - Module module.Mixed_7b.branch3x3_2b.conv
2021-11-13 17:14:20,503 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,503 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,506 - Module module.Mixed_7b.branch3x3dbl_1
2021-11-13 17:14:20,506 - 	Skipping
2021-11-13 17:14:20,507 - Module module.Mixed_7b.branch3x3dbl_1.conv
2021-11-13 17:14:20,507 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,507 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,511 - Module module.Mixed_7b.branch3x3dbl_2
2021-11-13 17:14:20,511 - 	Skipping
2021-11-13 17:14:20,512 - Module module.Mixed_7b.branch3x3dbl_2.conv
2021-11-13 17:14:20,512 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,512 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,515 - Module module.Mixed_7b.branch3x3dbl_3a
2021-11-13 17:14:20,515 - 	Skipping
2021-11-13 17:14:20,516 - Module module.Mixed_7b.branch3x3dbl_3a.conv
2021-11-13 17:14:20,516 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,516 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,520 - Module module.Mixed_7b.branch3x3dbl_3b
2021-11-13 17:14:20,520 - 	Skipping
2021-11-13 17:14:20,521 - Module module.Mixed_7b.branch3x3dbl_3b.conv
2021-11-13 17:14:20,521 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,521 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,524 - Module module.Mixed_7b.branch_pool
2021-11-13 17:14:20,524 - 	Skipping
2021-11-13 17:14:20,525 - Module module.Mixed_7b.branch_pool.conv
2021-11-13 17:14:20,525 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,525 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,529 - Module module.Mixed_7c
2021-11-13 17:14:20,529 - 	Skipping
2021-11-13 17:14:20,529 - Module module.Mixed_7c.branch1x1
2021-11-13 17:14:20,529 - 	Skipping
2021-11-13 17:14:20,530 - Module module.Mixed_7c.branch1x1.conv
2021-11-13 17:14:20,530 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,530 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,533 - Module module.Mixed_7c.branch3x3_1
2021-11-13 17:14:20,533 - 	Skipping
2021-11-13 17:14:20,534 - Module module.Mixed_7c.branch3x3_1.conv
2021-11-13 17:14:20,535 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,535 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,538 - Module module.Mixed_7c.branch3x3_2a
2021-11-13 17:14:20,538 - 	Skipping
2021-11-13 17:14:20,539 - Module module.Mixed_7c.branch3x3_2a.conv
2021-11-13 17:14:20,539 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,539 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,542 - Module module.Mixed_7c.branch3x3_2b
2021-11-13 17:14:20,542 - 	Skipping
2021-11-13 17:14:20,543 - Module module.Mixed_7c.branch3x3_2b.conv
2021-11-13 17:14:20,544 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,544 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,547 - Module module.Mixed_7c.branch3x3dbl_1
2021-11-13 17:14:20,548 - 	Skipping
2021-11-13 17:14:20,549 - Module module.Mixed_7c.branch3x3dbl_1.conv
2021-11-13 17:14:20,549 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,549 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,552 - Module module.Mixed_7c.branch3x3dbl_2
2021-11-13 17:14:20,552 - 	Skipping
2021-11-13 17:14:20,553 - Module module.Mixed_7c.branch3x3dbl_2.conv
2021-11-13 17:14:20,553 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,553 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,557 - Module module.Mixed_7c.branch3x3dbl_3a
2021-11-13 17:14:20,557 - 	Skipping
2021-11-13 17:14:20,558 - Module module.Mixed_7c.branch3x3dbl_3a.conv
2021-11-13 17:14:20,558 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,558 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,561 - Module module.Mixed_7c.branch3x3dbl_3b
2021-11-13 17:14:20,561 - 	Skipping
2021-11-13 17:14:20,562 - Module module.Mixed_7c.branch3x3dbl_3b.conv
2021-11-13 17:14:20,562 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,563 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,566 - Module module.Mixed_7c.branch_pool
2021-11-13 17:14:20,566 - 	Skipping
2021-11-13 17:14:20,567 - Module module.Mixed_7c.branch_pool.conv
2021-11-13 17:14:20,567 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-13 17:14:20,567 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,572 - Module module.fc
2021-11-13 17:14:20,572 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-13 17:14:20,572 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_1a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_1a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_2a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_2a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_2b_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_2b_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,577 - Parameter 'module.Conv2d_3b_1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Conv2d_3b_1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Conv2d_4a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Conv2d_4a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,578 - Parameter 'module.Mixed_5c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,579 - Parameter 'module.Mixed_5d.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_5d.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6a.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6b.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,580 - Parameter 'module.Mixed_6b.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,581 - Parameter 'module.Mixed_6c.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6d.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6d.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6d.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,582 - Parameter 'module.Mixed_6d.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6d.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6e.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6e.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,583 - Parameter 'module.Mixed_6e.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.Mixed_6e.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,584 - Parameter 'module.AuxLogits.conv0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,584 - Parameter 'module.AuxLogits.conv1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.AuxLogits.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.AuxLogits.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch3x3_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch3x3_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7a.branch7x7x3_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7b.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,585 - Parameter 'module.Mixed_7b.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3_2a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3_2a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3_2b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3_2b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_3a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_3a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_3b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch3x3dbl_3b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch3x3_2a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,586 - Parameter 'module.Mixed_7c.branch3x3_2a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3_2b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3_2b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_3a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_3a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_3b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch3x3dbl_3b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.Mixed_7c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,587 - Parameter 'module.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-13 17:14:20,587 - Parameter 'module.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-13 17:14:20,613 - Quantized model:

DataParallel(
  (module): Inception3(
    (Conv2d_1a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=25.116467, output_zero_point=0.000000
        weights_scale=57.839279, weights_zero_point=-118.000000
        (wrapped_module): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
      )
      (bn): Identity()
    )
    (Conv2d_2a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=23.814812, output_zero_point=0.000000
        weights_scale=91.453255, weights_zero_point=-76.000000
        (wrapped_module): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_2b_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=22.681568, output_zero_point=0.000000
        weights_scale=121.423393, weights_zero_point=-100.000000
        (wrapped_module): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_3b_1x1): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=26.841505, output_zero_point=0.000000
        weights_scale=113.792465, weights_zero_point=-106.000000
        (wrapped_module): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_4a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=26.944969, output_zero_point=0.000000
        weights_scale=173.668213, weights_zero_point=-127.000000
        (wrapped_module): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Mixed_5b): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=28.497339, output_zero_point=0.000000
          weights_scale=233.696152, weights_zero_point=-130.000000
          (wrapped_module): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.181660, output_zero_point=0.000000
          weights_scale=250.440506, weights_zero_point=-129.000000
          (wrapped_module): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=29.600254, output_zero_point=0.000000
          weights_scale=210.722931, weights_zero_point=-114.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.301685, output_zero_point=0.000000
          weights_scale=183.401199, weights_zero_point=-89.000000
          (wrapped_module): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.820778, output_zero_point=0.000000
          weights_scale=173.440628, weights_zero_point=-121.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=27.333513, output_zero_point=0.000000
          weights_scale=235.430481, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=69.726189, output_zero_point=0.000000
          weights_scale=115.613274, weights_zero_point=-164.000000
          (wrapped_module): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_5c): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=28.708134, output_zero_point=0.000000
          weights_scale=185.854675, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.812336, output_zero_point=0.000000
          weights_scale=221.937332, weights_zero_point=-132.000000
          (wrapped_module): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.252514, output_zero_point=0.000000
          weights_scale=268.838531, weights_zero_point=-119.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.901993, output_zero_point=0.000000
          weights_scale=200.180267, weights_zero_point=-118.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.122124, output_zero_point=0.000000
          weights_scale=219.704071, weights_zero_point=-98.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=20.619204, output_zero_point=0.000000
          weights_scale=159.367966, weights_zero_point=-66.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.787731, output_zero_point=0.000000
          weights_scale=112.256493, weights_zero_point=-121.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_5d): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.940670, output_zero_point=0.000000
          weights_scale=160.903946, weights_zero_point=-173.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.751881, output_zero_point=0.000000
          weights_scale=221.842331, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.844185, output_zero_point=0.000000
          weights_scale=384.225189, weights_zero_point=-114.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=29.225071, output_zero_point=0.000000
          weights_scale=152.031830, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.250402, output_zero_point=0.000000
          weights_scale=214.610001, weights_zero_point=-126.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.677155, output_zero_point=0.000000
          weights_scale=283.270233, weights_zero_point=-85.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.311428, output_zero_point=0.000000
          weights_scale=112.111809, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6a): InceptionB(
      (branch3x3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.405270, output_zero_point=0.000000
          weights_scale=345.323090, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.441277, output_zero_point=0.000000
          weights_scale=194.035721, weights_zero_point=-126.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.876575, output_zero_point=0.000000
          weights_scale=252.662827, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.106899, output_zero_point=0.000000
          weights_scale=300.238403, weights_zero_point=-86.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
    )
    (Mixed_6b): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.518055, output_zero_point=0.000000
          weights_scale=212.494919, weights_zero_point=-109.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.214058, output_zero_point=0.000000
          weights_scale=224.252426, weights_zero_point=-80.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.101196, output_zero_point=0.000000
          weights_scale=182.452042, weights_zero_point=-80.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.293385, output_zero_point=0.000000
          weights_scale=196.628983, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=42.428837, output_zero_point=0.000000
          weights_scale=241.909012, weights_zero_point=-145.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=44.480652, output_zero_point=0.000000
          weights_scale=272.477875, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=45.641865, output_zero_point=0.000000
          weights_scale=224.714310, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.535149, output_zero_point=0.000000
          weights_scale=229.181229, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.826805, output_zero_point=0.000000
          weights_scale=299.566376, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=63.736088, output_zero_point=0.000000
          weights_scale=101.813553, weights_zero_point=-147.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6c): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.772423, output_zero_point=0.000000
          weights_scale=205.792435, weights_zero_point=-115.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.632809, output_zero_point=0.000000
          weights_scale=146.691681, weights_zero_point=-141.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.447899, output_zero_point=0.000000
          weights_scale=234.311081, weights_zero_point=-110.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=44.332878, output_zero_point=0.000000
          weights_scale=200.441696, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.053520, output_zero_point=0.000000
          weights_scale=169.996704, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.551636, output_zero_point=0.000000
          weights_scale=292.734314, weights_zero_point=-103.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.712090, output_zero_point=0.000000
          weights_scale=218.001434, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.522635, output_zero_point=0.000000
          weights_scale=249.194092, weights_zero_point=-96.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.347155, output_zero_point=0.000000
          weights_scale=242.304962, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=62.187904, output_zero_point=0.000000
          weights_scale=137.780121, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6d): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.670513, output_zero_point=0.000000
          weights_scale=130.717911, weights_zero_point=-154.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.600883, output_zero_point=0.000000
          weights_scale=230.687149, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.148865, output_zero_point=0.000000
          weights_scale=156.647110, weights_zero_point=-78.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.157887, output_zero_point=0.000000
          weights_scale=228.079788, weights_zero_point=-103.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.197197, output_zero_point=0.000000
          weights_scale=168.429398, weights_zero_point=-112.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.426975, output_zero_point=0.000000
          weights_scale=171.427673, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.088593, output_zero_point=0.000000
          weights_scale=235.805008, weights_zero_point=-107.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.603867, output_zero_point=0.000000
          weights_scale=155.897659, weights_zero_point=-149.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.205288, output_zero_point=0.000000
          weights_scale=164.953217, weights_zero_point=-157.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=65.319580, output_zero_point=0.000000
          weights_scale=81.718330, weights_zero_point=-146.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6e): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=50.753494, output_zero_point=0.000000
          weights_scale=134.851929, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.828094, output_zero_point=0.000000
          weights_scale=161.791412, weights_zero_point=-95.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.372322, output_zero_point=0.000000
          weights_scale=169.476089, weights_zero_point=-71.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=48.569138, output_zero_point=0.000000
          weights_scale=391.694305, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.996212, output_zero_point=0.000000
          weights_scale=214.595215, weights_zero_point=-131.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.136314, output_zero_point=0.000000
          weights_scale=290.781158, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.914577, output_zero_point=0.000000
          weights_scale=235.433029, weights_zero_point=-101.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.680706, output_zero_point=0.000000
          weights_scale=347.535034, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=58.395302, output_zero_point=0.000000
          weights_scale=437.720947, weights_zero_point=-104.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=71.163963, output_zero_point=0.000000
          weights_scale=141.906174, weights_zero_point=-125.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (AuxLogits): InceptionAux(
      (conv0): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          weights_scale=329.854614, weights_zero_point=-91.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (bn): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (conv1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          weights_scale=1550.066895, weights_zero_point=-127.000000
          (wrapped_module): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)
        )
        (bn): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (fc): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=False
        weights_scale=391.454559, weights_zero_point=-89.000000
        base_bias_scale=1264299008.000000, base_bias_zero_point=0.000000
        (wrapped_module): Linear(in_features=768, out_features=1000, bias=True)
      )
    )
    (Mixed_7a): InceptionD(
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=42.466160, output_zero_point=0.000000
          weights_scale=156.875381, weights_zero_point=-115.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.123844, output_zero_point=0.000000
          weights_scale=281.107880, weights_zero_point=-77.000000
          (wrapped_module): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
      (branch7x7x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.423519, output_zero_point=0.000000
          weights_scale=109.459068, weights_zero_point=-118.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7x3_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.102749, output_zero_point=0.000000
          weights_scale=283.353882, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7x3_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=52.738613, output_zero_point=0.000000
          weights_scale=345.840942, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7x3_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=46.752419, output_zero_point=0.000000
          weights_scale=167.826477, weights_zero_point=-94.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
    )
    (Mixed_7b): InceptionE(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=62.377773, output_zero_point=0.000000
          weights_scale=158.751724, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=51.599426, output_zero_point=0.000000
          weights_scale=225.686890, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=57.921940, output_zero_point=0.000000
          weights_scale=256.622345, weights_zero_point=-88.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=54.381519, output_zero_point=0.000000
          weights_scale=156.567505, weights_zero_point=-92.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=50.835201, output_zero_point=0.000000
          weights_scale=220.083130, weights_zero_point=-94.000000
          (wrapped_module): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=53.787712, output_zero_point=0.000000
          weights_scale=475.481689, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.791264, output_zero_point=0.000000
          weights_scale=204.687500, weights_zero_point=-117.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.639427, output_zero_point=0.000000
          weights_scale=331.727753, weights_zero_point=-102.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=105.061790, output_zero_point=0.000000
          weights_scale=129.995590, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_7c): InceptionE(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.803699, output_zero_point=0.000000
          weights_scale=69.987938, weights_zero_point=-85.000000
          (wrapped_module): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=57.517414, output_zero_point=0.000000
          weights_scale=131.107544, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=21.480377, output_zero_point=0.000000
          weights_scale=108.998085, weights_zero_point=-43.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=24.145729, output_zero_point=0.000000
          weights_scale=101.049492, weights_zero_point=-47.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=46.318707, output_zero_point=0.000000
          weights_scale=164.204681, weights_zero_point=-132.000000
          (wrapped_module): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.267349, output_zero_point=0.000000
          weights_scale=274.946320, weights_zero_point=-78.000000
          (wrapped_module): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.149796, output_zero_point=0.000000
          weights_scale=159.698288, weights_zero_point=-109.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=41.410839, output_zero_point=0.000000
          weights_scale=183.429901, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=53.418743, output_zero_point=0.000000
          weights_scale=47.231201, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (fc): RangeLinearQuantParamLayerWrapper(
      weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=21.503002, output_zero_point=-51.000000
      weights_scale=624.406128, weights_zero_point=-109.000000
      (wrapped_module): Linear(in_features=2048, out_features=1000, bias=True)
    )
  )
)

2021-11-13 17:14:21,071 - Per-layer quantization parameters saved to logs/resnet50-imagenet-grayencode___2021.11.13-171404/layer_quant_params.yaml
2021-11-13 17:14:25,113 - --- test ---------------------
2021-11-13 17:14:25,114 - 50000 samples (256 per mini-batch)
2021-11-13 17:14:54,384 - Test: [   10/  195]    Loss 1.044037    Top1 76.367188    Top5 92.968750    
2021-11-13 17:15:06,290 - Test: [   20/  195]    Loss 1.053027    Top1 76.113281    Top5 92.617188    
2021-11-13 17:15:18,174 - Test: [   30/  195]    Loss 1.052903    Top1 75.950521    Top5 92.565104    
2021-11-13 17:15:30,095 - Test: [   40/  195]    Loss 1.056150    Top1 75.869141    Top5 92.666016    
2021-11-13 17:15:42,012 - Test: [   50/  195]    Loss 1.053318    Top1 75.859375    Top5 92.757812    
2021-11-13 17:15:42,772 - 
2021-11-13 17:15:42,773 - Log file for this run: /home/th.nguyen/drift-encode/logs/resnet50-imagenet-grayencode___2021.11.13-171404/resnet50-imagenet-grayencode___2021.11.13-171404.log
