2021-11-20 13:22:24,105 - Log file for this run: /home/th.nguyen/drift-encode/logs/Inception_v3-imagenet-helmet___2021.11.20-132224/Inception_v3-imagenet-helmet___2021.11.20-132224.log
2021-11-20 13:22:24,105 - Number of CPUs: 56
2021-11-20 13:22:24,290 - Number of GPUs: 6
2021-11-20 13:22:24,290 - CUDA version: 10.0.130
2021-11-20 13:22:24,293 - CUDNN version: 7603
2021-11-20 13:22:24,293 - Kernel: 5.4.0-90-generic
2021-11-20 13:22:24,293 - Python: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
[GCC 7.5.0]
2021-11-20 13:22:24,294 - pip freeze: {'absl-py': '0.12.0', 'aiohttp': '3.7.4.post0', 'argon2-cffi': '21.1.0', 'astor': '0.8.1', 'astunparse': '1.6.3', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'backcall': '0.2.0', 'black': '21.9b0', 'bleach': '4.1.0', 'blessed': '1.19.0', 'blessings': '1.7', 'blinker': '1.4', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2020.12.5', 'cffi': '1.14.6', 'chardet': '4.0.0', 'charset-normalizer': '2.0.4', 'click': '8.0.1', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'dataclasses': '0.8', 'decorator': '5.1.0', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'gast': '0.2.2', 'gitdb': '4.0.9', 'gitdb2': '3.0.3.post1', 'gitpython': '3.1.0', 'google-auth': '1.30.1', 'google-auth-oauthlib': '0.4.4', 'google-pasta': '0.2.0', 'gpustat': '1.0.0.dev1', 'graphviz': '0.10.1', 'grpcio': '1.38.0', 'gym': '0.12.5', 'h5py': '2.10.0', 'idna': '2.10', 'idna-ssl': '1.1.0', 'importlib-metadata': '4.8.1', 'ipykernel': '5.5.6', 'ipython': '7.16.1', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.17.2', 'jinja2': '3.0.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '7.0.6', 'jupyter-console': '6.4.0', 'jupyter-core': '4.8.1', 'jupyterlab-pygments': '0.1.2', 'keras-applications': '1.0.8', 'keras-preprocessing': '1.1.2', 'kiwisolver': '1.3.1', 'markdown': '3.3.4', 'markupsafe': '2.0.1', 'matplotlib': '3.3.4', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.10.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'mypy-extensions': '0.4.3', 'nbclient': '0.5.4', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'notebook': '6.4.4', 'numpy': '1.18.5', 'nvidia-ml-py3': '7.352.0', 'oauthlib': '3.1.0', 'olefile': '0.46', 'opt-einsum': '3.3.0', 'packaging': '21.0', 'pandas': '1.1.5', 'pandocfilters': '1.5.0', 'parse': '1.19.0', 'parso': '0.7.1', 'pathspec': '0.9.0', 'pexpect': '4.8.0', 'pickle5': '0.0.11', 'pickleshare': '0.7.5', 'pillow': '6.2.2', 'pip': '21.2.2', 'platformdirs': '2.4.0', 'pluggy': '0.13.1', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.11.0', 'prompt-toolkit': '3.0.20', 'protobuf': '3.17.1', 'psutil': '5.8.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pyglet': '1.5.21', 'pygments': '2.10.0', 'pyhocon': '0.3.58', 'pyjwt': '2.1.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyrsistent': '0.18.0', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'python-jsonrpc-server': '0.4.0', 'python-language-server': '0.36.2', 'pytz': '2021.3', 'pyyaml': '6.0', 'pyzmq': '22.3.0', 'qgrid': '1.1.1', 'qtconsole': '5.1.1', 'qtpy': '1.11.2', 'ranger-fm': '1.9.3', 'regex': '2021.10.21', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'rsa': '4.7.2', 'scikit-learn': '0.21.2', 'scipy': '1.5.4', 'seaborn': '0.11.2', 'send2trash': '1.8.0', 'setuptools': '57.0.0', 'six': '1.16.0', 'smmap': '4.0.0', 'smmap2': '2.0.5', 'tabulate': '0.8.3', 'tensorboard': '1.15.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.0', 'tensorflow': '1.15.0', 'tensorflow-estimator': '1.15.1', 'termcolor': '1.1.0', 'terminado': '0.12.1', 'testpath': '0.5.0', 'timm': '0.4.9', 'tomli': '1.2.1', 'torch': '1.3.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchsummary': '1.5.1', 'torchvision': '0.4.2', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '4.3.3', 'traittypes': '0.2.1', 'typed-ast': '1.4.3', 'typing-extensions': '3.10.0.1', 'ujson': '4.1.0', 'urllib3': '1.26.4', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.2.1', 'werkzeug': '2.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '3.0.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2021-11-20 13:22:24,384 - Git is dirty
2021-11-20 13:22:24,385 - Active Git branch: master
2021-11-20 13:22:24,426 - Git commit: f401db9403c1608e10258733e013b6c087e3ef34
2021-11-20 13:22:24,427 - Command line: compress_classifier.py --arch Inception_v3 -p 10 -j 22 /home/imagenet/ --pretrained --run --qe-config-file ./conf/inception_conf.yaml --gpus 5 --name Inception_v3-imagenet-helmet --method helmet --mlc 8 --num_bits 8 --encode
2021-11-20 13:22:24,427 - Distiller: 0.4.0rc0
2021-11-20 13:22:24,429 - Random seed: 330
2021-11-20 13:22:27,587 - => created a pretrained inception_v3 model with the imagenet dataset
2021-11-20 13:22:29,625 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2021-11-20 13:22:29,628 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2021-11-20 13:22:29,812 - Dataset sizes:
	test=50000
2021-11-20 13:22:29,813 - Reading configuration from: ./conf/inception_conf.yaml
2021-11-20 13:22:29,818 - Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers
2021-11-20 13:22:29,821 - Loading activation stats from: ./quant_stats/inceptionv3-imagenet.yaml
2021-11-20 13:22:30,538 - Preparing model for quantization using PostTrainLinearQuantizer
2021-11-20 13:22:36,934 - Applying batch-norm folding ahead of post-training quantization
2021-11-20 13:22:36,937 - Fusing sequence ['module.Conv2d_1a_3x3.conv', 'module.Conv2d_1a_3x3.bn']
2021-11-20 13:22:36,938 - Fusing sequence ['module.Conv2d_2a_3x3.conv', 'module.Conv2d_2a_3x3.bn']
2021-11-20 13:22:36,939 - Fusing sequence ['module.Conv2d_2b_3x3.conv', 'module.Conv2d_2b_3x3.bn']
2021-11-20 13:22:36,939 - Fusing sequence ['module.Conv2d_3b_1x1.conv', 'module.Conv2d_3b_1x1.bn']
2021-11-20 13:22:36,939 - Fusing sequence ['module.Conv2d_4a_3x3.conv', 'module.Conv2d_4a_3x3.bn']
2021-11-20 13:22:36,940 - Fusing sequence ['module.Mixed_5b.branch1x1.conv', 'module.Mixed_5b.branch1x1.bn']
2021-11-20 13:22:36,940 - Fusing sequence ['module.Mixed_5b.branch5x5_1.conv', 'module.Mixed_5b.branch5x5_1.bn']
2021-11-20 13:22:36,941 - Fusing sequence ['module.Mixed_5b.branch5x5_2.conv', 'module.Mixed_5b.branch5x5_2.bn']
2021-11-20 13:22:36,941 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_1.conv', 'module.Mixed_5b.branch3x3dbl_1.bn']
2021-11-20 13:22:36,941 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_2.conv', 'module.Mixed_5b.branch3x3dbl_2.bn']
2021-11-20 13:22:36,942 - Fusing sequence ['module.Mixed_5b.branch3x3dbl_3.conv', 'module.Mixed_5b.branch3x3dbl_3.bn']
2021-11-20 13:22:36,942 - Fusing sequence ['module.Mixed_5b.branch_pool.conv', 'module.Mixed_5b.branch_pool.bn']
2021-11-20 13:22:36,942 - Fusing sequence ['module.Mixed_5c.branch1x1.conv', 'module.Mixed_5c.branch1x1.bn']
2021-11-20 13:22:36,943 - Fusing sequence ['module.Mixed_5c.branch5x5_1.conv', 'module.Mixed_5c.branch5x5_1.bn']
2021-11-20 13:22:36,943 - Fusing sequence ['module.Mixed_5c.branch5x5_2.conv', 'module.Mixed_5c.branch5x5_2.bn']
2021-11-20 13:22:36,943 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_1.conv', 'module.Mixed_5c.branch3x3dbl_1.bn']
2021-11-20 13:22:36,944 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_2.conv', 'module.Mixed_5c.branch3x3dbl_2.bn']
2021-11-20 13:22:36,944 - Fusing sequence ['module.Mixed_5c.branch3x3dbl_3.conv', 'module.Mixed_5c.branch3x3dbl_3.bn']
2021-11-20 13:22:36,945 - Fusing sequence ['module.Mixed_5c.branch_pool.conv', 'module.Mixed_5c.branch_pool.bn']
2021-11-20 13:22:36,945 - Fusing sequence ['module.Mixed_5d.branch1x1.conv', 'module.Mixed_5d.branch1x1.bn']
2021-11-20 13:22:36,945 - Fusing sequence ['module.Mixed_5d.branch5x5_1.conv', 'module.Mixed_5d.branch5x5_1.bn']
2021-11-20 13:22:36,946 - Fusing sequence ['module.Mixed_5d.branch5x5_2.conv', 'module.Mixed_5d.branch5x5_2.bn']
2021-11-20 13:22:36,946 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_1.conv', 'module.Mixed_5d.branch3x3dbl_1.bn']
2021-11-20 13:22:36,946 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_2.conv', 'module.Mixed_5d.branch3x3dbl_2.bn']
2021-11-20 13:22:36,947 - Fusing sequence ['module.Mixed_5d.branch3x3dbl_3.conv', 'module.Mixed_5d.branch3x3dbl_3.bn']
2021-11-20 13:22:36,947 - Fusing sequence ['module.Mixed_5d.branch_pool.conv', 'module.Mixed_5d.branch_pool.bn']
2021-11-20 13:22:36,947 - Fusing sequence ['module.Mixed_6a.branch3x3.conv', 'module.Mixed_6a.branch3x3.bn']
2021-11-20 13:22:36,948 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_1.conv', 'module.Mixed_6a.branch3x3dbl_1.bn']
2021-11-20 13:22:36,948 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_2.conv', 'module.Mixed_6a.branch3x3dbl_2.bn']
2021-11-20 13:22:36,948 - Fusing sequence ['module.Mixed_6a.branch3x3dbl_3.conv', 'module.Mixed_6a.branch3x3dbl_3.bn']
2021-11-20 13:22:36,949 - Fusing sequence ['module.Mixed_6b.branch1x1.conv', 'module.Mixed_6b.branch1x1.bn']
2021-11-20 13:22:36,949 - Fusing sequence ['module.Mixed_6b.branch7x7_1.conv', 'module.Mixed_6b.branch7x7_1.bn']
2021-11-20 13:22:36,950 - Fusing sequence ['module.Mixed_6b.branch7x7_2.conv', 'module.Mixed_6b.branch7x7_2.bn']
2021-11-20 13:22:36,950 - Fusing sequence ['module.Mixed_6b.branch7x7_3.conv', 'module.Mixed_6b.branch7x7_3.bn']
2021-11-20 13:22:36,950 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_1.conv', 'module.Mixed_6b.branch7x7dbl_1.bn']
2021-11-20 13:22:36,951 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_2.conv', 'module.Mixed_6b.branch7x7dbl_2.bn']
2021-11-20 13:22:36,951 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_3.conv', 'module.Mixed_6b.branch7x7dbl_3.bn']
2021-11-20 13:22:36,951 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_4.conv', 'module.Mixed_6b.branch7x7dbl_4.bn']
2021-11-20 13:22:36,952 - Fusing sequence ['module.Mixed_6b.branch7x7dbl_5.conv', 'module.Mixed_6b.branch7x7dbl_5.bn']
2021-11-20 13:22:36,952 - Fusing sequence ['module.Mixed_6b.branch_pool.conv', 'module.Mixed_6b.branch_pool.bn']
2021-11-20 13:22:36,952 - Fusing sequence ['module.Mixed_6c.branch1x1.conv', 'module.Mixed_6c.branch1x1.bn']
2021-11-20 13:22:36,953 - Fusing sequence ['module.Mixed_6c.branch7x7_1.conv', 'module.Mixed_6c.branch7x7_1.bn']
2021-11-20 13:22:36,953 - Fusing sequence ['module.Mixed_6c.branch7x7_2.conv', 'module.Mixed_6c.branch7x7_2.bn']
2021-11-20 13:22:36,953 - Fusing sequence ['module.Mixed_6c.branch7x7_3.conv', 'module.Mixed_6c.branch7x7_3.bn']
2021-11-20 13:22:36,954 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_1.conv', 'module.Mixed_6c.branch7x7dbl_1.bn']
2021-11-20 13:22:36,954 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_2.conv', 'module.Mixed_6c.branch7x7dbl_2.bn']
2021-11-20 13:22:36,954 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_3.conv', 'module.Mixed_6c.branch7x7dbl_3.bn']
2021-11-20 13:22:36,955 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_4.conv', 'module.Mixed_6c.branch7x7dbl_4.bn']
2021-11-20 13:22:36,955 - Fusing sequence ['module.Mixed_6c.branch7x7dbl_5.conv', 'module.Mixed_6c.branch7x7dbl_5.bn']
2021-11-20 13:22:36,956 - Fusing sequence ['module.Mixed_6c.branch_pool.conv', 'module.Mixed_6c.branch_pool.bn']
2021-11-20 13:22:36,956 - Fusing sequence ['module.Mixed_6d.branch1x1.conv', 'module.Mixed_6d.branch1x1.bn']
2021-11-20 13:22:36,956 - Fusing sequence ['module.Mixed_6d.branch7x7_1.conv', 'module.Mixed_6d.branch7x7_1.bn']
2021-11-20 13:22:36,957 - Fusing sequence ['module.Mixed_6d.branch7x7_2.conv', 'module.Mixed_6d.branch7x7_2.bn']
2021-11-20 13:22:36,957 - Fusing sequence ['module.Mixed_6d.branch7x7_3.conv', 'module.Mixed_6d.branch7x7_3.bn']
2021-11-20 13:22:36,957 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_1.conv', 'module.Mixed_6d.branch7x7dbl_1.bn']
2021-11-20 13:22:36,958 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_2.conv', 'module.Mixed_6d.branch7x7dbl_2.bn']
2021-11-20 13:22:36,958 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_3.conv', 'module.Mixed_6d.branch7x7dbl_3.bn']
2021-11-20 13:22:36,958 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_4.conv', 'module.Mixed_6d.branch7x7dbl_4.bn']
2021-11-20 13:22:36,959 - Fusing sequence ['module.Mixed_6d.branch7x7dbl_5.conv', 'module.Mixed_6d.branch7x7dbl_5.bn']
2021-11-20 13:22:36,959 - Fusing sequence ['module.Mixed_6d.branch_pool.conv', 'module.Mixed_6d.branch_pool.bn']
2021-11-20 13:22:36,959 - Fusing sequence ['module.Mixed_6e.branch1x1.conv', 'module.Mixed_6e.branch1x1.bn']
2021-11-20 13:22:36,960 - Fusing sequence ['module.Mixed_6e.branch7x7_1.conv', 'module.Mixed_6e.branch7x7_1.bn']
2021-11-20 13:22:36,960 - Fusing sequence ['module.Mixed_6e.branch7x7_2.conv', 'module.Mixed_6e.branch7x7_2.bn']
2021-11-20 13:22:36,960 - Fusing sequence ['module.Mixed_6e.branch7x7_3.conv', 'module.Mixed_6e.branch7x7_3.bn']
2021-11-20 13:22:36,961 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_1.conv', 'module.Mixed_6e.branch7x7dbl_1.bn']
2021-11-20 13:22:36,961 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_2.conv', 'module.Mixed_6e.branch7x7dbl_2.bn']
2021-11-20 13:22:36,962 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_3.conv', 'module.Mixed_6e.branch7x7dbl_3.bn']
2021-11-20 13:22:36,962 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_4.conv', 'module.Mixed_6e.branch7x7dbl_4.bn']
2021-11-20 13:22:36,962 - Fusing sequence ['module.Mixed_6e.branch7x7dbl_5.conv', 'module.Mixed_6e.branch7x7dbl_5.bn']
2021-11-20 13:22:36,963 - Fusing sequence ['module.Mixed_6e.branch_pool.conv', 'module.Mixed_6e.branch_pool.bn']
2021-11-20 13:22:36,963 - Fusing sequence ['module.Mixed_7a.branch3x3_1.conv', 'module.Mixed_7a.branch3x3_1.bn']
2021-11-20 13:22:36,963 - Fusing sequence ['module.Mixed_7a.branch3x3_2.conv', 'module.Mixed_7a.branch3x3_2.bn']
2021-11-20 13:22:36,964 - Fusing sequence ['module.Mixed_7a.branch7x7x3_1.conv', 'module.Mixed_7a.branch7x7x3_1.bn']
2021-11-20 13:22:36,964 - Fusing sequence ['module.Mixed_7a.branch7x7x3_2.conv', 'module.Mixed_7a.branch7x7x3_2.bn']
2021-11-20 13:22:36,965 - Fusing sequence ['module.Mixed_7a.branch7x7x3_3.conv', 'module.Mixed_7a.branch7x7x3_3.bn']
2021-11-20 13:22:36,965 - Fusing sequence ['module.Mixed_7a.branch7x7x3_4.conv', 'module.Mixed_7a.branch7x7x3_4.bn']
2021-11-20 13:22:36,965 - Fusing sequence ['module.Mixed_7b.branch1x1.conv', 'module.Mixed_7b.branch1x1.bn']
2021-11-20 13:22:36,966 - Fusing sequence ['module.Mixed_7b.branch3x3_1.conv', 'module.Mixed_7b.branch3x3_1.bn']
2021-11-20 13:22:36,966 - Fusing sequence ['module.Mixed_7b.branch3x3_2a.conv', 'module.Mixed_7b.branch3x3_2a.bn']
2021-11-20 13:22:36,966 - Fusing sequence ['module.Mixed_7b.branch3x3_2b.conv', 'module.Mixed_7b.branch3x3_2b.bn']
2021-11-20 13:22:36,967 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_1.conv', 'module.Mixed_7b.branch3x3dbl_1.bn']
2021-11-20 13:22:36,967 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_2.conv', 'module.Mixed_7b.branch3x3dbl_2.bn']
2021-11-20 13:22:36,967 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_3a.conv', 'module.Mixed_7b.branch3x3dbl_3a.bn']
2021-11-20 13:22:36,968 - Fusing sequence ['module.Mixed_7b.branch3x3dbl_3b.conv', 'module.Mixed_7b.branch3x3dbl_3b.bn']
2021-11-20 13:22:36,968 - Fusing sequence ['module.Mixed_7b.branch_pool.conv', 'module.Mixed_7b.branch_pool.bn']
2021-11-20 13:22:36,968 - Fusing sequence ['module.Mixed_7c.branch1x1.conv', 'module.Mixed_7c.branch1x1.bn']
2021-11-20 13:22:36,969 - Fusing sequence ['module.Mixed_7c.branch3x3_1.conv', 'module.Mixed_7c.branch3x3_1.bn']
2021-11-20 13:22:36,969 - Fusing sequence ['module.Mixed_7c.branch3x3_2a.conv', 'module.Mixed_7c.branch3x3_2a.bn']
2021-11-20 13:22:36,969 - Fusing sequence ['module.Mixed_7c.branch3x3_2b.conv', 'module.Mixed_7c.branch3x3_2b.bn']
2021-11-20 13:22:36,970 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_1.conv', 'module.Mixed_7c.branch3x3dbl_1.bn']
2021-11-20 13:22:36,970 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_2.conv', 'module.Mixed_7c.branch3x3dbl_2.bn']
2021-11-20 13:22:36,970 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_3a.conv', 'module.Mixed_7c.branch3x3dbl_3a.bn']
2021-11-20 13:22:36,971 - Fusing sequence ['module.Mixed_7c.branch3x3dbl_3b.conv', 'module.Mixed_7c.branch3x3dbl_3b.bn']
2021-11-20 13:22:36,971 - Fusing sequence ['module.Mixed_7c.branch_pool.conv', 'module.Mixed_7c.branch_pool.bn']
2021-11-20 13:22:39,286 - Propagating output statistics from BN modules to folded modules
2021-11-20 13:22:39,287 -   Conv2d_1a_3x3.bn --> module.Conv2d_1a_3x3.conv
2021-11-20 13:22:39,287 -   Conv2d_2a_3x3.bn --> module.Conv2d_2a_3x3.conv
2021-11-20 13:22:39,287 -   Conv2d_2b_3x3.bn --> module.Conv2d_2b_3x3.conv
2021-11-20 13:22:39,287 -   Conv2d_3b_1x1.bn --> module.Conv2d_3b_1x1.conv
2021-11-20 13:22:39,287 -   Conv2d_4a_3x3.bn --> module.Conv2d_4a_3x3.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch1x1.bn --> module.Mixed_5b.branch1x1.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch5x5_1.bn --> module.Mixed_5b.branch5x5_1.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch5x5_2.bn --> module.Mixed_5b.branch5x5_2.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch3x3dbl_1.bn --> module.Mixed_5b.branch3x3dbl_1.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch3x3dbl_2.bn --> module.Mixed_5b.branch3x3dbl_2.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch3x3dbl_3.bn --> module.Mixed_5b.branch3x3dbl_3.conv
2021-11-20 13:22:39,287 -   Mixed_5b.branch_pool.bn --> module.Mixed_5b.branch_pool.conv
2021-11-20 13:22:39,287 -   Mixed_5c.branch1x1.bn --> module.Mixed_5c.branch1x1.conv
2021-11-20 13:22:39,287 -   Mixed_5c.branch5x5_1.bn --> module.Mixed_5c.branch5x5_1.conv
2021-11-20 13:22:39,287 -   Mixed_5c.branch5x5_2.bn --> module.Mixed_5c.branch5x5_2.conv
2021-11-20 13:22:39,288 -   Mixed_5c.branch3x3dbl_1.bn --> module.Mixed_5c.branch3x3dbl_1.conv
2021-11-20 13:22:39,288 -   Mixed_5c.branch3x3dbl_2.bn --> module.Mixed_5c.branch3x3dbl_2.conv
2021-11-20 13:22:39,288 -   Mixed_5c.branch3x3dbl_3.bn --> module.Mixed_5c.branch3x3dbl_3.conv
2021-11-20 13:22:39,288 -   Mixed_5c.branch_pool.bn --> module.Mixed_5c.branch_pool.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch1x1.bn --> module.Mixed_5d.branch1x1.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch5x5_1.bn --> module.Mixed_5d.branch5x5_1.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch5x5_2.bn --> module.Mixed_5d.branch5x5_2.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch3x3dbl_1.bn --> module.Mixed_5d.branch3x3dbl_1.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch3x3dbl_2.bn --> module.Mixed_5d.branch3x3dbl_2.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch3x3dbl_3.bn --> module.Mixed_5d.branch3x3dbl_3.conv
2021-11-20 13:22:39,288 -   Mixed_5d.branch_pool.bn --> module.Mixed_5d.branch_pool.conv
2021-11-20 13:22:39,288 -   Mixed_6a.branch3x3.bn --> module.Mixed_6a.branch3x3.conv
2021-11-20 13:22:39,288 -   Mixed_6a.branch3x3dbl_1.bn --> module.Mixed_6a.branch3x3dbl_1.conv
2021-11-20 13:22:39,288 -   Mixed_6a.branch3x3dbl_2.bn --> module.Mixed_6a.branch3x3dbl_2.conv
2021-11-20 13:22:39,288 -   Mixed_6a.branch3x3dbl_3.bn --> module.Mixed_6a.branch3x3dbl_3.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch1x1.bn --> module.Mixed_6b.branch1x1.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7_1.bn --> module.Mixed_6b.branch7x7_1.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7_2.bn --> module.Mixed_6b.branch7x7_2.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7_3.bn --> module.Mixed_6b.branch7x7_3.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7dbl_1.bn --> module.Mixed_6b.branch7x7dbl_1.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7dbl_2.bn --> module.Mixed_6b.branch7x7dbl_2.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7dbl_3.bn --> module.Mixed_6b.branch7x7dbl_3.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7dbl_4.bn --> module.Mixed_6b.branch7x7dbl_4.conv
2021-11-20 13:22:39,288 -   Mixed_6b.branch7x7dbl_5.bn --> module.Mixed_6b.branch7x7dbl_5.conv
2021-11-20 13:22:39,289 -   Mixed_6b.branch_pool.bn --> module.Mixed_6b.branch_pool.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch1x1.bn --> module.Mixed_6c.branch1x1.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7_1.bn --> module.Mixed_6c.branch7x7_1.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7_2.bn --> module.Mixed_6c.branch7x7_2.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7_3.bn --> module.Mixed_6c.branch7x7_3.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7dbl_1.bn --> module.Mixed_6c.branch7x7dbl_1.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7dbl_2.bn --> module.Mixed_6c.branch7x7dbl_2.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7dbl_3.bn --> module.Mixed_6c.branch7x7dbl_3.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7dbl_4.bn --> module.Mixed_6c.branch7x7dbl_4.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch7x7dbl_5.bn --> module.Mixed_6c.branch7x7dbl_5.conv
2021-11-20 13:22:39,289 -   Mixed_6c.branch_pool.bn --> module.Mixed_6c.branch_pool.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch1x1.bn --> module.Mixed_6d.branch1x1.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7_1.bn --> module.Mixed_6d.branch7x7_1.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7_2.bn --> module.Mixed_6d.branch7x7_2.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7_3.bn --> module.Mixed_6d.branch7x7_3.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7dbl_1.bn --> module.Mixed_6d.branch7x7dbl_1.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7dbl_2.bn --> module.Mixed_6d.branch7x7dbl_2.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7dbl_3.bn --> module.Mixed_6d.branch7x7dbl_3.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7dbl_4.bn --> module.Mixed_6d.branch7x7dbl_4.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch7x7dbl_5.bn --> module.Mixed_6d.branch7x7dbl_5.conv
2021-11-20 13:22:39,289 -   Mixed_6d.branch_pool.bn --> module.Mixed_6d.branch_pool.conv
2021-11-20 13:22:39,289 -   Mixed_6e.branch1x1.bn --> module.Mixed_6e.branch1x1.conv
2021-11-20 13:22:39,289 -   Mixed_6e.branch7x7_1.bn --> module.Mixed_6e.branch7x7_1.conv
2021-11-20 13:22:39,289 -   Mixed_6e.branch7x7_2.bn --> module.Mixed_6e.branch7x7_2.conv
2021-11-20 13:22:39,289 -   Mixed_6e.branch7x7_3.bn --> module.Mixed_6e.branch7x7_3.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch7x7dbl_1.bn --> module.Mixed_6e.branch7x7dbl_1.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch7x7dbl_2.bn --> module.Mixed_6e.branch7x7dbl_2.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch7x7dbl_3.bn --> module.Mixed_6e.branch7x7dbl_3.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch7x7dbl_4.bn --> module.Mixed_6e.branch7x7dbl_4.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch7x7dbl_5.bn --> module.Mixed_6e.branch7x7dbl_5.conv
2021-11-20 13:22:39,290 -   Mixed_6e.branch_pool.bn --> module.Mixed_6e.branch_pool.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch3x3_1.bn --> module.Mixed_7a.branch3x3_1.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch3x3_2.bn --> module.Mixed_7a.branch3x3_2.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch7x7x3_1.bn --> module.Mixed_7a.branch7x7x3_1.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch7x7x3_2.bn --> module.Mixed_7a.branch7x7x3_2.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch7x7x3_3.bn --> module.Mixed_7a.branch7x7x3_3.conv
2021-11-20 13:22:39,290 -   Mixed_7a.branch7x7x3_4.bn --> module.Mixed_7a.branch7x7x3_4.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch1x1.bn --> module.Mixed_7b.branch1x1.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3_1.bn --> module.Mixed_7b.branch3x3_1.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3_2a.bn --> module.Mixed_7b.branch3x3_2a.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3_2b.bn --> module.Mixed_7b.branch3x3_2b.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3dbl_1.bn --> module.Mixed_7b.branch3x3dbl_1.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3dbl_2.bn --> module.Mixed_7b.branch3x3dbl_2.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3dbl_3a.bn --> module.Mixed_7b.branch3x3dbl_3a.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch3x3dbl_3b.bn --> module.Mixed_7b.branch3x3dbl_3b.conv
2021-11-20 13:22:39,290 -   Mixed_7b.branch_pool.bn --> module.Mixed_7b.branch_pool.conv
2021-11-20 13:22:39,290 -   Mixed_7c.branch1x1.bn --> module.Mixed_7c.branch1x1.conv
2021-11-20 13:22:39,290 -   Mixed_7c.branch3x3_1.bn --> module.Mixed_7c.branch3x3_1.conv
2021-11-20 13:22:39,290 -   Mixed_7c.branch3x3_2a.bn --> module.Mixed_7c.branch3x3_2a.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch3x3_2b.bn --> module.Mixed_7c.branch3x3_2b.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch3x3dbl_1.bn --> module.Mixed_7c.branch3x3dbl_1.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch3x3dbl_2.bn --> module.Mixed_7c.branch3x3dbl_2.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch3x3dbl_3a.bn --> module.Mixed_7c.branch3x3dbl_3a.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch3x3dbl_3b.bn --> module.Mixed_7c.branch3x3dbl_3b.conv
2021-11-20 13:22:39,291 -   Mixed_7c.branch_pool.bn --> module.Mixed_7c.branch_pool.conv
2021-11-20 13:22:39,291 - Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid
2021-11-20 13:22:39,291 -   Module Conv2d_1a_3x3.conv followed by Relu, updating stats
2021-11-20 13:22:39,291 -   Module Conv2d_2a_3x3.conv followed by Relu, updating stats
2021-11-20 13:22:39,291 -   Module Conv2d_2b_3x3.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Conv2d_3b_1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Conv2d_4a_3x3.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch5x5_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch5x5_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5b.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch5x5_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch5x5_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5c.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch5x5_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch5x5_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,292 -   Module Mixed_5d.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6a.branch3x3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6a.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6a.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6a.branch3x3dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6b.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,293 -   Module Mixed_6c.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6c.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6c.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6d.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7dbl_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7dbl_4.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch7x7dbl_5.conv followed by Relu, updating stats
2021-11-20 13:22:39,294 -   Module Mixed_6e.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch3x3_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch3x3_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch7x7x3_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch7x7x3_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch7x7x3_3.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7a.branch7x7x3_4.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3_2a.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3_2b.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3dbl_3a.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch3x3dbl_3b.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7b.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch1x1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3_2a.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3_2b.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3dbl_1.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3dbl_2.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3dbl_3a.conv followed by Relu, updating stats
2021-11-20 13:22:39,295 -   Module Mixed_7c.branch3x3dbl_3b.conv followed by Relu, updating stats
2021-11-20 13:22:39,296 -   Module Mixed_7c.branch_pool.conv followed by Relu, updating stats
2021-11-20 13:22:39,430 - Updated stats saved to logs/Inception_v3-imagenet-helmet___2021.11.20-132224/quant_stats_after_prepare_model.yaml
2021-11-20 13:22:39,430 - Module module
2021-11-20 13:22:39,430 - 	Skipping
2021-11-20 13:22:39,430 - Module module.Conv2d_1a_3x3
2021-11-20 13:22:39,431 - 	Skipping
2021-11-20 13:22:39,434 - Module module.Conv2d_1a_3x3.conv
2021-11-20 13:22:39,434 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,434 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,436 - Module module.Conv2d_2a_3x3
2021-11-20 13:22:39,436 - 	Skipping
2021-11-20 13:22:39,437 - Module module.Conv2d_2a_3x3.conv
2021-11-20 13:22:39,437 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,437 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,439 - Module module.Conv2d_2b_3x3
2021-11-20 13:22:39,439 - 	Skipping
2021-11-20 13:22:39,440 - Module module.Conv2d_2b_3x3.conv
2021-11-20 13:22:39,441 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,441 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,442 - Module module.Conv2d_3b_1x1
2021-11-20 13:22:39,442 - 	Skipping
2021-11-20 13:22:39,444 - Module module.Conv2d_3b_1x1.conv
2021-11-20 13:22:39,444 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,444 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,446 - Module module.Conv2d_4a_3x3
2021-11-20 13:22:39,446 - 	Skipping
2021-11-20 13:22:39,447 - Module module.Conv2d_4a_3x3.conv
2021-11-20 13:22:39,447 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,447 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,449 - Module module.Mixed_5b
2021-11-20 13:22:39,449 - 	Skipping
2021-11-20 13:22:39,449 - Module module.Mixed_5b.branch1x1
2021-11-20 13:22:39,449 - 	Skipping
2021-11-20 13:22:39,450 - Module module.Mixed_5b.branch1x1.conv
2021-11-20 13:22:39,451 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,451 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,453 - Module module.Mixed_5b.branch5x5_1
2021-11-20 13:22:39,453 - 	Skipping
2021-11-20 13:22:39,454 - Module module.Mixed_5b.branch5x5_1.conv
2021-11-20 13:22:39,454 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,454 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,456 - Module module.Mixed_5b.branch5x5_2
2021-11-20 13:22:39,456 - 	Skipping
2021-11-20 13:22:39,457 - Module module.Mixed_5b.branch5x5_2.conv
2021-11-20 13:22:39,457 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,457 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,459 - Module module.Mixed_5b.branch3x3dbl_1
2021-11-20 13:22:39,459 - 	Skipping
2021-11-20 13:22:39,461 - Module module.Mixed_5b.branch3x3dbl_1.conv
2021-11-20 13:22:39,461 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,461 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,463 - Module module.Mixed_5b.branch3x3dbl_2
2021-11-20 13:22:39,463 - 	Skipping
2021-11-20 13:22:39,464 - Module module.Mixed_5b.branch3x3dbl_2.conv
2021-11-20 13:22:39,464 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,464 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,466 - Module module.Mixed_5b.branch3x3dbl_3
2021-11-20 13:22:39,466 - 	Skipping
2021-11-20 13:22:39,467 - Module module.Mixed_5b.branch3x3dbl_3.conv
2021-11-20 13:22:39,468 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,468 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,470 - Module module.Mixed_5b.branch_pool
2021-11-20 13:22:39,470 - 	Skipping
2021-11-20 13:22:39,471 - Module module.Mixed_5b.branch_pool.conv
2021-11-20 13:22:39,471 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,471 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,473 - Module module.Mixed_5c
2021-11-20 13:22:39,473 - 	Skipping
2021-11-20 13:22:39,473 - Module module.Mixed_5c.branch1x1
2021-11-20 13:22:39,473 - 	Skipping
2021-11-20 13:22:39,474 - Module module.Mixed_5c.branch1x1.conv
2021-11-20 13:22:39,475 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,475 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,477 - Module module.Mixed_5c.branch5x5_1
2021-11-20 13:22:39,477 - 	Skipping
2021-11-20 13:22:39,478 - Module module.Mixed_5c.branch5x5_1.conv
2021-11-20 13:22:39,478 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,478 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,480 - Module module.Mixed_5c.branch5x5_2
2021-11-20 13:22:39,480 - 	Skipping
2021-11-20 13:22:39,481 - Module module.Mixed_5c.branch5x5_2.conv
2021-11-20 13:22:39,481 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,481 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,483 - Module module.Mixed_5c.branch3x3dbl_1
2021-11-20 13:22:39,484 - 	Skipping
2021-11-20 13:22:39,485 - Module module.Mixed_5c.branch3x3dbl_1.conv
2021-11-20 13:22:39,485 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,485 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,487 - Module module.Mixed_5c.branch3x3dbl_2
2021-11-20 13:22:39,487 - 	Skipping
2021-11-20 13:22:39,488 - Module module.Mixed_5c.branch3x3dbl_2.conv
2021-11-20 13:22:39,488 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,488 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,490 - Module module.Mixed_5c.branch3x3dbl_3
2021-11-20 13:22:39,491 - 	Skipping
2021-11-20 13:22:39,492 - Module module.Mixed_5c.branch3x3dbl_3.conv
2021-11-20 13:22:39,492 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,492 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,494 - Module module.Mixed_5c.branch_pool
2021-11-20 13:22:39,494 - 	Skipping
2021-11-20 13:22:39,495 - Module module.Mixed_5c.branch_pool.conv
2021-11-20 13:22:39,495 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,495 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,498 - Module module.Mixed_5d
2021-11-20 13:22:39,498 - 	Skipping
2021-11-20 13:22:39,498 - Module module.Mixed_5d.branch1x1
2021-11-20 13:22:39,498 - 	Skipping
2021-11-20 13:22:39,499 - Module module.Mixed_5d.branch1x1.conv
2021-11-20 13:22:39,499 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,499 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,501 - Module module.Mixed_5d.branch5x5_1
2021-11-20 13:22:39,501 - 	Skipping
2021-11-20 13:22:39,503 - Module module.Mixed_5d.branch5x5_1.conv
2021-11-20 13:22:39,503 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,503 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,505 - Module module.Mixed_5d.branch5x5_2
2021-11-20 13:22:39,505 - 	Skipping
2021-11-20 13:22:39,506 - Module module.Mixed_5d.branch5x5_2.conv
2021-11-20 13:22:39,506 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,506 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,508 - Module module.Mixed_5d.branch3x3dbl_1
2021-11-20 13:22:39,509 - 	Skipping
2021-11-20 13:22:39,510 - Module module.Mixed_5d.branch3x3dbl_1.conv
2021-11-20 13:22:39,510 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,510 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,512 - Module module.Mixed_5d.branch3x3dbl_2
2021-11-20 13:22:39,512 - 	Skipping
2021-11-20 13:22:39,513 - Module module.Mixed_5d.branch3x3dbl_2.conv
2021-11-20 13:22:39,514 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,514 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,516 - Module module.Mixed_5d.branch3x3dbl_3
2021-11-20 13:22:39,516 - 	Skipping
2021-11-20 13:22:39,518 - Module module.Mixed_5d.branch3x3dbl_3.conv
2021-11-20 13:22:39,518 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,518 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,520 - Module module.Mixed_5d.branch_pool
2021-11-20 13:22:39,520 - 	Skipping
2021-11-20 13:22:39,521 - Module module.Mixed_5d.branch_pool.conv
2021-11-20 13:22:39,521 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,521 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,524 - Module module.Mixed_6a
2021-11-20 13:22:39,524 - 	Skipping
2021-11-20 13:22:39,524 - Module module.Mixed_6a.branch3x3
2021-11-20 13:22:39,524 - 	Skipping
2021-11-20 13:22:39,525 - Module module.Mixed_6a.branch3x3.conv
2021-11-20 13:22:39,525 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,525 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,528 - Module module.Mixed_6a.branch3x3dbl_1
2021-11-20 13:22:39,528 - 	Skipping
2021-11-20 13:22:39,529 - Module module.Mixed_6a.branch3x3dbl_1.conv
2021-11-20 13:22:39,529 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,529 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,531 - Module module.Mixed_6a.branch3x3dbl_2
2021-11-20 13:22:39,531 - 	Skipping
2021-11-20 13:22:39,533 - Module module.Mixed_6a.branch3x3dbl_2.conv
2021-11-20 13:22:39,533 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,533 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,535 - Module module.Mixed_6a.branch3x3dbl_3
2021-11-20 13:22:39,535 - 	Skipping
2021-11-20 13:22:39,536 - Module module.Mixed_6a.branch3x3dbl_3.conv
2021-11-20 13:22:39,536 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,536 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,539 - Module module.Mixed_6b
2021-11-20 13:22:39,539 - 	Skipping
2021-11-20 13:22:39,539 - Module module.Mixed_6b.branch1x1
2021-11-20 13:22:39,539 - 	Skipping
2021-11-20 13:22:39,540 - Module module.Mixed_6b.branch1x1.conv
2021-11-20 13:22:39,540 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,540 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,542 - Module module.Mixed_6b.branch7x7_1
2021-11-20 13:22:39,542 - 	Skipping
2021-11-20 13:22:39,544 - Module module.Mixed_6b.branch7x7_1.conv
2021-11-20 13:22:39,544 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,544 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,546 - Module module.Mixed_6b.branch7x7_2
2021-11-20 13:22:39,546 - 	Skipping
2021-11-20 13:22:39,547 - Module module.Mixed_6b.branch7x7_2.conv
2021-11-20 13:22:39,548 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,548 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,550 - Module module.Mixed_6b.branch7x7_3
2021-11-20 13:22:39,550 - 	Skipping
2021-11-20 13:22:39,551 - Module module.Mixed_6b.branch7x7_3.conv
2021-11-20 13:22:39,551 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,551 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,554 - Module module.Mixed_6b.branch7x7dbl_1
2021-11-20 13:22:39,554 - 	Skipping
2021-11-20 13:22:39,555 - Module module.Mixed_6b.branch7x7dbl_1.conv
2021-11-20 13:22:39,555 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,555 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,557 - Module module.Mixed_6b.branch7x7dbl_2
2021-11-20 13:22:39,557 - 	Skipping
2021-11-20 13:22:39,559 - Module module.Mixed_6b.branch7x7dbl_2.conv
2021-11-20 13:22:39,559 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,559 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,561 - Module module.Mixed_6b.branch7x7dbl_3
2021-11-20 13:22:39,561 - 	Skipping
2021-11-20 13:22:39,562 - Module module.Mixed_6b.branch7x7dbl_3.conv
2021-11-20 13:22:39,562 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,563 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,565 - Module module.Mixed_6b.branch7x7dbl_4
2021-11-20 13:22:39,565 - 	Skipping
2021-11-20 13:22:39,566 - Module module.Mixed_6b.branch7x7dbl_4.conv
2021-11-20 13:22:39,566 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,566 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,569 - Module module.Mixed_6b.branch7x7dbl_5
2021-11-20 13:22:39,569 - 	Skipping
2021-11-20 13:22:39,570 - Module module.Mixed_6b.branch7x7dbl_5.conv
2021-11-20 13:22:39,570 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,570 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,573 - Module module.Mixed_6b.branch_pool
2021-11-20 13:22:39,573 - 	Skipping
2021-11-20 13:22:39,574 - Module module.Mixed_6b.branch_pool.conv
2021-11-20 13:22:39,574 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,574 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,576 - Module module.Mixed_6c
2021-11-20 13:22:39,576 - 	Skipping
2021-11-20 13:22:39,576 - Module module.Mixed_6c.branch1x1
2021-11-20 13:22:39,577 - 	Skipping
2021-11-20 13:22:39,578 - Module module.Mixed_6c.branch1x1.conv
2021-11-20 13:22:39,578 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,578 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,580 - Module module.Mixed_6c.branch7x7_1
2021-11-20 13:22:39,580 - 	Skipping
2021-11-20 13:22:39,582 - Module module.Mixed_6c.branch7x7_1.conv
2021-11-20 13:22:39,582 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,582 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,584 - Module module.Mixed_6c.branch7x7_2
2021-11-20 13:22:39,584 - 	Skipping
2021-11-20 13:22:39,586 - Module module.Mixed_6c.branch7x7_2.conv
2021-11-20 13:22:39,586 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,586 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,588 - Module module.Mixed_6c.branch7x7_3
2021-11-20 13:22:39,588 - 	Skipping
2021-11-20 13:22:39,589 - Module module.Mixed_6c.branch7x7_3.conv
2021-11-20 13:22:39,589 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,589 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,592 - Module module.Mixed_6c.branch7x7dbl_1
2021-11-20 13:22:39,592 - 	Skipping
2021-11-20 13:22:39,593 - Module module.Mixed_6c.branch7x7dbl_1.conv
2021-11-20 13:22:39,593 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,593 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,596 - Module module.Mixed_6c.branch7x7dbl_2
2021-11-20 13:22:39,596 - 	Skipping
2021-11-20 13:22:39,597 - Module module.Mixed_6c.branch7x7dbl_2.conv
2021-11-20 13:22:39,597 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,597 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,600 - Module module.Mixed_6c.branch7x7dbl_3
2021-11-20 13:22:39,600 - 	Skipping
2021-11-20 13:22:39,601 - Module module.Mixed_6c.branch7x7dbl_3.conv
2021-11-20 13:22:39,601 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,601 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,604 - Module module.Mixed_6c.branch7x7dbl_4
2021-11-20 13:22:39,604 - 	Skipping
2021-11-20 13:22:39,605 - Module module.Mixed_6c.branch7x7dbl_4.conv
2021-11-20 13:22:39,605 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,605 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,607 - Module module.Mixed_6c.branch7x7dbl_5
2021-11-20 13:22:39,608 - 	Skipping
2021-11-20 13:22:39,609 - Module module.Mixed_6c.branch7x7dbl_5.conv
2021-11-20 13:22:39,609 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,609 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,611 - Module module.Mixed_6c.branch_pool
2021-11-20 13:22:39,611 - 	Skipping
2021-11-20 13:22:39,613 - Module module.Mixed_6c.branch_pool.conv
2021-11-20 13:22:39,613 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,613 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,615 - Module module.Mixed_6d
2021-11-20 13:22:39,615 - 	Skipping
2021-11-20 13:22:39,616 - Module module.Mixed_6d.branch1x1
2021-11-20 13:22:39,616 - 	Skipping
2021-11-20 13:22:39,617 - Module module.Mixed_6d.branch1x1.conv
2021-11-20 13:22:39,617 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,617 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,620 - Module module.Mixed_6d.branch7x7_1
2021-11-20 13:22:39,620 - 	Skipping
2021-11-20 13:22:39,622 - Module module.Mixed_6d.branch7x7_1.conv
2021-11-20 13:22:39,622 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,622 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,624 - Module module.Mixed_6d.branch7x7_2
2021-11-20 13:22:39,624 - 	Skipping
2021-11-20 13:22:39,626 - Module module.Mixed_6d.branch7x7_2.conv
2021-11-20 13:22:39,626 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,626 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,629 - Module module.Mixed_6d.branch7x7_3
2021-11-20 13:22:39,629 - 	Skipping
2021-11-20 13:22:39,630 - Module module.Mixed_6d.branch7x7_3.conv
2021-11-20 13:22:39,630 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,630 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,633 - Module module.Mixed_6d.branch7x7dbl_1
2021-11-20 13:22:39,633 - 	Skipping
2021-11-20 13:22:39,634 - Module module.Mixed_6d.branch7x7dbl_1.conv
2021-11-20 13:22:39,634 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,634 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,637 - Module module.Mixed_6d.branch7x7dbl_2
2021-11-20 13:22:39,637 - 	Skipping
2021-11-20 13:22:39,638 - Module module.Mixed_6d.branch7x7dbl_2.conv
2021-11-20 13:22:39,638 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,638 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,641 - Module module.Mixed_6d.branch7x7dbl_3
2021-11-20 13:22:39,641 - 	Skipping
2021-11-20 13:22:39,642 - Module module.Mixed_6d.branch7x7dbl_3.conv
2021-11-20 13:22:39,642 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,642 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,645 - Module module.Mixed_6d.branch7x7dbl_4
2021-11-20 13:22:39,645 - 	Skipping
2021-11-20 13:22:39,647 - Module module.Mixed_6d.branch7x7dbl_4.conv
2021-11-20 13:22:39,647 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,647 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,649 - Module module.Mixed_6d.branch7x7dbl_5
2021-11-20 13:22:39,649 - 	Skipping
2021-11-20 13:22:39,651 - Module module.Mixed_6d.branch7x7dbl_5.conv
2021-11-20 13:22:39,651 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,651 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,654 - Module module.Mixed_6d.branch_pool
2021-11-20 13:22:39,654 - 	Skipping
2021-11-20 13:22:39,655 - Module module.Mixed_6d.branch_pool.conv
2021-11-20 13:22:39,655 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,655 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,658 - Module module.Mixed_6e
2021-11-20 13:22:39,658 - 	Skipping
2021-11-20 13:22:39,658 - Module module.Mixed_6e.branch1x1
2021-11-20 13:22:39,658 - 	Skipping
2021-11-20 13:22:39,659 - Module module.Mixed_6e.branch1x1.conv
2021-11-20 13:22:39,659 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,659 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,662 - Module module.Mixed_6e.branch7x7_1
2021-11-20 13:22:39,662 - 	Skipping
2021-11-20 13:22:39,663 - Module module.Mixed_6e.branch7x7_1.conv
2021-11-20 13:22:39,663 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,663 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,666 - Module module.Mixed_6e.branch7x7_2
2021-11-20 13:22:39,666 - 	Skipping
2021-11-20 13:22:39,667 - Module module.Mixed_6e.branch7x7_2.conv
2021-11-20 13:22:39,668 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,668 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,670 - Module module.Mixed_6e.branch7x7_3
2021-11-20 13:22:39,670 - 	Skipping
2021-11-20 13:22:39,672 - Module module.Mixed_6e.branch7x7_3.conv
2021-11-20 13:22:39,672 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,672 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,674 - Module module.Mixed_6e.branch7x7dbl_1
2021-11-20 13:22:39,674 - 	Skipping
2021-11-20 13:22:39,676 - Module module.Mixed_6e.branch7x7dbl_1.conv
2021-11-20 13:22:39,676 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,676 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,678 - Module module.Mixed_6e.branch7x7dbl_2
2021-11-20 13:22:39,679 - 	Skipping
2021-11-20 13:22:39,680 - Module module.Mixed_6e.branch7x7dbl_2.conv
2021-11-20 13:22:39,680 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,680 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,683 - Module module.Mixed_6e.branch7x7dbl_3
2021-11-20 13:22:39,683 - 	Skipping
2021-11-20 13:22:39,684 - Module module.Mixed_6e.branch7x7dbl_3.conv
2021-11-20 13:22:39,684 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,684 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,687 - Module module.Mixed_6e.branch7x7dbl_4
2021-11-20 13:22:39,687 - 	Skipping
2021-11-20 13:22:39,688 - Module module.Mixed_6e.branch7x7dbl_4.conv
2021-11-20 13:22:39,688 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,688 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,691 - Module module.Mixed_6e.branch7x7dbl_5
2021-11-20 13:22:39,691 - 	Skipping
2021-11-20 13:22:39,692 - Module module.Mixed_6e.branch7x7dbl_5.conv
2021-11-20 13:22:39,692 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,693 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,695 - Module module.Mixed_6e.branch_pool
2021-11-20 13:22:39,695 - 	Skipping
2021-11-20 13:22:39,697 - Module module.Mixed_6e.branch_pool.conv
2021-11-20 13:22:39,697 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,697 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,700 - Module module.AuxLogits
2021-11-20 13:22:39,700 - 	Skipping
2021-11-20 13:22:39,700 - Module module.AuxLogits.conv0
2021-11-20 13:22:39,700 - 	Skipping
2021-11-20 13:22:39,701 - Module module.AuxLogits.conv0.conv
2021-11-20 13:22:39,701 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,701 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,704 - Module module.AuxLogits.conv0.bn
2021-11-20 13:22:39,704 - 	Replacing: torch.nn.modules.batchnorm.BatchNorm2d
2021-11-20 13:22:39,704 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:22:39,707 - Module module.AuxLogits.conv1
2021-11-20 13:22:39,707 - 	Skipping
2021-11-20 13:22:39,708 - Module module.AuxLogits.conv1.conv
2021-11-20 13:22:39,708 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,708 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,711 - Module module.AuxLogits.conv1.bn
2021-11-20 13:22:39,711 - 	Replacing: torch.nn.modules.batchnorm.BatchNorm2d
2021-11-20 13:22:39,711 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-20 13:22:39,741 - Module module.AuxLogits.fc
2021-11-20 13:22:39,741 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-20 13:22:39,741 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,748 - Module module.Mixed_7a
2021-11-20 13:22:39,748 - 	Skipping
2021-11-20 13:22:39,749 - Module module.Mixed_7a.branch3x3_1
2021-11-20 13:22:39,749 - 	Skipping
2021-11-20 13:22:39,752 - Module module.Mixed_7a.branch3x3_1.conv
2021-11-20 13:22:39,752 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,752 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,756 - Module module.Mixed_7a.branch3x3_2
2021-11-20 13:22:39,756 - 	Skipping
2021-11-20 13:22:39,758 - Module module.Mixed_7a.branch3x3_2.conv
2021-11-20 13:22:39,758 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,759 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,763 - Module module.Mixed_7a.branch7x7x3_1
2021-11-20 13:22:39,763 - 	Skipping
2021-11-20 13:22:39,764 - Module module.Mixed_7a.branch7x7x3_1.conv
2021-11-20 13:22:39,764 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,764 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,767 - Module module.Mixed_7a.branch7x7x3_2
2021-11-20 13:22:39,767 - 	Skipping
2021-11-20 13:22:39,769 - Module module.Mixed_7a.branch7x7x3_2.conv
2021-11-20 13:22:39,769 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,769 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,772 - Module module.Mixed_7a.branch7x7x3_3
2021-11-20 13:22:39,772 - 	Skipping
2021-11-20 13:22:39,773 - Module module.Mixed_7a.branch7x7x3_3.conv
2021-11-20 13:22:39,773 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,773 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,776 - Module module.Mixed_7a.branch7x7x3_4
2021-11-20 13:22:39,776 - 	Skipping
2021-11-20 13:22:39,777 - Module module.Mixed_7a.branch7x7x3_4.conv
2021-11-20 13:22:39,778 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,778 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,781 - Module module.Mixed_7b
2021-11-20 13:22:39,781 - 	Skipping
2021-11-20 13:22:39,781 - Module module.Mixed_7b.branch1x1
2021-11-20 13:22:39,781 - 	Skipping
2021-11-20 13:22:39,782 - Module module.Mixed_7b.branch1x1.conv
2021-11-20 13:22:39,782 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,782 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,785 - Module module.Mixed_7b.branch3x3_1
2021-11-20 13:22:39,785 - 	Skipping
2021-11-20 13:22:39,786 - Module module.Mixed_7b.branch3x3_1.conv
2021-11-20 13:22:39,787 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,787 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,790 - Module module.Mixed_7b.branch3x3_2a
2021-11-20 13:22:39,790 - 	Skipping
2021-11-20 13:22:39,791 - Module module.Mixed_7b.branch3x3_2a.conv
2021-11-20 13:22:39,791 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,791 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,794 - Module module.Mixed_7b.branch3x3_2b
2021-11-20 13:22:39,794 - 	Skipping
2021-11-20 13:22:39,795 - Module module.Mixed_7b.branch3x3_2b.conv
2021-11-20 13:22:39,795 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,795 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,798 - Module module.Mixed_7b.branch3x3dbl_1
2021-11-20 13:22:39,798 - 	Skipping
2021-11-20 13:22:39,799 - Module module.Mixed_7b.branch3x3dbl_1.conv
2021-11-20 13:22:39,799 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,799 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,802 - Module module.Mixed_7b.branch3x3dbl_2
2021-11-20 13:22:39,802 - 	Skipping
2021-11-20 13:22:39,804 - Module module.Mixed_7b.branch3x3dbl_2.conv
2021-11-20 13:22:39,804 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,804 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,807 - Module module.Mixed_7b.branch3x3dbl_3a
2021-11-20 13:22:39,807 - 	Skipping
2021-11-20 13:22:39,808 - Module module.Mixed_7b.branch3x3dbl_3a.conv
2021-11-20 13:22:39,808 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,808 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,811 - Module module.Mixed_7b.branch3x3dbl_3b
2021-11-20 13:22:39,811 - 	Skipping
2021-11-20 13:22:39,812 - Module module.Mixed_7b.branch3x3dbl_3b.conv
2021-11-20 13:22:39,812 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,812 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,815 - Module module.Mixed_7b.branch_pool
2021-11-20 13:22:39,815 - 	Skipping
2021-11-20 13:22:39,817 - Module module.Mixed_7b.branch_pool.conv
2021-11-20 13:22:39,817 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,817 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,820 - Module module.Mixed_7c
2021-11-20 13:22:39,820 - 	Skipping
2021-11-20 13:22:39,820 - Module module.Mixed_7c.branch1x1
2021-11-20 13:22:39,820 - 	Skipping
2021-11-20 13:22:39,821 - Module module.Mixed_7c.branch1x1.conv
2021-11-20 13:22:39,821 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,821 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,824 - Module module.Mixed_7c.branch3x3_1
2021-11-20 13:22:39,824 - 	Skipping
2021-11-20 13:22:39,826 - Module module.Mixed_7c.branch3x3_1.conv
2021-11-20 13:22:39,826 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,826 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,829 - Module module.Mixed_7c.branch3x3_2a
2021-11-20 13:22:39,829 - 	Skipping
2021-11-20 13:22:39,830 - Module module.Mixed_7c.branch3x3_2a.conv
2021-11-20 13:22:39,830 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,830 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,833 - Module module.Mixed_7c.branch3x3_2b
2021-11-20 13:22:39,834 - 	Skipping
2021-11-20 13:22:39,835 - Module module.Mixed_7c.branch3x3_2b.conv
2021-11-20 13:22:39,835 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,835 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,838 - Module module.Mixed_7c.branch3x3dbl_1
2021-11-20 13:22:39,839 - 	Skipping
2021-11-20 13:22:39,840 - Module module.Mixed_7c.branch3x3dbl_1.conv
2021-11-20 13:22:39,840 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,840 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,843 - Module module.Mixed_7c.branch3x3dbl_2
2021-11-20 13:22:39,843 - 	Skipping
2021-11-20 13:22:39,844 - Module module.Mixed_7c.branch3x3dbl_2.conv
2021-11-20 13:22:39,844 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,845 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,848 - Module module.Mixed_7c.branch3x3dbl_3a
2021-11-20 13:22:39,848 - 	Skipping
2021-11-20 13:22:39,849 - Module module.Mixed_7c.branch3x3dbl_3a.conv
2021-11-20 13:22:39,849 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,849 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,852 - Module module.Mixed_7c.branch3x3dbl_3b
2021-11-20 13:22:39,852 - 	Skipping
2021-11-20 13:22:39,854 - Module module.Mixed_7c.branch3x3dbl_3b.conv
2021-11-20 13:22:39,854 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,854 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,857 - Module module.Mixed_7c.branch_pool
2021-11-20 13:22:39,857 - 	Skipping
2021-11-20 13:22:39,858 - Module module.Mixed_7c.branch_pool.conv
2021-11-20 13:22:39,858 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-20 13:22:39,858 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,863 - Module module.fc
2021-11-20 13:22:39,863 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-20 13:22:39,863 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_1a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_1a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_2a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_2a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_2b_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_2b_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_3b_1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_3b_1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_4a_3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,868 - Parameter 'module.Conv2d_4a_3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,869 - Parameter 'module.Mixed_5c.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch5x5_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch5x5_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch5x5_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch5x5_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,870 - Parameter 'module.Mixed_5d.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_5d.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_5d.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_5d.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_5d.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6a.branch3x3dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,871 - Parameter 'module.Mixed_6b.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,872 - Parameter 'module.Mixed_6c.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,873 - Parameter 'module.Mixed_6d.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6d.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch7x7_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch7x7_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch7x7_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch7x7_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,874 - Parameter 'module.Mixed_6e.branch7x7_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_5.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch7x7dbl_5.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_6e.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.AuxLogits.conv0.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.AuxLogits.conv1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.AuxLogits.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.AuxLogits.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_7a.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,875 - Parameter 'module.Mixed_7a.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch3x3_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch3x3_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_3.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_3.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_4.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7a.branch7x7x3_4.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_2a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_2a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_2b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3_2b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,876 - Parameter 'module.Mixed_7b.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_3a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_3a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_3b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch3x3dbl_3b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7b.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch1x1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch1x1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_2a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_2a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_2b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3_2b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3dbl_1.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3dbl_1.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3dbl_2.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3dbl_2.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,877 - Parameter 'module.Mixed_7c.branch3x3dbl_3a.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,878 - Parameter 'module.Mixed_7c.branch3x3dbl_3a.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,878 - Parameter 'module.Mixed_7c.branch3x3dbl_3b.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,878 - Parameter 'module.Mixed_7c.branch3x3dbl_3b.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,878 - Parameter 'module.Mixed_7c.branch_pool.conv.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,878 - Parameter 'module.Mixed_7c.branch_pool.conv.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,878 - Parameter 'module.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-20 13:22:39,878 - Parameter 'module.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-20 13:22:39,907 - Quantized model:

DataParallel(
  (module): Inception3(
    (Conv2d_1a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=25.116467, output_zero_point=0.000000
        weights_scale=57.839279, weights_zero_point=-118.000000
        (wrapped_module): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
      )
      (bn): Identity()
    )
    (Conv2d_2a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=23.814812, output_zero_point=0.000000
        weights_scale=91.453255, weights_zero_point=-76.000000
        (wrapped_module): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_2b_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=22.681568, output_zero_point=0.000000
        weights_scale=121.423393, weights_zero_point=-100.000000
        (wrapped_module): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_3b_1x1): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=26.841505, output_zero_point=0.000000
        weights_scale=113.792465, weights_zero_point=-106.000000
        (wrapped_module): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Conv2d_4a_3x3): BasicConv2d(
      (conv): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=True
          output_scale=26.944969, output_zero_point=0.000000
        weights_scale=173.668213, weights_zero_point=-127.000000
        (wrapped_module): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1))
      )
      (bn): Identity()
    )
    (Mixed_5b): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=28.497339, output_zero_point=0.000000
          weights_scale=233.696152, weights_zero_point=-130.000000
          (wrapped_module): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.181660, output_zero_point=0.000000
          weights_scale=250.440506, weights_zero_point=-129.000000
          (wrapped_module): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=29.600254, output_zero_point=0.000000
          weights_scale=210.722931, weights_zero_point=-114.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.301685, output_zero_point=0.000000
          weights_scale=183.401199, weights_zero_point=-89.000000
          (wrapped_module): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.820778, output_zero_point=0.000000
          weights_scale=173.440628, weights_zero_point=-121.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=27.333513, output_zero_point=0.000000
          weights_scale=235.430481, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=69.726189, output_zero_point=0.000000
          weights_scale=115.613274, weights_zero_point=-164.000000
          (wrapped_module): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_5c): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=28.708134, output_zero_point=0.000000
          weights_scale=185.854675, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.812336, output_zero_point=0.000000
          weights_scale=221.937332, weights_zero_point=-132.000000
          (wrapped_module): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.252514, output_zero_point=0.000000
          weights_scale=268.838531, weights_zero_point=-119.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.901993, output_zero_point=0.000000
          weights_scale=200.180267, weights_zero_point=-118.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.122124, output_zero_point=0.000000
          weights_scale=219.704071, weights_zero_point=-98.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=20.619204, output_zero_point=0.000000
          weights_scale=159.367966, weights_zero_point=-66.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.787731, output_zero_point=0.000000
          weights_scale=112.256493, weights_zero_point=-121.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_5d): InceptionA(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.940670, output_zero_point=0.000000
          weights_scale=160.903946, weights_zero_point=-173.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.751881, output_zero_point=0.000000
          weights_scale=221.842331, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch5x5_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.844185, output_zero_point=0.000000
          weights_scale=384.225189, weights_zero_point=-114.000000
          (wrapped_module): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=29.225071, output_zero_point=0.000000
          weights_scale=152.031830, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.250402, output_zero_point=0.000000
          weights_scale=214.610001, weights_zero_point=-126.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.677155, output_zero_point=0.000000
          weights_scale=283.270233, weights_zero_point=-85.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.311428, output_zero_point=0.000000
          weights_scale=112.111809, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6a): InceptionB(
      (branch3x3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.405270, output_zero_point=0.000000
          weights_scale=345.323090, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.441277, output_zero_point=0.000000
          weights_scale=194.035721, weights_zero_point=-126.000000
          (wrapped_module): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.876575, output_zero_point=0.000000
          weights_scale=252.662827, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=31.106899, output_zero_point=0.000000
          weights_scale=300.238403, weights_zero_point=-86.000000
          (wrapped_module): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
    )
    (Mixed_6b): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.518055, output_zero_point=0.000000
          weights_scale=212.494919, weights_zero_point=-109.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.214058, output_zero_point=0.000000
          weights_scale=224.252426, weights_zero_point=-80.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.101196, output_zero_point=0.000000
          weights_scale=182.452042, weights_zero_point=-80.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.293385, output_zero_point=0.000000
          weights_scale=196.628983, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=42.428837, output_zero_point=0.000000
          weights_scale=241.909012, weights_zero_point=-145.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=44.480652, output_zero_point=0.000000
          weights_scale=272.477875, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=45.641865, output_zero_point=0.000000
          weights_scale=224.714310, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.535149, output_zero_point=0.000000
          weights_scale=229.181229, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.826805, output_zero_point=0.000000
          weights_scale=299.566376, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=63.736088, output_zero_point=0.000000
          weights_scale=101.813553, weights_zero_point=-147.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6c): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.772423, output_zero_point=0.000000
          weights_scale=205.792435, weights_zero_point=-115.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.632809, output_zero_point=0.000000
          weights_scale=146.691681, weights_zero_point=-141.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.447899, output_zero_point=0.000000
          weights_scale=234.311081, weights_zero_point=-110.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=44.332878, output_zero_point=0.000000
          weights_scale=200.441696, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=39.053520, output_zero_point=0.000000
          weights_scale=169.996704, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.551636, output_zero_point=0.000000
          weights_scale=292.734314, weights_zero_point=-103.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.712090, output_zero_point=0.000000
          weights_scale=218.001434, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.522635, output_zero_point=0.000000
          weights_scale=249.194092, weights_zero_point=-96.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=30.347155, output_zero_point=0.000000
          weights_scale=242.304962, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=62.187904, output_zero_point=0.000000
          weights_scale=137.780121, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6d): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.670513, output_zero_point=0.000000
          weights_scale=130.717911, weights_zero_point=-154.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.600883, output_zero_point=0.000000
          weights_scale=230.687149, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.148865, output_zero_point=0.000000
          weights_scale=156.647110, weights_zero_point=-78.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.157887, output_zero_point=0.000000
          weights_scale=228.079788, weights_zero_point=-103.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.197197, output_zero_point=0.000000
          weights_scale=168.429398, weights_zero_point=-112.000000
          (wrapped_module): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.426975, output_zero_point=0.000000
          weights_scale=171.427673, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.088593, output_zero_point=0.000000
          weights_scale=235.805008, weights_zero_point=-107.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.603867, output_zero_point=0.000000
          weights_scale=155.897659, weights_zero_point=-149.000000
          (wrapped_module): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=40.205288, output_zero_point=0.000000
          weights_scale=164.953217, weights_zero_point=-157.000000
          (wrapped_module): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=65.319580, output_zero_point=0.000000
          weights_scale=81.718330, weights_zero_point=-146.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_6e): InceptionC(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=50.753494, output_zero_point=0.000000
          weights_scale=134.851929, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.828094, output_zero_point=0.000000
          weights_scale=161.791412, weights_zero_point=-95.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=32.372322, output_zero_point=0.000000
          weights_scale=169.476089, weights_zero_point=-71.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=48.569138, output_zero_point=0.000000
          weights_scale=391.694305, weights_zero_point=-122.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.996212, output_zero_point=0.000000
          weights_scale=214.595215, weights_zero_point=-131.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.136314, output_zero_point=0.000000
          weights_scale=290.781158, weights_zero_point=-93.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.914577, output_zero_point=0.000000
          weights_scale=235.433029, weights_zero_point=-101.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7dbl_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=35.680706, output_zero_point=0.000000
          weights_scale=347.535034, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7dbl_5): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=58.395302, output_zero_point=0.000000
          weights_scale=437.720947, weights_zero_point=-104.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=71.163963, output_zero_point=0.000000
          weights_scale=141.906174, weights_zero_point=-125.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (AuxLogits): InceptionAux(
      (conv0): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          weights_scale=329.854614, weights_zero_point=-91.000000
          (wrapped_module): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (bn): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (conv1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          weights_scale=1550.066895, weights_zero_point=-127.000000
          (wrapped_module): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)
        )
        (bn): RangeLinearFakeQuantWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=False
          wrapped_module_float_dtype=torch.float32.
          (wrapped_module): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (fc): RangeLinearQuantParamLayerWrapper(
        weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
        scale_approx_mult_bits=None
        preset_activation_stats=False
        weights_scale=391.454559, weights_zero_point=-89.000000
        base_bias_scale=1264299008.000000, base_bias_zero_point=0.000000
        (wrapped_module): Linear(in_features=768, out_features=1000, bias=True)
      )
    )
    (Mixed_7a): InceptionD(
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=42.466160, output_zero_point=0.000000
          weights_scale=156.875381, weights_zero_point=-115.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=34.123844, output_zero_point=0.000000
          weights_scale=281.107880, weights_zero_point=-77.000000
          (wrapped_module): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
      (branch7x7x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.423519, output_zero_point=0.000000
          weights_scale=109.459068, weights_zero_point=-118.000000
          (wrapped_module): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch7x7x3_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=37.102749, output_zero_point=0.000000
          weights_scale=283.353882, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))
        )
        (bn): Identity()
      )
      (branch7x7x3_3): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=52.738613, output_zero_point=0.000000
          weights_scale=345.840942, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))
        )
        (bn): Identity()
      )
      (branch7x7x3_4): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=46.752419, output_zero_point=0.000000
          weights_scale=167.826477, weights_zero_point=-94.000000
          (wrapped_module): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
        )
        (bn): Identity()
      )
    )
    (Mixed_7b): InceptionE(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=62.377773, output_zero_point=0.000000
          weights_scale=158.751724, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=51.599426, output_zero_point=0.000000
          weights_scale=225.686890, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=57.921940, output_zero_point=0.000000
          weights_scale=256.622345, weights_zero_point=-88.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=54.381519, output_zero_point=0.000000
          weights_scale=156.567505, weights_zero_point=-92.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=50.835201, output_zero_point=0.000000
          weights_scale=220.083130, weights_zero_point=-94.000000
          (wrapped_module): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=53.787712, output_zero_point=0.000000
          weights_scale=475.481689, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.791264, output_zero_point=0.000000
          weights_scale=204.687500, weights_zero_point=-117.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=60.639427, output_zero_point=0.000000
          weights_scale=331.727753, weights_zero_point=-102.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=105.061790, output_zero_point=0.000000
          weights_scale=129.995590, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (Mixed_7c): InceptionE(
      (branch1x1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.803699, output_zero_point=0.000000
          weights_scale=69.987938, weights_zero_point=-85.000000
          (wrapped_module): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=57.517414, output_zero_point=0.000000
          weights_scale=131.107544, weights_zero_point=-113.000000
          (wrapped_module): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=21.480377, output_zero_point=0.000000
          weights_scale=108.998085, weights_zero_point=-43.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3_2b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=24.145729, output_zero_point=0.000000
          weights_scale=101.049492, weights_zero_point=-47.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch3x3dbl_1): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=46.318707, output_zero_point=0.000000
          weights_scale=164.204681, weights_zero_point=-132.000000
          (wrapped_module): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_2): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.267349, output_zero_point=0.000000
          weights_scale=274.946320, weights_zero_point=-78.000000
          (wrapped_module): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3a): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=38.149796, output_zero_point=0.000000
          weights_scale=159.698288, weights_zero_point=-109.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        )
        (bn): Identity()
      )
      (branch3x3dbl_3b): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=41.410839, output_zero_point=0.000000
          weights_scale=183.429901, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))
        )
        (bn): Identity()
      )
      (branch_pool): BasicConv2d(
        (conv): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=53.418743, output_zero_point=0.000000
          weights_scale=47.231201, weights_zero_point=-135.000000
          (wrapped_module): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn): Identity()
      )
    )
    (fc): RangeLinearQuantParamLayerWrapper(
      weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=21.503002, output_zero_point=-51.000000
      weights_scale=624.406128, weights_zero_point=-109.000000
      (wrapped_module): Linear(in_features=2048, out_features=1000, bias=True)
    )
  )
)

2021-11-20 13:22:40,346 - Per-layer quantization parameters saved to logs/Inception_v3-imagenet-helmet___2021.11.20-132224/layer_quant_params.yaml
2021-11-20 13:22:47,832 - --- test ---------------------
2021-11-20 13:22:47,832 - 50000 samples (256 per mini-batch)
2021-11-20 13:23:13,155 - Test: [   10/  195]    Loss 6.907756    Top1 0.117188    Top5 0.468750    
2021-11-20 13:23:25,272 - Test: [   20/  195]    Loss 6.907756    Top1 0.058594    Top5 0.390625    
2021-11-20 13:23:37,571 - Test: [   30/  195]    Loss 6.907756    Top1 0.065104    Top5 0.429687    
2021-11-20 13:23:49,874 - Test: [   40/  195]    Loss 6.907756    Top1 0.068359    Top5 0.458984    
2021-11-20 13:24:02,591 - Test: [   50/  195]    Loss 6.907756    Top1 0.054687    Top5 0.406250    
2021-11-20 13:24:15,195 - Test: [   60/  195]    Loss 6.907756    Top1 0.052083    Top5 0.384115    
2021-11-20 13:24:27,539 - Test: [   70/  195]    Loss 6.907756    Top1 0.055804    Top5 0.418527    
2021-11-20 13:24:39,815 - Test: [   80/  195]    Loss 6.907756    Top1 0.068359    Top5 0.458984    
2021-11-20 13:24:52,103 - Test: [   90/  195]    Loss 6.907756    Top1 0.078125    Top5 0.473090    
2021-11-20 13:25:04,640 - Test: [  100/  195]    Loss 6.907756    Top1 0.082031    Top5 0.468750    
2021-11-20 13:25:17,298 - Test: [  110/  195]    Loss 6.907756    Top1 0.088778    Top5 0.475852    
2021-11-20 13:25:29,595 - Test: [  120/  195]    Loss 6.907756    Top1 0.087891    Top5 0.478516    
2021-11-20 13:25:42,160 - Test: [  130/  195]    Loss 6.907756    Top1 0.090144    Top5 0.486779    
2021-11-20 13:25:54,798 - Test: [  140/  195]    Loss 6.907756    Top1 0.094866    Top5 0.493862    
2021-11-20 13:26:07,099 - Test: [  150/  195]    Loss 6.907756    Top1 0.098958    Top5 0.497396    
2021-11-20 13:26:19,498 - Test: [  160/  195]    Loss 6.907756    Top1 0.097656    Top5 0.505371    
2021-11-20 13:26:31,923 - Test: [  170/  195]    Loss 6.907756    Top1 0.096507    Top5 0.494026    
2021-11-20 13:26:44,227 - Test: [  180/  195]    Loss 6.907756    Top1 0.099826    Top5 0.501302    
2021-11-20 13:26:56,499 - Test: [  190/  195]    Loss 6.907756    Top1 0.096628    Top5 0.495477    
2021-11-20 13:27:05,645 - ==> Top1: 0.100    Top5: 0.500    Loss: 6.908

2021-11-20 13:27:05,742 - 
2021-11-20 13:27:05,742 - Log file for this run: /home/th.nguyen/drift-encode/logs/Inception_v3-imagenet-helmet___2021.11.20-132224/Inception_v3-imagenet-helmet___2021.11.20-132224.log
