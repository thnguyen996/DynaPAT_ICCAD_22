2021-11-21 10:23:49,354 - Log file for this run: /home/th.nguyen/drift-encode/logs/resnet50-imagenet-flipcy___2021.11.21-102349/resnet50-imagenet-flipcy___2021.11.21-102349.log
2021-11-21 10:23:49,354 - Number of CPUs: 56
2021-11-21 10:23:50,027 - Number of GPUs: 6
2021-11-21 10:23:50,028 - CUDA version: 10.0.130
2021-11-21 10:23:50,029 - CUDNN version: 7603
2021-11-21 10:23:50,029 - Kernel: 5.4.0-90-generic
2021-11-21 10:23:50,029 - Python: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) 
[GCC 7.5.0]
2021-11-21 10:23:50,030 - pip freeze: {'absl-py': '0.12.0', 'aiohttp': '3.7.4.post0', 'argon2-cffi': '21.1.0', 'astor': '0.8.1', 'astunparse': '1.6.3', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'backcall': '0.2.0', 'black': '21.9b0', 'bleach': '4.1.0', 'blessed': '1.19.0', 'blessings': '1.7', 'blinker': '1.4', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2020.12.5', 'cffi': '1.14.6', 'chardet': '4.0.0', 'charset-normalizer': '2.0.4', 'click': '8.0.1', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'dataclasses': '0.8', 'decorator': '5.1.0', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'gast': '0.2.2', 'gitdb': '4.0.9', 'gitdb2': '3.0.3.post1', 'gitpython': '3.1.0', 'google-auth': '1.30.1', 'google-auth-oauthlib': '0.4.4', 'google-pasta': '0.2.0', 'gpustat': '1.0.0.dev1', 'graphviz': '0.10.1', 'grpcio': '1.38.0', 'gym': '0.12.5', 'h5py': '2.10.0', 'idna': '2.10', 'idna-ssl': '1.1.0', 'importlib-metadata': '4.8.1', 'ipykernel': '5.5.6', 'ipython': '7.16.1', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.17.2', 'jinja2': '3.0.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '7.0.6', 'jupyter-console': '6.4.0', 'jupyter-core': '4.8.1', 'jupyterlab-pygments': '0.1.2', 'keras-applications': '1.0.8', 'keras-preprocessing': '1.1.2', 'kiwisolver': '1.3.1', 'markdown': '3.3.4', 'markupsafe': '2.0.1', 'matplotlib': '3.3.4', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.10.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'mypy-extensions': '0.4.3', 'nbclient': '0.5.4', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'notebook': '6.4.4', 'numpy': '1.18.5', 'nvidia-ml-py3': '7.352.0', 'oauthlib': '3.1.0', 'olefile': '0.46', 'opt-einsum': '3.3.0', 'packaging': '21.0', 'pandas': '1.1.5', 'pandocfilters': '1.5.0', 'parse': '1.19.0', 'parso': '0.7.1', 'pathspec': '0.9.0', 'pexpect': '4.8.0', 'pickle5': '0.0.11', 'pickleshare': '0.7.5', 'pillow': '6.2.2', 'pip': '21.2.2', 'platformdirs': '2.4.0', 'pluggy': '0.13.1', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.11.0', 'prompt-toolkit': '3.0.20', 'protobuf': '3.17.1', 'psutil': '5.8.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pyglet': '1.5.21', 'pygments': '2.10.0', 'pyhocon': '0.3.58', 'pyjwt': '2.1.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyrsistent': '0.18.0', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'python-jsonrpc-server': '0.4.0', 'python-language-server': '0.36.2', 'pytz': '2021.3', 'pyyaml': '6.0', 'pyzmq': '22.3.0', 'qgrid': '1.1.1', 'qtconsole': '5.1.1', 'qtpy': '1.11.2', 'ranger-fm': '1.9.3', 'regex': '2021.10.21', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'rsa': '4.7.2', 'scikit-learn': '0.21.2', 'scipy': '1.5.4', 'seaborn': '0.11.2', 'send2trash': '1.8.0', 'setuptools': '57.0.0', 'six': '1.16.0', 'smmap': '4.0.0', 'smmap2': '2.0.5', 'tabulate': '0.8.3', 'tensorboard': '1.15.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.0', 'tensorflow': '1.15.0', 'tensorflow-estimator': '1.15.1', 'termcolor': '1.1.0', 'terminado': '0.12.1', 'testpath': '0.5.0', 'timm': '0.4.9', 'tomli': '1.2.1', 'torch': '1.3.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchsummary': '1.5.1', 'torchvision': '0.4.2', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '4.3.3', 'traittypes': '0.2.1', 'typed-ast': '1.4.3', 'typing-extensions': '3.10.0.1', 'ujson': '4.1.0', 'urllib3': '1.26.4', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.2.1', 'werkzeug': '2.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '3.0.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2021-11-21 10:23:50,142 - Git is dirty
2021-11-21 10:23:50,142 - Active Git branch: master
2021-11-21 10:23:50,191 - Git commit: f401db9403c1608e10258733e013b6c087e3ef34
2021-11-21 10:23:50,192 - Command line: compress_classifier.py --arch resnet50 -p 10 -j 22 /home/imagenet/ --pretrained --run --qe-config-file ./conf/resnet50_conf.yaml --gpus 3 --name resnet50-imagenet-flipcy --method flipcy --mlc 8 --num_bits 8 --encode
2021-11-21 10:23:50,192 - Distiller: 0.4.0rc0
2021-11-21 10:23:50,194 - Random seed: 46501
2021-11-21 10:23:50,902 - => created a pretrained resnet50 model with the imagenet dataset
2021-11-21 10:23:53,924 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2021-11-21 10:23:53,926 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2021-11-21 10:23:54,111 - Dataset sizes:
	test=50000
2021-11-21 10:23:54,121 - Reading configuration from: ./conf/resnet50_conf.yaml
2021-11-21 10:23:54,128 - Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers
2021-11-21 10:23:54,130 - Loading activation stats from: ./quant_stats/resnet50.yaml
2021-11-21 10:23:54,603 - Preparing model for quantization using PostTrainLinearQuantizer
2021-11-21 10:23:57,966 - Applying batch-norm folding ahead of post-training quantization
2021-11-21 10:23:57,968 - Fusing sequence ['module.conv1', 'module.bn1']
2021-11-21 10:23:57,970 - Fusing sequence ['module.layer1.0.conv1', 'module.layer1.0.bn1']
2021-11-21 10:23:57,970 - Fusing sequence ['module.layer1.0.conv2', 'module.layer1.0.bn2']
2021-11-21 10:23:57,971 - Fusing sequence ['module.layer1.0.conv3', 'module.layer1.0.bn3']
2021-11-21 10:23:57,971 - Fusing sequence ['module.layer1.0.downsample.0', 'module.layer1.0.downsample.1']
2021-11-21 10:23:57,972 - Fusing sequence ['module.layer1.1.conv1', 'module.layer1.1.bn1']
2021-11-21 10:23:57,972 - Fusing sequence ['module.layer1.1.conv2', 'module.layer1.1.bn2']
2021-11-21 10:23:57,972 - Fusing sequence ['module.layer1.1.conv3', 'module.layer1.1.bn3']
2021-11-21 10:23:57,973 - Fusing sequence ['module.layer1.2.conv1', 'module.layer1.2.bn1']
2021-11-21 10:23:57,973 - Fusing sequence ['module.layer1.2.conv2', 'module.layer1.2.bn2']
2021-11-21 10:23:57,973 - Fusing sequence ['module.layer1.2.conv3', 'module.layer1.2.bn3']
2021-11-21 10:23:57,974 - Fusing sequence ['module.layer2.0.conv1', 'module.layer2.0.bn1']
2021-11-21 10:23:57,974 - Fusing sequence ['module.layer2.0.conv2', 'module.layer2.0.bn2']
2021-11-21 10:23:57,974 - Fusing sequence ['module.layer2.0.conv3', 'module.layer2.0.bn3']
2021-11-21 10:23:57,975 - Fusing sequence ['module.layer2.0.downsample.0', 'module.layer2.0.downsample.1']
2021-11-21 10:23:57,975 - Fusing sequence ['module.layer2.1.conv1', 'module.layer2.1.bn1']
2021-11-21 10:23:57,975 - Fusing sequence ['module.layer2.1.conv2', 'module.layer2.1.bn2']
2021-11-21 10:23:57,976 - Fusing sequence ['module.layer2.1.conv3', 'module.layer2.1.bn3']
2021-11-21 10:23:57,976 - Fusing sequence ['module.layer2.2.conv1', 'module.layer2.2.bn1']
2021-11-21 10:23:57,976 - Fusing sequence ['module.layer2.2.conv2', 'module.layer2.2.bn2']
2021-11-21 10:23:57,976 - Fusing sequence ['module.layer2.2.conv3', 'module.layer2.2.bn3']
2021-11-21 10:23:57,977 - Fusing sequence ['module.layer2.3.conv1', 'module.layer2.3.bn1']
2021-11-21 10:23:57,977 - Fusing sequence ['module.layer2.3.conv2', 'module.layer2.3.bn2']
2021-11-21 10:23:57,977 - Fusing sequence ['module.layer2.3.conv3', 'module.layer2.3.bn3']
2021-11-21 10:23:57,978 - Fusing sequence ['module.layer3.0.conv1', 'module.layer3.0.bn1']
2021-11-21 10:23:57,978 - Fusing sequence ['module.layer3.0.conv2', 'module.layer3.0.bn2']
2021-11-21 10:23:57,978 - Fusing sequence ['module.layer3.0.conv3', 'module.layer3.0.bn3']
2021-11-21 10:23:57,979 - Fusing sequence ['module.layer3.0.downsample.0', 'module.layer3.0.downsample.1']
2021-11-21 10:23:57,979 - Fusing sequence ['module.layer3.1.conv1', 'module.layer3.1.bn1']
2021-11-21 10:23:57,979 - Fusing sequence ['module.layer3.1.conv2', 'module.layer3.1.bn2']
2021-11-21 10:23:57,980 - Fusing sequence ['module.layer3.1.conv3', 'module.layer3.1.bn3']
2021-11-21 10:23:57,980 - Fusing sequence ['module.layer3.2.conv1', 'module.layer3.2.bn1']
2021-11-21 10:23:57,982 - Fusing sequence ['module.layer3.2.conv2', 'module.layer3.2.bn2']
2021-11-21 10:23:57,982 - Fusing sequence ['module.layer3.2.conv3', 'module.layer3.2.bn3']
2021-11-21 10:23:57,983 - Fusing sequence ['module.layer3.3.conv1', 'module.layer3.3.bn1']
2021-11-21 10:23:57,983 - Fusing sequence ['module.layer3.3.conv2', 'module.layer3.3.bn2']
2021-11-21 10:23:57,983 - Fusing sequence ['module.layer3.3.conv3', 'module.layer3.3.bn3']
2021-11-21 10:23:57,984 - Fusing sequence ['module.layer3.4.conv1', 'module.layer3.4.bn1']
2021-11-21 10:23:57,984 - Fusing sequence ['module.layer3.4.conv2', 'module.layer3.4.bn2']
2021-11-21 10:23:57,984 - Fusing sequence ['module.layer3.4.conv3', 'module.layer3.4.bn3']
2021-11-21 10:23:57,985 - Fusing sequence ['module.layer3.5.conv1', 'module.layer3.5.bn1']
2021-11-21 10:23:57,985 - Fusing sequence ['module.layer3.5.conv2', 'module.layer3.5.bn2']
2021-11-21 10:23:57,985 - Fusing sequence ['module.layer3.5.conv3', 'module.layer3.5.bn3']
2021-11-21 10:23:57,986 - Fusing sequence ['module.layer4.0.conv1', 'module.layer4.0.bn1']
2021-11-21 10:23:57,986 - Fusing sequence ['module.layer4.0.conv2', 'module.layer4.0.bn2']
2021-11-21 10:23:57,986 - Fusing sequence ['module.layer4.0.conv3', 'module.layer4.0.bn3']
2021-11-21 10:23:57,987 - Fusing sequence ['module.layer4.0.downsample.0', 'module.layer4.0.downsample.1']
2021-11-21 10:23:57,987 - Fusing sequence ['module.layer4.1.conv1', 'module.layer4.1.bn1']
2021-11-21 10:23:57,987 - Fusing sequence ['module.layer4.1.conv2', 'module.layer4.1.bn2']
2021-11-21 10:23:57,987 - Fusing sequence ['module.layer4.1.conv3', 'module.layer4.1.bn3']
2021-11-21 10:23:57,988 - Fusing sequence ['module.layer4.2.conv1', 'module.layer4.2.bn1']
2021-11-21 10:23:57,988 - Fusing sequence ['module.layer4.2.conv2', 'module.layer4.2.bn2']
2021-11-21 10:23:57,988 - Fusing sequence ['module.layer4.2.conv3', 'module.layer4.2.bn3']
2021-11-21 10:23:58,837 - Propagating output statistics from BN modules to folded modules
2021-11-21 10:23:58,838 -   bn1 --> module.conv1
2021-11-21 10:23:58,849 -   layer1.0.bn1 --> module.layer1.0.conv1
2021-11-21 10:23:58,849 -   layer1.0.bn2 --> module.layer1.0.conv2
2021-11-21 10:23:58,849 -   layer1.0.bn3 --> module.layer1.0.conv3
2021-11-21 10:23:58,849 -   layer1.0.downsample.1 --> module.layer1.0.downsample.0
2021-11-21 10:23:58,849 -   layer1.1.bn1 --> module.layer1.1.conv1
2021-11-21 10:23:58,849 -   layer1.1.bn2 --> module.layer1.1.conv2
2021-11-21 10:23:58,849 -   layer1.1.bn3 --> module.layer1.1.conv3
2021-11-21 10:23:58,849 -   layer1.2.bn1 --> module.layer1.2.conv1
2021-11-21 10:23:58,849 -   layer1.2.bn2 --> module.layer1.2.conv2
2021-11-21 10:23:58,849 -   layer1.2.bn3 --> module.layer1.2.conv3
2021-11-21 10:23:58,849 -   layer2.0.bn1 --> module.layer2.0.conv1
2021-11-21 10:23:58,850 -   layer2.0.bn2 --> module.layer2.0.conv2
2021-11-21 10:23:58,850 -   layer2.0.bn3 --> module.layer2.0.conv3
2021-11-21 10:23:58,850 -   layer2.0.downsample.1 --> module.layer2.0.downsample.0
2021-11-21 10:23:58,850 -   layer2.1.bn1 --> module.layer2.1.conv1
2021-11-21 10:23:58,850 -   layer2.1.bn2 --> module.layer2.1.conv2
2021-11-21 10:23:58,850 -   layer2.1.bn3 --> module.layer2.1.conv3
2021-11-21 10:23:58,850 -   layer2.2.bn1 --> module.layer2.2.conv1
2021-11-21 10:23:58,850 -   layer2.2.bn2 --> module.layer2.2.conv2
2021-11-21 10:23:58,850 -   layer2.2.bn3 --> module.layer2.2.conv3
2021-11-21 10:23:58,850 -   layer2.3.bn1 --> module.layer2.3.conv1
2021-11-21 10:23:58,850 -   layer2.3.bn2 --> module.layer2.3.conv2
2021-11-21 10:23:58,850 -   layer2.3.bn3 --> module.layer2.3.conv3
2021-11-21 10:23:58,850 -   layer3.0.bn1 --> module.layer3.0.conv1
2021-11-21 10:23:58,850 -   layer3.0.bn2 --> module.layer3.0.conv2
2021-11-21 10:23:58,850 -   layer3.0.bn3 --> module.layer3.0.conv3
2021-11-21 10:23:58,850 -   layer3.0.downsample.1 --> module.layer3.0.downsample.0
2021-11-21 10:23:58,850 -   layer3.1.bn1 --> module.layer3.1.conv1
2021-11-21 10:23:58,850 -   layer3.1.bn2 --> module.layer3.1.conv2
2021-11-21 10:23:58,851 -   layer3.1.bn3 --> module.layer3.1.conv3
2021-11-21 10:23:58,851 -   layer3.2.bn1 --> module.layer3.2.conv1
2021-11-21 10:23:58,851 -   layer3.2.bn2 --> module.layer3.2.conv2
2021-11-21 10:23:58,851 -   layer3.2.bn3 --> module.layer3.2.conv3
2021-11-21 10:23:58,851 -   layer3.3.bn1 --> module.layer3.3.conv1
2021-11-21 10:23:58,851 -   layer3.3.bn2 --> module.layer3.3.conv2
2021-11-21 10:23:58,851 -   layer3.3.bn3 --> module.layer3.3.conv3
2021-11-21 10:23:58,851 -   layer3.4.bn1 --> module.layer3.4.conv1
2021-11-21 10:23:58,851 -   layer3.4.bn2 --> module.layer3.4.conv2
2021-11-21 10:23:58,851 -   layer3.4.bn3 --> module.layer3.4.conv3
2021-11-21 10:23:58,851 -   layer3.5.bn1 --> module.layer3.5.conv1
2021-11-21 10:23:58,851 -   layer3.5.bn2 --> module.layer3.5.conv2
2021-11-21 10:23:58,851 -   layer3.5.bn3 --> module.layer3.5.conv3
2021-11-21 10:23:58,851 -   layer4.0.bn1 --> module.layer4.0.conv1
2021-11-21 10:23:58,851 -   layer4.0.bn2 --> module.layer4.0.conv2
2021-11-21 10:23:58,851 -   layer4.0.bn3 --> module.layer4.0.conv3
2021-11-21 10:23:58,851 -   layer4.0.downsample.1 --> module.layer4.0.downsample.0
2021-11-21 10:23:58,852 -   layer4.1.bn1 --> module.layer4.1.conv1
2021-11-21 10:23:58,852 -   layer4.1.bn2 --> module.layer4.1.conv2
2021-11-21 10:23:58,852 -   layer4.1.bn3 --> module.layer4.1.conv3
2021-11-21 10:23:58,852 -   layer4.2.bn1 --> module.layer4.2.conv1
2021-11-21 10:23:58,852 -   layer4.2.bn2 --> module.layer4.2.conv2
2021-11-21 10:23:58,852 -   layer4.2.bn3 --> module.layer4.2.conv3
2021-11-21 10:23:58,852 - Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid
2021-11-21 10:23:58,853 -   Module conv1 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.0.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.0.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.0.add followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.1.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.1.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.1.add followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.2.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,853 -   Module layer1.2.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer1.2.add followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.0.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.0.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.0.add followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.1.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.1.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.1.add followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.2.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.2.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,854 -   Module layer2.2.add followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer2.3.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer2.3.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer2.3.add followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.0.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.0.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.0.add followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.1.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.1.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.1.add followed by Relu, updating stats
2021-11-21 10:23:58,855 -   Module layer3.2.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.2.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.2.add followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.3.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.3.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.3.add followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.4.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.4.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.4.add followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.5.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,856 -   Module layer3.5.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer3.5.add followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.0.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.0.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.0.add followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.1.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.1.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.1.add followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.2.conv1 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.2.conv2 followed by Relu, updating stats
2021-11-21 10:23:58,857 -   Module layer4.2.add followed by Relu, updating stats
2021-11-21 10:23:59,033 - Updated stats saved to logs/resnet50-imagenet-flipcy___2021.11.21-102349/quant_stats_after_prepare_model.yaml
2021-11-21 10:23:59,034 - Module module
2021-11-21 10:23:59,034 - 	Skipping
2021-11-21 10:23:59,038 - Module module.conv1
2021-11-21 10:23:59,038 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,038 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,040 - Module module.relu
2021-11-21 10:23:59,040 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,040 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,042 - Module module.maxpool
2021-11-21 10:23:59,042 - 	Replacing: torch.nn.modules.pooling.MaxPool2d
2021-11-21 10:23:59,042 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-21 10:23:59,043 - Module module.layer1
2021-11-21 10:23:59,044 - 	Skipping
2021-11-21 10:23:59,044 - Module module.layer1.0
2021-11-21 10:23:59,044 - 	Skipping
2021-11-21 10:23:59,045 - Module module.layer1.0.conv1
2021-11-21 10:23:59,045 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,045 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,047 - Module module.layer1.0.relu1
2021-11-21 10:23:59,047 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,047 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,049 - Module module.layer1.0.conv2
2021-11-21 10:23:59,049 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,049 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,051 - Module module.layer1.0.relu2
2021-11-21 10:23:59,051 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,051 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,053 - Module module.layer1.0.conv3
2021-11-21 10:23:59,053 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,054 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,055 - Module module.layer1.0.downsample
2021-11-21 10:23:59,055 - 	Skipping
2021-11-21 10:23:59,056 - Module module.layer1.0.downsample.0
2021-11-21 10:23:59,056 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,056 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,058 - Module module.layer1.0.add
2021-11-21 10:23:59,058 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,058 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,060 - Module module.layer1.0.relu3
2021-11-21 10:23:59,060 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,060 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,061 - Module module.layer1.1
2021-11-21 10:23:59,061 - 	Skipping
2021-11-21 10:23:59,062 - Module module.layer1.1.conv1
2021-11-21 10:23:59,062 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,063 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,064 - Module module.layer1.1.relu1
2021-11-21 10:23:59,064 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,064 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,067 - Module module.layer1.1.conv2
2021-11-21 10:23:59,067 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,067 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,069 - Module module.layer1.1.relu2
2021-11-21 10:23:59,069 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,069 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,071 - Module module.layer1.1.conv3
2021-11-21 10:23:59,071 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,071 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,073 - Module module.layer1.1.add
2021-11-21 10:23:59,073 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,073 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,075 - Module module.layer1.1.relu3
2021-11-21 10:23:59,075 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,075 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,076 - Module module.layer1.2
2021-11-21 10:23:59,077 - 	Skipping
2021-11-21 10:23:59,078 - Module module.layer1.2.conv1
2021-11-21 10:23:59,078 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,078 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,079 - Module module.layer1.2.relu1
2021-11-21 10:23:59,080 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,080 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,082 - Module module.layer1.2.conv2
2021-11-21 10:23:59,082 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,082 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,084 - Module module.layer1.2.relu2
2021-11-21 10:23:59,084 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,084 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,086 - Module module.layer1.2.conv3
2021-11-21 10:23:59,086 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,087 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,089 - Module module.layer1.2.add
2021-11-21 10:23:59,089 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,089 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,091 - Module module.layer1.2.relu3
2021-11-21 10:23:59,091 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,091 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,092 - Module module.layer2
2021-11-21 10:23:59,092 - 	Skipping
2021-11-21 10:23:59,093 - Module module.layer2.0
2021-11-21 10:23:59,093 - 	Skipping
2021-11-21 10:23:59,094 - Module module.layer2.0.conv1
2021-11-21 10:23:59,094 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,094 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,096 - Module module.layer2.0.relu1
2021-11-21 10:23:59,096 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,096 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,098 - Module module.layer2.0.conv2
2021-11-21 10:23:59,098 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,098 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,100 - Module module.layer2.0.relu2
2021-11-21 10:23:59,100 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,100 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,103 - Module module.layer2.0.conv3
2021-11-21 10:23:59,103 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,103 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,104 - Module module.layer2.0.downsample
2021-11-21 10:23:59,105 - 	Skipping
2021-11-21 10:23:59,106 - Module module.layer2.0.downsample.0
2021-11-21 10:23:59,106 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,106 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,108 - Module module.layer2.0.add
2021-11-21 10:23:59,108 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,108 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,110 - Module module.layer2.0.relu3
2021-11-21 10:23:59,110 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,110 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,112 - Module module.layer2.1
2021-11-21 10:23:59,112 - 	Skipping
2021-11-21 10:23:59,113 - Module module.layer2.1.conv1
2021-11-21 10:23:59,113 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,113 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,115 - Module module.layer2.1.relu1
2021-11-21 10:23:59,115 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,115 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,117 - Module module.layer2.1.conv2
2021-11-21 10:23:59,118 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,118 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,120 - Module module.layer2.1.relu2
2021-11-21 10:23:59,120 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,120 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,122 - Module module.layer2.1.conv3
2021-11-21 10:23:59,122 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,122 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,125 - Module module.layer2.1.add
2021-11-21 10:23:59,125 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,125 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,127 - Module module.layer2.1.relu3
2021-11-21 10:23:59,127 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,127 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,129 - Module module.layer2.2
2021-11-21 10:23:59,129 - 	Skipping
2021-11-21 10:23:59,130 - Module module.layer2.2.conv1
2021-11-21 10:23:59,130 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,130 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,132 - Module module.layer2.2.relu1
2021-11-21 10:23:59,132 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,132 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,135 - Module module.layer2.2.conv2
2021-11-21 10:23:59,135 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,135 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,137 - Module module.layer2.2.relu2
2021-11-21 10:23:59,137 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,137 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,140 - Module module.layer2.2.conv3
2021-11-21 10:23:59,140 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,140 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,142 - Module module.layer2.2.add
2021-11-21 10:23:59,142 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,142 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,144 - Module module.layer2.2.relu3
2021-11-21 10:23:59,145 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,145 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,146 - Module module.layer2.3
2021-11-21 10:23:59,146 - 	Skipping
2021-11-21 10:23:59,147 - Module module.layer2.3.conv1
2021-11-21 10:23:59,148 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,148 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,150 - Module module.layer2.3.relu1
2021-11-21 10:23:59,150 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,150 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,153 - Module module.layer2.3.conv2
2021-11-21 10:23:59,153 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,153 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,155 - Module module.layer2.3.relu2
2021-11-21 10:23:59,155 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,155 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,158 - Module module.layer2.3.conv3
2021-11-21 10:23:59,158 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,158 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,160 - Module module.layer2.3.add
2021-11-21 10:23:59,160 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,160 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,162 - Module module.layer2.3.relu3
2021-11-21 10:23:59,162 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,162 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,164 - Module module.layer3
2021-11-21 10:23:59,164 - 	Skipping
2021-11-21 10:23:59,164 - Module module.layer3.0
2021-11-21 10:23:59,164 - 	Skipping
2021-11-21 10:23:59,165 - Module module.layer3.0.conv1
2021-11-21 10:23:59,165 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,165 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,168 - Module module.layer3.0.relu1
2021-11-21 10:23:59,168 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,168 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,171 - Module module.layer3.0.conv2
2021-11-21 10:23:59,171 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,171 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,173 - Module module.layer3.0.relu2
2021-11-21 10:23:59,173 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,173 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,176 - Module module.layer3.0.conv3
2021-11-21 10:23:59,176 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,176 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,178 - Module module.layer3.0.downsample
2021-11-21 10:23:59,178 - 	Skipping
2021-11-21 10:23:59,179 - Module module.layer3.0.downsample.0
2021-11-21 10:23:59,179 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,179 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,181 - Module module.layer3.0.add
2021-11-21 10:23:59,181 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,181 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,184 - Module module.layer3.0.relu3
2021-11-21 10:23:59,184 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,184 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,186 - Module module.layer3.1
2021-11-21 10:23:59,186 - 	Skipping
2021-11-21 10:23:59,187 - Module module.layer3.1.conv1
2021-11-21 10:23:59,188 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,188 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,190 - Module module.layer3.1.relu1
2021-11-21 10:23:59,190 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,190 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,193 - Module module.layer3.1.conv2
2021-11-21 10:23:59,193 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,193 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,195 - Module module.layer3.1.relu2
2021-11-21 10:23:59,195 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,195 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,198 - Module module.layer3.1.conv3
2021-11-21 10:23:59,198 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,198 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,201 - Module module.layer3.1.add
2021-11-21 10:23:59,201 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,201 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,203 - Module module.layer3.1.relu3
2021-11-21 10:23:59,203 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,203 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,205 - Module module.layer3.2
2021-11-21 10:23:59,205 - 	Skipping
2021-11-21 10:23:59,206 - Module module.layer3.2.conv1
2021-11-21 10:23:59,206 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,206 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,209 - Module module.layer3.2.relu1
2021-11-21 10:23:59,209 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,209 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,212 - Module module.layer3.2.conv2
2021-11-21 10:23:59,212 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,212 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,214 - Module module.layer3.2.relu2
2021-11-21 10:23:59,214 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,214 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,217 - Module module.layer3.2.conv3
2021-11-21 10:23:59,217 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,217 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,220 - Module module.layer3.2.add
2021-11-21 10:23:59,220 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,220 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,222 - Module module.layer3.2.relu3
2021-11-21 10:23:59,222 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,222 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,224 - Module module.layer3.3
2021-11-21 10:23:59,224 - 	Skipping
2021-11-21 10:23:59,225 - Module module.layer3.3.conv1
2021-11-21 10:23:59,225 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,225 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,228 - Module module.layer3.3.relu1
2021-11-21 10:23:59,228 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,228 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,231 - Module module.layer3.3.conv2
2021-11-21 10:23:59,231 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,231 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,233 - Module module.layer3.3.relu2
2021-11-21 10:23:59,233 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,233 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,237 - Module module.layer3.3.conv3
2021-11-21 10:23:59,237 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,237 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,239 - Module module.layer3.3.add
2021-11-21 10:23:59,239 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,239 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,242 - Module module.layer3.3.relu3
2021-11-21 10:23:59,242 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,242 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,244 - Module module.layer3.4
2021-11-21 10:23:59,244 - 	Skipping
2021-11-21 10:23:59,245 - Module module.layer3.4.conv1
2021-11-21 10:23:59,245 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,245 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,248 - Module module.layer3.4.relu1
2021-11-21 10:23:59,248 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,248 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,251 - Module module.layer3.4.conv2
2021-11-21 10:23:59,251 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,251 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,254 - Module module.layer3.4.relu2
2021-11-21 10:23:59,254 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,254 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,257 - Module module.layer3.4.conv3
2021-11-21 10:23:59,257 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,257 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,260 - Module module.layer3.4.add
2021-11-21 10:23:59,260 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,260 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,263 - Module module.layer3.4.relu3
2021-11-21 10:23:59,263 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,263 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,265 - Module module.layer3.5
2021-11-21 10:23:59,265 - 	Skipping
2021-11-21 10:23:59,266 - Module module.layer3.5.conv1
2021-11-21 10:23:59,266 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,266 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,269 - Module module.layer3.5.relu1
2021-11-21 10:23:59,269 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,269 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,272 - Module module.layer3.5.conv2
2021-11-21 10:23:59,272 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,272 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,275 - Module module.layer3.5.relu2
2021-11-21 10:23:59,275 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,275 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,279 - Module module.layer3.5.conv3
2021-11-21 10:23:59,279 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,279 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,282 - Module module.layer3.5.add
2021-11-21 10:23:59,282 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,282 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,284 - Module module.layer3.5.relu3
2021-11-21 10:23:59,284 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,284 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,287 - Module module.layer4
2021-11-21 10:23:59,287 - 	Skipping
2021-11-21 10:23:59,287 - Module module.layer4.0
2021-11-21 10:23:59,287 - 	Skipping
2021-11-21 10:23:59,288 - Module module.layer4.0.conv1
2021-11-21 10:23:59,288 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,288 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,291 - Module module.layer4.0.relu1
2021-11-21 10:23:59,291 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,291 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,294 - Module module.layer4.0.conv2
2021-11-21 10:23:59,294 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,294 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,297 - Module module.layer4.0.relu2
2021-11-21 10:23:59,297 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,297 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,301 - Module module.layer4.0.conv3
2021-11-21 10:23:59,301 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,301 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,303 - Module module.layer4.0.downsample
2021-11-21 10:23:59,303 - 	Skipping
2021-11-21 10:23:59,304 - Module module.layer4.0.downsample.0
2021-11-21 10:23:59,305 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,305 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,307 - Module module.layer4.0.add
2021-11-21 10:23:59,308 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,308 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,310 - Module module.layer4.0.relu3
2021-11-21 10:23:59,310 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,310 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,313 - Module module.layer4.1
2021-11-21 10:23:59,313 - 	Skipping
2021-11-21 10:23:59,314 - Module module.layer4.1.conv1
2021-11-21 10:23:59,314 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,314 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,317 - Module module.layer4.1.relu1
2021-11-21 10:23:59,317 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,317 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,320 - Module module.layer4.1.conv2
2021-11-21 10:23:59,320 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,320 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,323 - Module module.layer4.1.relu2
2021-11-21 10:23:59,323 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,323 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,326 - Module module.layer4.1.conv3
2021-11-21 10:23:59,326 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,327 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,329 - Module module.layer4.1.add
2021-11-21 10:23:59,330 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,330 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,332 - Module module.layer4.1.relu3
2021-11-21 10:23:59,332 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,332 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,335 - Module module.layer4.2
2021-11-21 10:23:59,335 - 	Skipping
2021-11-21 10:23:59,336 - Module module.layer4.2.conv1
2021-11-21 10:23:59,336 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,336 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,339 - Module module.layer4.2.relu1
2021-11-21 10:23:59,339 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,339 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,342 - Module module.layer4.2.conv2
2021-11-21 10:23:59,343 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,343 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,345 - Module module.layer4.2.relu2
2021-11-21 10:23:59,345 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,345 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,349 - Module module.layer4.2.conv3
2021-11-21 10:23:59,349 - 	Replacing: torch.nn.modules.conv.Conv2d
2021-11-21 10:23:59,349 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,352 - Module module.layer4.2.add
2021-11-21 10:23:59,352 - 	Replacing: distiller.modules.eltwise.EltwiseAdd
2021-11-21 10:23:59,352 - 	With:      distiller.quantization.range_linear.RangeLinearQuantEltwiseAddWrapper
2021-11-21 10:23:59,355 - Module module.layer4.2.relu3
2021-11-21 10:23:59,355 - 	Replacing: torch.nn.modules.activation.ReLU
2021-11-21 10:23:59,355 - 	With:      torch.nn.modules.linear.Identity
2021-11-21 10:23:59,358 - Module module.avgpool
2021-11-21 10:23:59,358 - 	Replacing: torch.nn.modules.pooling.AdaptiveAvgPool2d
2021-11-21 10:23:59,358 - 	With:      distiller.quantization.range_linear.RangeLinearFakeQuantWrapper
2021-11-21 10:23:59,361 - Module module.fc
2021-11-21 10:23:59,362 - 	Replacing: torch.nn.modules.linear.Linear
2021-11-21 10:23:59,362 - 	With:      distiller.quantization.range_linear.RangeLinearQuantParamLayerWrapper
2021-11-21 10:23:59,366 - Parameter 'module.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.downsample.0.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.0.downsample.0.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.1.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.1.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,366 - Parameter 'module.layer1.1.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.1.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.1.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.1.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer1.2.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.downsample.0.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,367 - Parameter 'module.layer2.0.downsample.0.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.1.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.2.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,368 - Parameter 'module.layer2.3.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.downsample.0.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.0.downsample.0.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.1.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.2.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.2.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.2.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,369 - Parameter 'module.layer3.2.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.2.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.2.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.3.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.4.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,370 - Parameter 'module.layer3.5.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.downsample.0.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.0.downsample.0.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.1.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.2.conv1.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.2.conv1.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,371 - Parameter 'module.layer4.2.conv2.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,372 - Parameter 'module.layer4.2.conv2.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,372 - Parameter 'module.layer4.2.conv3.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,372 - Parameter 'module.layer4.2.conv3.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,372 - Parameter 'module.fc.wrapped_module.weight' will be quantized to 8 bits
2021-11-21 10:23:59,372 - Parameter 'module.fc.wrapped_module.bias' will be quantized to 32 bits
2021-11-21 10:23:59,389 - Quantized model:

DataParallel(
  (module): ResNet(
    (conv1): RangeLinearQuantParamLayerWrapper(
      weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=76.314362, output_zero_point=0.000000
      weights_scale=309.716522, weights_zero_point=-153.000000
      (wrapped_module): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))
    )
    (bn1): Identity()
    (relu): Identity()
    (maxpool): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=76.314369, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (layer1): Sequential(
      (0): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=202.452362, output_zero_point=0.000000
          weights_scale=192.523788, weights_zero_point=-165.000000
          (wrapped_module): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=149.640030, output_zero_point=0.000000
          weights_scale=335.675415, weights_zero_point=-111.000000
          (wrapped_module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=93.623878, output_zero_point=-116.000000
          weights_scale=132.834869, weights_zero_point=-104.000000
          (wrapped_module): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (downsample): Sequential(
          (0): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=54.578442, output_zero_point=-145.000000
            weights_scale=91.953682, weights_zero_point=-123.000000
            (wrapped_module): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): Identity()
        )
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=112.423798, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (1): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=190.108200, output_zero_point=0.000000
          weights_scale=326.063599, weights_zero_point=-92.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=134.566086, output_zero_point=0.000000
          weights_scale=217.689285, weights_zero_point=-108.000000
          (wrapped_module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=103.417084, output_zero_point=-128.000000
          weights_scale=127.874222, weights_zero_point=-127.000000
          (wrapped_module): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=93.685059, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (2): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=188.027359, output_zero_point=0.000000
          weights_scale=393.401642, weights_zero_point=-141.000000
          (wrapped_module): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=113.214050, output_zero_point=0.000000
          weights_scale=200.475327, weights_zero_point=-69.000000
          (wrapped_module): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=69.077072, output_zero_point=-138.000000
          weights_scale=126.852081, weights_zero_point=-120.000000
          (wrapped_module): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=95.319214, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
    )
    (layer2): Sequential(
      (0): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=115.523026, output_zero_point=0.000000
          weights_scale=245.739792, weights_zero_point=-102.000000
          (wrapped_module): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=171.919067, output_zero_point=0.000000
          weights_scale=347.482300, weights_zero_point=-168.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=66.666504, output_zero_point=-113.000000
          weights_scale=97.182014, weights_zero_point=-106.000000
          (wrapped_module): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (downsample): Sequential(
          (0): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=90.857582, output_zero_point=-114.000000
            weights_scale=166.199631, weights_zero_point=-117.000000
            (wrapped_module): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))
          )
          (1): Identity()
        )
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=114.899010, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (1): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=331.259308, output_zero_point=0.000000
          weights_scale=657.691895, weights_zero_point=-87.000000
          (wrapped_module): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=192.341324, output_zero_point=0.000000
          weights_scale=255.814560, weights_zero_point=-105.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=64.800430, output_zero_point=-114.000000
          weights_scale=126.008141, weights_zero_point=-123.000000
          (wrapped_module): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=86.104538, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (2): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=209.841064, output_zero_point=0.000000
          weights_scale=525.140442, weights_zero_point=-128.000000
          (wrapped_module): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=227.464676, output_zero_point=0.000000
          weights_scale=355.190155, weights_zero_point=-97.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=92.014252, output_zero_point=-131.000000
          weights_scale=134.780914, weights_zero_point=-111.000000
          (wrapped_module): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=81.861839, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (3): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=191.900879, output_zero_point=0.000000
          weights_scale=456.028473, weights_zero_point=-86.000000
          (wrapped_module): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=197.868942, output_zero_point=0.000000
          weights_scale=357.245514, weights_zero_point=-137.000000
          (wrapped_module): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=94.590790, output_zero_point=-125.000000
          weights_scale=120.772308, weights_zero_point=-95.000000
          (wrapped_module): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=80.790489, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
    )
    (layer3): Sequential(
      (0): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=112.272530, output_zero_point=0.000000
          weights_scale=263.541260, weights_zero_point=-109.000000
          (wrapped_module): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=194.774536, output_zero_point=0.000000
          weights_scale=482.902527, weights_zero_point=-132.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=82.143326, output_zero_point=-112.000000
          weights_scale=135.952927, weights_zero_point=-130.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (downsample): Sequential(
          (0): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=115.818336, output_zero_point=-119.000000
            weights_scale=242.965057, weights_zero_point=-134.000000
            (wrapped_module): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))
          )
          (1): Identity()
        )
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=127.171211, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (1): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=165.146164, output_zero_point=0.000000
          weights_scale=421.759155, weights_zero_point=-92.000000
          (wrapped_module): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=219.473846, output_zero_point=0.000000
          weights_scale=417.351227, weights_zero_point=-101.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=74.461113, output_zero_point=-124.000000
          weights_scale=93.775642, weights_zero_point=-126.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=97.407700, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (2): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=219.130310, output_zero_point=0.000000
          weights_scale=438.512848, weights_zero_point=-85.000000
          (wrapped_module): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=274.205719, output_zero_point=0.000000
          weights_scale=433.063385, weights_zero_point=-127.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=126.801300, output_zero_point=-130.000000
          weights_scale=142.829025, weights_zero_point=-121.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=102.290245, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (3): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=197.905746, output_zero_point=0.000000
          weights_scale=401.145142, weights_zero_point=-116.000000
          (wrapped_module): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=252.665329, output_zero_point=0.000000
          weights_scale=378.823639, weights_zero_point=-91.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=92.691887, output_zero_point=-130.000000
          weights_scale=106.624992, weights_zero_point=-83.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=89.601845, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (4): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=196.544342, output_zero_point=0.000000
          weights_scale=389.477478, weights_zero_point=-92.000000
          (wrapped_module): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=242.208862, output_zero_point=0.000000
          weights_scale=379.009308, weights_zero_point=-79.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=107.356262, output_zero_point=-138.000000
          weights_scale=83.625671, weights_zero_point=-104.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=87.914017, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (5): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=161.216522, output_zero_point=0.000000
          weights_scale=375.412018, weights_zero_point=-112.000000
          (wrapped_module): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=160.947433, output_zero_point=0.000000
          weights_scale=336.583344, weights_zero_point=-108.000000
          (wrapped_module): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=84.507507, output_zero_point=-152.000000
          weights_scale=116.734207, weights_zero_point=-104.000000
          (wrapped_module): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=109.208725, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
    )
    (layer4): Sequential(
      (0): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=149.072815, output_zero_point=0.000000
          weights_scale=286.792725, weights_zero_point=-97.000000
          (wrapped_module): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=163.396027, output_zero_point=0.000000
          weights_scale=324.073486, weights_zero_point=-120.000000
          (wrapped_module): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=36.546757, output_zero_point=-99.000000
          weights_scale=48.860126, weights_zero_point=-91.000000
          (wrapped_module): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (downsample): Sequential(
          (0): RangeLinearQuantParamLayerWrapper(
            weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
              inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
            scale_approx_mult_bits=None
            preset_activation_stats=True
              output_scale=42.311848, output_zero_point=-110.000000
            weights_scale=33.423954, weights_zero_point=-111.000000
            (wrapped_module): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))
          )
          (1): Identity()
        )
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=41.802349, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (1): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=189.979401, output_zero_point=0.000000
          weights_scale=1021.163513, weights_zero_point=-100.000000
          (wrapped_module): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=243.201523, output_zero_point=0.000000
          weights_scale=431.719666, weights_zero_point=-139.000000
          (wrapped_module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=43.842846, output_zero_point=-106.000000
          weights_scale=47.366165, weights_zero_point=-95.000000
          (wrapped_module): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=33.071167, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
      (2): DistillerBottleneck(
        (conv1): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=189.059372, output_zero_point=0.000000
          weights_scale=858.630920, weights_zero_point=-179.000000
          (wrapped_module): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn1): Identity()
        (relu1): Identity()
        (conv2): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=217.052673, output_zero_point=0.000000
          weights_scale=583.753845, weights_zero_point=-112.000000
          (wrapped_module): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): Identity()
        (relu2): Identity()
        (conv3): RangeLinearQuantParamLayerWrapper(
          weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=20.011677, output_zero_point=-77.000000
          weights_scale=25.240526, weights_zero_point=-76.000000
          (wrapped_module): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
        )
        (bn3): Identity()
        (add): RangeLinearQuantEltwiseAddWrapper(
          output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=True ; per_channel=False)
          accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
            inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
          scale_approx_mult_bits=None
          preset_activation_stats=True
            output_scale=18.665882, output_zero_point=0.000000
          (wrapped_module): EltwiseAdd()
        )
        (relu3): Identity()
      )
    )
    (avgpool): RangeLinearFakeQuantWrapper(
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=56.292549, output_zero_point=0.000000
      wrapped_module_float_dtype=torch.float32.
      (wrapped_module): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (fc): RangeLinearQuantParamLayerWrapper(
      weights_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      output_quant_settings=(num_bits=8 ; quant_mode=ASYMMETRIC_UNSIGNED ; clip_mode=AVG ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
      accum_quant_settings=(num_bits=32 ; quant_mode=SYMMETRIC ; clip_mode=NONE ; clip_n_stds=None ; clip_half_range=False ; per_channel=False)
        inputs_quant_auto_fallback=True, forced_quant_settings_for_inputs=None
      scale_approx_mult_bits=None
      preset_activation_stats=True
        output_scale=11.161633, output_zero_point=-64.000000
      weights_scale=260.071014, weights_zero_point=-63.000000
      (wrapped_module): Linear(in_features=2048, out_features=1000, bias=True)
    )
  )
)

2021-11-21 10:23:59,781 - Per-layer quantization parameters saved to logs/resnet50-imagenet-flipcy___2021.11.21-102349/layer_quant_params.yaml
2021-11-21 10:24:06,129 - --- test ---------------------
2021-11-21 10:24:06,130 - 50000 samples (256 per mini-batch)
2021-11-21 10:24:37,780 - Test: [   10/  195]    Loss 6.907756    Top1 0.078125    Top5 0.468750    
2021-11-21 10:25:00,655 - Test: [   20/  195]    Loss 6.907756    Top1 0.078125    Top5 0.468750    
2021-11-21 10:25:22,631 - Test: [   30/  195]    Loss 6.907756    Top1 0.078125    Top5 0.429687    
2021-11-21 10:25:45,760 - Test: [   40/  195]    Loss 6.907756    Top1 0.078125    Top5 0.439453    
2021-11-21 10:26:08,689 - Test: [   50/  195]    Loss 6.907756    Top1 0.085937    Top5 0.460938    
2021-11-21 10:26:31,675 - Test: [   60/  195]    Loss 6.907756    Top1 0.104167    Top5 0.462240    
2021-11-21 10:26:54,492 - Test: [   70/  195]    Loss 6.907756    Top1 0.100446    Top5 0.491071    
2021-11-21 10:27:16,303 - Test: [   80/  195]    Loss 6.907756    Top1 0.117188    Top5 0.498047    
2021-11-21 10:27:38,736 - Test: [   90/  195]    Loss 6.907756    Top1 0.112847    Top5 0.499132    
2021-11-21 10:28:01,403 - Test: [  100/  195]    Loss 6.907756    Top1 0.105469    Top5 0.488281    
2021-11-21 10:28:23,889 - Test: [  110/  195]    Loss 6.907756    Top1 0.106534    Top5 0.500710    
2021-11-21 10:28:46,436 - Test: [  120/  195]    Loss 6.907756    Top1 0.104167    Top5 0.494792    
2021-11-21 10:29:08,979 - Test: [  130/  195]    Loss 6.907756    Top1 0.099159    Top5 0.474760    
2021-11-21 10:29:30,707 - Test: [  140/  195]    Loss 6.907756    Top1 0.094866    Top5 0.482701    
2021-11-21 10:29:52,994 - Test: [  150/  195]    Loss 6.907756    Top1 0.093750    Top5 0.473958    
2021-11-21 10:30:15,485 - Test: [  160/  195]    Loss 6.907756    Top1 0.092773    Top5 0.471191    
2021-11-21 10:30:37,939 - Test: [  170/  195]    Loss 6.907756    Top1 0.094210    Top5 0.480239    
2021-11-21 10:31:00,386 - Test: [  180/  195]    Loss 6.907756    Top1 0.095486    Top5 0.486111    
2021-11-21 10:31:22,903 - Test: [  190/  195]    Loss 6.907756    Top1 0.100740    Top5 0.499589    
2021-11-21 10:31:36,081 - ==> Top1: 0.100    Top5: 0.500    Loss: 6.908

2021-11-21 10:31:36,146 - 
2021-11-21 10:31:36,146 - Log file for this run: /home/th.nguyen/drift-encode/logs/resnet50-imagenet-flipcy___2021.11.21-102349/resnet50-imagenet-flipcy___2021.11.21-102349.log
